2025-04-03 18:07:33,245 - INFO - Starting regression training process...
2025-04-03 18:07:33,246 - INFO - Epoch 1 started.
2025-04-03 18:07:33,246 - INFO - Epoch 1, Batch 1: Training loss = 1.0000
2025-04-03 18:07:33,346 - INFO - Epoch 1, Batch 2: Training loss = 0.9900
2025-04-03 18:07:33,448 - INFO - Epoch 1, Batch 3: Training loss = 0.9800
2025-04-03 18:07:33,548 - INFO - Epoch 1, Batch 4: Training loss = 0.9700
2025-04-03 18:07:33,649 - INFO - Epoch 1, Batch 5: Training loss = 0.9600
2025-04-03 18:07:33,749 - INFO - Epoch 1, Batch 6: Training loss = 0.9500
2025-04-03 18:07:33,850 - INFO - Epoch 1, Batch 7: Training loss = 0.9400
2025-04-03 18:07:33,950 - INFO - Epoch 1, Batch 8: Training loss = 0.9300
2025-04-03 18:07:34,050 - INFO - Epoch 1, Batch 9: Training loss = 0.9200
2025-04-03 18:07:34,151 - INFO - Epoch 1, Batch 10: Training loss = 0.9100
2025-04-03 18:07:34,251 - INFO - Epoch 1, Batch 11: Training loss = 0.9000
2025-04-03 18:07:34,351 - INFO - Epoch 1, Batch 12: Training loss = 0.8900
2025-04-03 18:07:34,452 - INFO - Epoch 1, Batch 13: Training loss = 0.8800
2025-04-03 18:07:34,553 - INFO - Epoch 1, Batch 14: Training loss = 0.8700
2025-04-03 18:07:34,653 - INFO - Epoch 1, Batch 15: Training loss = 0.8600
2025-04-03 18:07:34,754 - INFO - Epoch 1, Batch 16: Training loss = 0.8500
2025-04-03 18:07:34,855 - INFO - Epoch 1, Batch 17: Training loss = 0.8400
2025-04-03 18:07:34,955 - INFO - Epoch 1, Batch 18: Training loss = 0.8300
2025-04-03 18:07:35,056 - INFO - Epoch 1, Batch 19: Training loss = 0.8200
2025-04-03 18:07:35,156 - INFO - Epoch 1, Batch 20: Training loss = 0.8100
2025-04-03 18:07:35,256 - INFO - Epoch 1, Batch 21: Training loss = 0.8000
2025-04-03 18:07:35,357 - INFO - Epoch 1, Batch 22: Training loss = 0.7900
2025-04-03 18:07:35,458 - INFO - Epoch 1, Batch 23: Training loss = 0.7800
2025-04-03 18:07:35,558 - INFO - Epoch 1, Batch 24: Training loss = 0.7700
2025-04-03 18:07:35,658 - INFO - Epoch 1, Batch 25: Training loss = 0.7600
2025-04-03 18:07:35,759 - INFO - Epoch 1, Batch 26: Training loss = 0.7500
2025-04-03 18:07:35,860 - INFO - Epoch 1, Batch 27: Training loss = 0.7400
2025-04-03 18:07:35,960 - INFO - Epoch 1, Batch 28: Training loss = 0.7300
2025-04-03 18:07:36,061 - INFO - Epoch 1, Batch 29: Training loss = 0.7200
2025-04-03 18:07:36,162 - INFO - Epoch 1, Batch 30: Training loss = 0.7100
2025-04-03 18:07:36,263 - INFO - Epoch 1, Batch 31: Training loss = 0.7000
2025-04-03 18:07:36,364 - INFO - Epoch 1, Batch 32: Training loss = 0.6900
2025-04-03 18:07:36,465 - INFO - Epoch 1, Batch 33: Training loss = 0.6800
2025-04-03 18:07:36,565 - INFO - Epoch 1, Batch 34: Training loss = 0.6700
2025-04-03 18:07:36,666 - INFO - Epoch 1, Batch 35: Training loss = 0.6600
2025-04-03 18:07:36,767 - INFO - Epoch 1, Batch 36: Training loss = 0.6500
2025-04-03 18:07:36,868 - INFO - Epoch 1, Batch 37: Training loss = 0.6400
2025-04-03 18:07:36,969 - INFO - Epoch 1, Batch 38: Training loss = 0.6300
2025-04-03 18:07:37,070 - INFO - Epoch 1, Batch 39: Training loss = 0.6200
2025-04-03 18:07:37,170 - INFO - Epoch 1, Batch 40: Training loss = 0.6100
2025-04-03 18:07:37,271 - INFO - Epoch 1, Batch 41: Training loss = 0.6000
2025-04-03 18:07:37,371 - INFO - Epoch 1, Batch 42: Training loss = 0.5900
2025-04-03 18:07:37,471 - INFO - Epoch 1, Batch 43: Training loss = 0.5800
2025-04-03 18:07:37,571 - INFO - Epoch 1, Batch 44: Training loss = 0.5700
2025-04-03 18:07:37,672 - INFO - Epoch 1, Batch 45: Training loss = 0.5600
2025-04-03 18:07:37,773 - INFO - Epoch 1, Batch 46: Training loss = 0.5500
2025-04-03 18:07:37,873 - INFO - Epoch 1, Batch 47: Training loss = 0.5400
2025-04-03 18:07:37,974 - INFO - Epoch 1, Batch 48: Training loss = 0.5300
2025-04-03 18:07:38,074 - INFO - Epoch 1, Batch 49: Training loss = 0.5200
2025-04-03 18:07:38,175 - INFO - Epoch 1, Batch 50: Training loss = 0.5100
2025-04-03 18:07:38,275 - INFO - Epoch 1, Batch 51: Training loss = 0.5000
2025-04-03 18:07:38,376 - INFO - Epoch 1, Batch 52: Training loss = 0.4900
2025-04-03 18:07:38,476 - INFO - Epoch 1, Batch 53: Training loss = 0.4800
2025-04-03 18:07:38,577 - INFO - Epoch 1, Batch 54: Training loss = 0.4700
2025-04-03 18:07:38,678 - INFO - Epoch 1, Batch 55: Training loss = 0.4600
2025-04-03 18:07:38,778 - INFO - Epoch 1, Batch 56: Training loss = 0.4500
2025-04-03 18:07:38,878 - INFO - Epoch 1, Batch 57: Training loss = 0.4400
2025-04-03 18:07:38,979 - INFO - Epoch 1, Batch 58: Training loss = 0.4300
2025-04-03 18:07:39,080 - INFO - Epoch 1, Batch 59: Training loss = 0.4200
2025-04-03 18:07:39,180 - INFO - Epoch 1, Batch 60: Training loss = 0.4100
2025-04-03 18:07:39,281 - INFO - Epoch 1, Batch 61: Training loss = 0.4000
2025-04-03 18:07:39,381 - INFO - Epoch 1, Batch 62: Training loss = 0.3900
2025-04-03 18:07:39,482 - INFO - Epoch 1, Batch 63: Training loss = 0.3800
2025-04-03 18:07:39,582 - INFO - Epoch 1, Batch 64: Training loss = 0.3700
2025-04-03 18:07:39,683 - INFO - Epoch 1, Batch 65: Training loss = 0.3600
2025-04-03 18:07:39,784 - INFO - Epoch 1, Batch 66: Training loss = 0.3500
2025-04-03 18:07:39,884 - INFO - Epoch 1, Batch 67: Training loss = 0.3400
2025-04-03 18:07:39,985 - INFO - Epoch 1, Batch 68: Training loss = 0.3300
2025-04-03 18:07:40,086 - INFO - Epoch 1, Batch 69: Training loss = 0.3200
2025-04-03 18:07:40,186 - INFO - Epoch 1, Batch 70: Training loss = 0.3100
2025-04-03 18:07:40,287 - INFO - Epoch 1, Batch 71: Training loss = 0.3000
2025-04-03 18:07:40,388 - INFO - Epoch 1, Batch 72: Training loss = 0.2900
2025-04-03 18:07:40,488 - INFO - Epoch 1, Batch 73: Training loss = 0.2800
2025-04-03 18:07:40,589 - INFO - Epoch 1, Batch 74: Training loss = 0.2700
2025-04-03 18:07:40,689 - INFO - Epoch 1, Batch 75: Training loss = 0.2600
2025-04-03 18:07:40,790 - INFO - Epoch 1, Batch 76: Training loss = 0.2500
2025-04-03 18:07:40,891 - INFO - Epoch 1, Batch 77: Training loss = 0.2400
2025-04-03 18:07:40,991 - INFO - Epoch 1, Batch 78: Training loss = 0.2300
2025-04-03 18:07:41,092 - INFO - Epoch 1, Batch 79: Training loss = 0.2200
2025-04-03 18:07:41,192 - INFO - Epoch 1, Batch 80: Training loss = 0.2100
2025-04-03 18:07:41,293 - INFO - Epoch 1, Batch 81: Training loss = 0.2000
2025-04-03 18:07:41,394 - INFO - Epoch 1, Batch 82: Training loss = 0.1900
2025-04-03 18:07:41,494 - INFO - Epoch 1, Batch 83: Training loss = 0.1800
2025-04-03 18:07:41,595 - INFO - Epoch 1, Batch 84: Training loss = 0.1700
2025-04-03 18:07:41,695 - INFO - Epoch 1, Batch 85: Training loss = 0.1600
2025-04-03 18:07:41,796 - INFO - Epoch 1, Batch 86: Training loss = 0.1500
2025-04-03 18:07:41,897 - INFO - Epoch 1, Batch 87: Training loss = 0.1400
2025-04-03 18:07:41,998 - INFO - Epoch 1, Batch 88: Training loss = 0.1300
2025-04-03 18:07:42,099 - INFO - Epoch 1, Batch 89: Training loss = 0.1200
2025-04-03 18:07:42,200 - INFO - Epoch 1, Batch 90: Training loss = 0.1100
2025-04-03 18:07:42,301 - INFO - Epoch 1, Batch 91: Training loss = 0.1000
2025-04-03 18:07:42,401 - INFO - Epoch 1, Batch 92: Training loss = 0.0900
2025-04-03 18:07:42,502 - INFO - Epoch 1, Batch 93: Training loss = 0.0800
2025-04-03 18:07:42,603 - INFO - Epoch 1, Batch 94: Training loss = 0.0700
2025-04-03 18:07:42,703 - INFO - Epoch 1, Batch 95: Training loss = 0.0600
2025-04-03 18:07:42,804 - INFO - Epoch 1, Batch 96: Training loss = 0.0500
2025-04-03 18:07:42,904 - INFO - Epoch 1, Batch 97: Training loss = 0.0400
2025-04-03 18:07:43,005 - INFO - Epoch 1, Batch 98: Training loss = 0.0300
2025-04-03 18:07:43,105 - INFO - Epoch 1, Batch 99: Training loss = 0.0200
2025-04-03 18:07:43,206 - INFO - Epoch 1, Batch 100: Training loss = 0.0100
2025-04-03 18:07:43,306 - INFO - Epoch 1: Validation loss = 0.1000, Validation accuracy = 0.8000
2025-04-03 18:07:43,306 - INFO - Epoch 1: MSE = 0.1000, RMSE = 0.3162, MAE = 0.0800, RÂ² = 0.9000
2025-04-03 18:07:43,307 - INFO - Epoch 2 started.
2025-04-03 18:07:43,307 - INFO - Epoch 2, Batch 1: Training loss = 1.0000
2025-04-03 18:07:43,407 - INFO - Epoch 2, Batch 2: Training loss = 0.9900
2025-04-03 18:07:43,508 - INFO - Epoch 2, Batch 3: Training loss = 0.9800
2025-04-03 18:07:43,609 - INFO - Epoch 2, Batch 4: Training loss = 0.9700
2025-04-03 18:07:43,709 - INFO - Epoch 2, Batch 5: Training loss = 0.9600
2025-04-03 18:07:43,810 - INFO - Epoch 2, Batch 6: Training loss = 0.9500
2025-04-03 18:07:43,911 - INFO - Epoch 2, Batch 7: Training loss = 0.9400
2025-04-03 18:07:44,012 - INFO - Epoch 2, Batch 8: Training loss = 0.9300
2025-04-03 18:07:44,112 - INFO - Epoch 2, Batch 9: Training loss = 0.9200
2025-04-03 18:07:44,213 - INFO - Epoch 2, Batch 10: Training loss = 0.9100
2025-04-03 18:07:44,313 - INFO - Epoch 2, Batch 11: Training loss = 0.9000
2025-04-03 18:07:44,414 - INFO - Epoch 2, Batch 12: Training loss = 0.8900
2025-04-03 18:07:44,514 - INFO - Epoch 2, Batch 13: Training loss = 0.8800
2025-04-03 18:07:44,615 - INFO - Epoch 2, Batch 14: Training loss = 0.8700
2025-04-03 18:07:44,715 - INFO - Epoch 2, Batch 15: Training loss = 0.8600
2025-04-03 18:07:44,816 - INFO - Epoch 2, Batch 16: Training loss = 0.8500
2025-04-03 18:07:44,916 - INFO - Epoch 2, Batch 17: Training loss = 0.8400
2025-04-03 18:07:45,017 - INFO - Epoch 2, Batch 18: Training loss = 0.8300
2025-04-03 18:07:45,118 - INFO - Epoch 2, Batch 19: Training loss = 0.8200
2025-04-03 18:07:45,218 - INFO - Epoch 2, Batch 20: Training loss = 0.8100
2025-04-03 18:07:45,319 - INFO - Epoch 2, Batch 21: Training loss = 0.8000
2025-04-03 18:07:45,419 - INFO - Epoch 2, Batch 22: Training loss = 0.7900
2025-04-03 18:07:45,520 - INFO - Epoch 2, Batch 23: Training loss = 0.7800
2025-04-03 18:07:45,621 - INFO - Epoch 2, Batch 24: Training loss = 0.7700
2025-04-03 18:07:45,722 - INFO - Epoch 2, Batch 25: Training loss = 0.7600
2025-04-03 18:07:45,823 - INFO - Epoch 2, Batch 26: Training loss = 0.7500
2025-04-03 18:07:45,924 - INFO - Epoch 2, Batch 27: Training loss = 0.7400
2025-04-03 18:07:46,024 - INFO - Epoch 2, Batch 28: Training loss = 0.7300
2025-04-03 18:07:46,125 - INFO - Epoch 2, Batch 29: Training loss = 0.7200
2025-04-03 18:07:46,226 - INFO - Epoch 2, Batch 30: Training loss = 0.7100
2025-04-03 18:07:46,326 - INFO - Epoch 2, Batch 31: Training loss = 0.7000
2025-04-03 18:07:46,427 - INFO - Epoch 2, Batch 32: Training loss = 0.6900
2025-04-03 18:07:46,527 - INFO - Epoch 2, Batch 33: Training loss = 0.6800
2025-04-03 18:07:46,627 - INFO - Epoch 2, Batch 34: Training loss = 0.6700
2025-04-03 18:07:46,728 - INFO - Epoch 2, Batch 35: Training loss = 0.6600
2025-04-03 18:07:46,828 - INFO - Epoch 2, Batch 36: Training loss = 0.6500
2025-04-03 18:07:46,929 - INFO - Epoch 2, Batch 37: Training loss = 0.6400
2025-04-03 18:07:47,030 - INFO - Epoch 2, Batch 38: Training loss = 0.6300
2025-04-03 18:07:47,130 - INFO - Epoch 2, Batch 39: Training loss = 0.6200
2025-04-03 18:07:47,231 - INFO - Epoch 2, Batch 40: Training loss = 0.6100
2025-04-03 18:07:47,331 - INFO - Epoch 2, Batch 41: Training loss = 0.6000
2025-04-03 18:07:47,431 - INFO - Epoch 2, Batch 42: Training loss = 0.5900
2025-04-03 18:07:47,532 - INFO - Epoch 2, Batch 43: Training loss = 0.5800
2025-04-03 18:07:47,633 - INFO - Epoch 2, Batch 44: Training loss = 0.5700
2025-04-03 18:07:47,734 - INFO - Epoch 2, Batch 45: Training loss = 0.5600
2025-04-03 18:07:47,834 - INFO - Epoch 2, Batch 46: Training loss = 0.5500
2025-04-03 18:07:47,935 - INFO - Epoch 2, Batch 47: Training loss = 0.5400
2025-04-03 18:07:48,036 - INFO - Epoch 2, Batch 48: Training loss = 0.5300
2025-04-03 18:07:48,136 - INFO - Epoch 2, Batch 49: Training loss = 0.5200
2025-04-03 18:07:48,236 - INFO - Epoch 2, Batch 50: Training loss = 0.5100
2025-04-03 18:07:48,337 - INFO - Epoch 2, Batch 51: Training loss = 0.5000
2025-04-03 18:07:48,438 - INFO - Epoch 2, Batch 52: Training loss = 0.4900
2025-04-03 18:07:48,539 - INFO - Epoch 2, Batch 53: Training loss = 0.4800
2025-04-03 18:07:48,639 - INFO - Epoch 2, Batch 54: Training loss = 0.4700
2025-04-03 18:07:48,740 - INFO - Epoch 2, Batch 55: Training loss = 0.4600
2025-04-03 18:07:48,840 - INFO - Epoch 2, Batch 56: Training loss = 0.4500
2025-04-03 18:07:48,941 - INFO - Epoch 2, Batch 57: Training loss = 0.4400
2025-04-03 18:07:49,041 - INFO - Epoch 2, Batch 58: Training loss = 0.4300
2025-04-03 18:07:49,141 - INFO - Epoch 2, Batch 59: Training loss = 0.4200
2025-04-03 18:07:49,242 - INFO - Epoch 2, Batch 60: Training loss = 0.4100
2025-04-03 18:07:49,343 - INFO - Epoch 2, Batch 61: Training loss = 0.4000
2025-04-03 18:07:49,443 - INFO - Epoch 2, Batch 62: Training loss = 0.3900
2025-04-03 18:07:49,544 - INFO - Epoch 2, Batch 63: Training loss = 0.3800
2025-04-03 18:07:49,644 - INFO - Epoch 2, Batch 64: Training loss = 0.3700
2025-04-03 18:07:49,745 - INFO - Epoch 2, Batch 65: Training loss = 0.3600
2025-04-03 18:07:49,846 - INFO - Epoch 2, Batch 66: Training loss = 0.3500
2025-04-03 18:07:49,946 - INFO - Epoch 2, Batch 67: Training loss = 0.3400
2025-04-03 18:07:50,047 - INFO - Epoch 2, Batch 68: Training loss = 0.3300
2025-04-03 18:07:50,147 - INFO - Epoch 2, Batch 69: Training loss = 0.3200
2025-04-03 18:07:50,248 - INFO - Epoch 2, Batch 70: Training loss = 0.3100
2025-04-03 18:07:50,348 - INFO - Epoch 2, Batch 71: Training loss = 0.3000
2025-04-03 18:07:50,449 - INFO - Epoch 2, Batch 72: Training loss = 0.2900
2025-04-03 18:07:50,549 - INFO - Epoch 2, Batch 73: Training loss = 0.2800
2025-04-03 18:07:50,650 - INFO - Epoch 2, Batch 74: Training loss = 0.2700
2025-04-03 18:07:50,751 - INFO - Epoch 2, Batch 75: Training loss = 0.2600
2025-04-03 18:07:50,851 - INFO - Epoch 2, Batch 76: Training loss = 0.2500
2025-04-03 18:07:50,952 - INFO - Epoch 2, Batch 77: Training loss = 0.2400
2025-04-03 18:07:51,052 - INFO - Epoch 2, Batch 78: Training loss = 0.2300
2025-04-03 18:07:51,153 - INFO - Epoch 2, Batch 79: Training loss = 0.2200
2025-04-03 18:07:51,253 - INFO - Epoch 2, Batch 80: Training loss = 0.2100
2025-04-03 18:07:51,354 - INFO - Epoch 2, Batch 81: Training loss = 0.2000
2025-04-03 18:07:51,454 - INFO - Epoch 2, Batch 82: Training loss = 0.1900
2025-04-03 18:07:51,555 - INFO - Epoch 2, Batch 83: Training loss = 0.1800
2025-04-03 18:07:51,656 - INFO - Epoch 2, Batch 84: Training loss = 0.1700
2025-04-03 18:07:51,757 - INFO - Epoch 2, Batch 85: Training loss = 0.1600
2025-04-03 18:07:51,858 - INFO - Epoch 2, Batch 86: Training loss = 0.1500
2025-04-03 18:07:51,958 - INFO - Epoch 2, Batch 87: Training loss = 0.1400
2025-04-03 18:07:52,058 - INFO - Epoch 2, Batch 88: Training loss = 0.1300
2025-04-03 18:07:52,159 - INFO - Epoch 2, Batch 89: Training loss = 0.1200
2025-04-03 18:07:52,259 - INFO - Epoch 2, Batch 90: Training loss = 0.1100
2025-04-03 18:07:52,360 - INFO - Epoch 2, Batch 91: Training loss = 0.1000
2025-04-03 18:07:52,461 - INFO - Epoch 2, Batch 92: Training loss = 0.0900
2025-04-03 18:07:52,561 - INFO - Epoch 2, Batch 93: Training loss = 0.0800
2025-04-03 18:07:52,662 - INFO - Epoch 2, Batch 94: Training loss = 0.0700
2025-04-03 18:07:52,763 - INFO - Epoch 2, Batch 95: Training loss = 0.0600
2025-04-03 18:07:52,863 - INFO - Epoch 2, Batch 96: Training loss = 0.0500
2025-04-03 18:07:52,964 - INFO - Epoch 2, Batch 97: Training loss = 0.0400
2025-04-03 18:07:53,065 - INFO - Epoch 2, Batch 98: Training loss = 0.0300
2025-04-03 18:07:53,165 - INFO - Epoch 2, Batch 99: Training loss = 0.0200
2025-04-03 18:07:53,266 - INFO - Epoch 2, Batch 100: Training loss = 0.0100
2025-04-03 18:07:53,366 - INFO - Epoch 2: Validation loss = 0.0900, Validation accuracy = 0.8200
2025-04-03 18:07:53,367 - INFO - Epoch 2: MSE = 0.0900, RMSE = 0.3000, MAE = 0.0720, RÂ² = 0.9100
2025-04-03 18:07:53,367 - INFO - Epoch 3 started.
2025-04-03 18:07:53,367 - INFO - Epoch 3, Batch 1: Training loss = 1.0000
2025-04-03 18:07:53,467 - INFO - Epoch 3, Batch 2: Training loss = 0.9900
2025-04-03 18:07:53,568 - INFO - Epoch 3, Batch 3: Training loss = 0.9800
2025-04-03 18:07:53,668 - INFO - Epoch 3, Batch 4: Training loss = 0.9700
2025-04-03 18:07:53,769 - INFO - Epoch 3, Batch 5: Training loss = 0.9600
2025-04-03 18:07:53,870 - INFO - Epoch 3, Batch 6: Training loss = 0.9500
2025-04-03 18:07:53,970 - INFO - Epoch 3, Batch 7: Training loss = 0.9400
2025-04-03 18:07:54,071 - INFO - Epoch 3, Batch 8: Training loss = 0.9300
2025-04-03 18:07:54,171 - INFO - Epoch 3, Batch 9: Training loss = 0.9200
2025-04-03 18:07:54,272 - INFO - Epoch 3, Batch 10: Training loss = 0.9100
2025-04-03 18:07:54,373 - INFO - Epoch 3, Batch 11: Training loss = 0.9000
2025-04-03 18:07:54,474 - INFO - Epoch 3, Batch 12: Training loss = 0.8900
2025-04-03 18:07:54,575 - INFO - Epoch 3, Batch 13: Training loss = 0.8800
2025-04-03 18:07:54,676 - INFO - Epoch 3, Batch 14: Training loss = 0.8700
2025-04-03 18:07:54,777 - INFO - Epoch 3, Batch 15: Training loss = 0.8600
2025-04-03 18:07:54,878 - INFO - Epoch 3, Batch 16: Training loss = 0.8500
2025-04-03 18:07:54,979 - INFO - Epoch 3, Batch 17: Training loss = 0.8400
2025-04-03 18:07:55,079 - INFO - Epoch 3, Batch 18: Training loss = 0.8300
2025-04-03 18:07:55,180 - INFO - Epoch 3, Batch 19: Training loss = 0.8200
2025-04-03 18:07:55,280 - INFO - Epoch 3, Batch 20: Training loss = 0.8100
2025-04-03 18:07:55,381 - INFO - Epoch 3, Batch 21: Training loss = 0.8000
2025-04-03 18:07:55,482 - INFO - Epoch 3, Batch 22: Training loss = 0.7900
2025-04-03 18:07:55,583 - INFO - Epoch 3, Batch 23: Training loss = 0.7800
2025-04-03 18:07:55,683 - INFO - Epoch 3, Batch 24: Training loss = 0.7700
2025-04-03 18:07:55,784 - INFO - Epoch 3, Batch 25: Training loss = 0.7600
2025-04-03 18:07:55,884 - INFO - Epoch 3, Batch 26: Training loss = 0.7500
2025-04-03 18:07:55,985 - INFO - Epoch 3, Batch 27: Training loss = 0.7400
2025-04-03 18:07:56,085 - INFO - Epoch 3, Batch 28: Training loss = 0.7300
2025-04-03 18:07:56,186 - INFO - Epoch 3, Batch 29: Training loss = 0.7200
2025-04-03 18:07:56,286 - INFO - Epoch 3, Batch 30: Training loss = 0.7100
2025-04-03 18:07:56,387 - INFO - Epoch 3, Batch 31: Training loss = 0.7000
2025-04-03 18:07:56,488 - INFO - Epoch 3, Batch 32: Training loss = 0.6900
2025-04-03 18:07:56,589 - INFO - Epoch 3, Batch 33: Training loss = 0.6800
2025-04-03 18:07:56,689 - INFO - Epoch 3, Batch 34: Training loss = 0.6700
2025-04-03 18:07:56,790 - INFO - Epoch 3, Batch 35: Training loss = 0.6600
2025-04-03 18:07:56,890 - INFO - Epoch 3, Batch 36: Training loss = 0.6500
2025-04-03 18:07:56,991 - INFO - Epoch 3, Batch 37: Training loss = 0.6400
2025-04-03 18:07:57,091 - INFO - Epoch 3, Batch 38: Training loss = 0.6300
2025-04-03 18:07:57,192 - INFO - Epoch 3, Batch 39: Training loss = 0.6200
2025-04-03 18:07:57,292 - INFO - Epoch 3, Batch 40: Training loss = 0.6100
2025-04-03 18:07:57,393 - INFO - Epoch 3, Batch 41: Training loss = 0.6000
2025-04-03 18:07:57,494 - INFO - Epoch 3, Batch 42: Training loss = 0.5900
2025-04-03 18:07:57,595 - INFO - Epoch 3, Batch 43: Training loss = 0.5800
2025-04-03 18:07:57,696 - INFO - Epoch 3, Batch 44: Training loss = 0.5700
2025-04-03 18:07:57,796 - INFO - Epoch 3, Batch 45: Training loss = 0.5600
2025-04-03 18:07:57,897 - INFO - Epoch 3, Batch 46: Training loss = 0.5500
2025-04-03 18:07:57,998 - INFO - Epoch 3, Batch 47: Training loss = 0.5400
2025-04-03 18:07:58,099 - INFO - Epoch 3, Batch 48: Training loss = 0.5300
2025-04-03 18:07:58,199 - INFO - Epoch 3, Batch 49: Training loss = 0.5200
2025-04-03 18:07:58,300 - INFO - Epoch 3, Batch 50: Training loss = 0.5100
2025-04-03 18:07:58,401 - INFO - Epoch 3, Batch 51: Training loss = 0.5000
2025-04-03 18:07:58,502 - INFO - Epoch 3, Batch 52: Training loss = 0.4900
2025-04-03 18:07:58,603 - INFO - Epoch 3, Batch 53: Training loss = 0.4800
2025-04-03 18:07:58,703 - INFO - Epoch 3, Batch 54: Training loss = 0.4700
2025-04-03 18:07:58,804 - INFO - Epoch 3, Batch 55: Training loss = 0.4600
2025-04-03 18:07:58,905 - INFO - Epoch 3, Batch 56: Training loss = 0.4500
2025-04-03 18:07:59,005 - INFO - Epoch 3, Batch 57: Training loss = 0.4400
2025-04-03 18:07:59,106 - INFO - Epoch 3, Batch 58: Training loss = 0.4300
2025-04-03 18:07:59,207 - INFO - Epoch 3, Batch 59: Training loss = 0.4200
2025-04-03 18:07:59,307 - INFO - Epoch 3, Batch 60: Training loss = 0.4100
2025-04-03 18:07:59,408 - INFO - Epoch 3, Batch 61: Training loss = 0.4000
2025-04-03 18:07:59,509 - INFO - Epoch 3, Batch 62: Training loss = 0.3900
2025-04-03 18:07:59,610 - INFO - Epoch 3, Batch 63: Training loss = 0.3800
2025-04-03 18:07:59,711 - INFO - Epoch 3, Batch 64: Training loss = 0.3700
2025-04-03 18:07:59,812 - INFO - Epoch 3, Batch 65: Training loss = 0.3600
2025-04-03 18:07:59,914 - INFO - Epoch 3, Batch 66: Training loss = 0.3500
2025-04-03 18:08:00,015 - INFO - Epoch 3, Batch 67: Training loss = 0.3400
2025-04-03 18:08:00,115 - INFO - Epoch 3, Batch 68: Training loss = 0.3300
2025-04-03 18:08:00,216 - INFO - Epoch 3, Batch 69: Training loss = 0.3200
2025-04-03 18:08:00,317 - INFO - Epoch 3, Batch 70: Training loss = 0.3100
2025-04-03 18:08:00,417 - INFO - Epoch 3, Batch 71: Training loss = 0.3000
2025-04-03 18:08:00,517 - INFO - Epoch 3, Batch 72: Training loss = 0.2900
2025-04-03 18:08:00,618 - INFO - Epoch 3, Batch 73: Training loss = 0.2800
2025-04-03 18:08:00,719 - INFO - Epoch 3, Batch 74: Training loss = 0.2700
2025-04-03 18:08:00,819 - INFO - Epoch 3, Batch 75: Training loss = 0.2600
2025-04-03 18:08:00,920 - INFO - Epoch 3, Batch 76: Training loss = 0.2500
2025-04-03 18:08:01,020 - INFO - Epoch 3, Batch 77: Training loss = 0.2400
2025-04-03 18:08:01,121 - INFO - Epoch 3, Batch 78: Training loss = 0.2300
2025-04-03 18:08:01,221 - INFO - Epoch 3, Batch 79: Training loss = 0.2200
2025-04-03 18:08:01,322 - INFO - Epoch 3, Batch 80: Training loss = 0.2100
2025-04-03 18:08:01,423 - INFO - Epoch 3, Batch 81: Training loss = 0.2000
2025-04-03 18:08:01,524 - INFO - Epoch 3, Batch 82: Training loss = 0.1900
2025-04-03 18:08:01,624 - INFO - Epoch 3, Batch 83: Training loss = 0.1800
2025-04-03 18:08:01,725 - INFO - Epoch 3, Batch 84: Training loss = 0.1700
2025-04-03 18:08:01,825 - INFO - Epoch 3, Batch 85: Training loss = 0.1600
2025-04-03 18:08:01,926 - INFO - Epoch 3, Batch 86: Training loss = 0.1500
2025-04-03 18:08:02,027 - INFO - Epoch 3, Batch 87: Training loss = 0.1400
2025-04-03 18:08:02,128 - INFO - Epoch 3, Batch 88: Training loss = 0.1300
2025-04-03 18:08:02,228 - INFO - Epoch 3, Batch 89: Training loss = 0.1200
2025-04-03 18:08:02,329 - INFO - Epoch 3, Batch 90: Training loss = 0.1100
2025-04-03 18:08:02,429 - INFO - Epoch 3, Batch 91: Training loss = 0.1000
2025-04-03 18:08:02,530 - INFO - Epoch 3, Batch 92: Training loss = 0.0900
2025-04-03 18:08:02,631 - INFO - Epoch 3, Batch 93: Training loss = 0.0800
2025-04-03 18:08:02,731 - INFO - Epoch 3, Batch 94: Training loss = 0.0700
2025-04-03 18:08:02,832 - INFO - Epoch 3, Batch 95: Training loss = 0.0600
2025-04-03 18:08:02,932 - INFO - Epoch 3, Batch 96: Training loss = 0.0500
2025-04-03 18:08:03,033 - INFO - Epoch 3, Batch 97: Training loss = 0.0400
2025-04-03 18:08:03,133 - INFO - Epoch 3, Batch 98: Training loss = 0.0300
2025-04-03 18:08:03,234 - INFO - Epoch 3, Batch 99: Training loss = 0.0200
2025-04-03 18:08:03,334 - INFO - Epoch 3, Batch 100: Training loss = 0.0100
2025-04-03 18:08:03,435 - INFO - Epoch 3: Validation loss = 0.0800, Validation accuracy = 0.8400
2025-04-03 18:08:03,435 - INFO - Epoch 3: MSE = 0.0800, RMSE = 0.2828, MAE = 0.0640, RÂ² = 0.9200
2025-04-03 18:08:03,435 - INFO - Epoch 4 started.
2025-04-03 18:08:03,435 - INFO - Epoch 4, Batch 1: Training loss = 1.0000
2025-04-03 18:08:03,536 - INFO - Epoch 4, Batch 2: Training loss = 0.9900
2025-04-03 18:08:03,636 - INFO - Epoch 4, Batch 3: Training loss = 0.9800
2025-04-03 18:08:03,737 - INFO - Epoch 4, Batch 4: Training loss = 0.9700
2025-04-03 18:08:03,838 - INFO - Epoch 4, Batch 5: Training loss = 0.9600
2025-04-03 18:08:03,939 - INFO - Epoch 4, Batch 6: Training loss = 0.9500
2025-04-03 18:08:04,040 - INFO - Epoch 4, Batch 7: Training loss = 0.9400
2025-04-03 18:08:04,140 - INFO - Epoch 4, Batch 8: Training loss = 0.9300
2025-04-03 18:08:04,240 - INFO - Epoch 4, Batch 9: Training loss = 0.9200
2025-04-03 18:08:04,341 - INFO - Epoch 4, Batch 10: Training loss = 0.9100
2025-04-03 18:08:04,441 - INFO - Epoch 4, Batch 11: Training loss = 0.9000
2025-04-03 18:08:04,542 - INFO - Epoch 4, Batch 12: Training loss = 0.8900
2025-04-03 18:08:04,643 - INFO - Epoch 4, Batch 13: Training loss = 0.8800
2025-04-03 18:08:04,743 - INFO - Epoch 4, Batch 14: Training loss = 0.8700
2025-04-03 18:08:04,843 - INFO - Epoch 4, Batch 15: Training loss = 0.8600
2025-04-03 18:08:04,944 - INFO - Epoch 4, Batch 16: Training loss = 0.8500
2025-04-03 18:08:05,045 - INFO - Epoch 4, Batch 17: Training loss = 0.8400
2025-04-03 18:08:05,146 - INFO - Epoch 4, Batch 18: Training loss = 0.8300
2025-04-03 18:08:05,247 - INFO - Epoch 4, Batch 19: Training loss = 0.8200
2025-04-03 18:08:05,347 - INFO - Epoch 4, Batch 20: Training loss = 0.8100
2025-04-03 18:08:05,448 - INFO - Epoch 4, Batch 21: Training loss = 0.8000
2025-04-03 18:08:05,548 - INFO - Epoch 4, Batch 22: Training loss = 0.7900
2025-04-03 18:08:05,649 - INFO - Epoch 4, Batch 23: Training loss = 0.7800
2025-04-03 18:08:05,750 - INFO - Epoch 4, Batch 24: Training loss = 0.7700
2025-04-03 18:08:05,851 - INFO - Epoch 4, Batch 25: Training loss = 0.7600
2025-04-03 18:08:05,951 - INFO - Epoch 4, Batch 26: Training loss = 0.7500
2025-04-03 18:08:06,052 - INFO - Epoch 4, Batch 27: Training loss = 0.7400
2025-04-03 18:08:06,153 - INFO - Epoch 4, Batch 28: Training loss = 0.7300
2025-04-03 18:08:06,254 - INFO - Epoch 4, Batch 29: Training loss = 0.7200
2025-04-03 18:08:06,355 - INFO - Epoch 4, Batch 30: Training loss = 0.7100
2025-04-03 18:08:06,456 - INFO - Epoch 4, Batch 31: Training loss = 0.7000
2025-04-03 18:08:06,556 - INFO - Epoch 4, Batch 32: Training loss = 0.6900
2025-04-03 18:08:06,657 - INFO - Epoch 4, Batch 33: Training loss = 0.6800
2025-04-03 18:08:06,758 - INFO - Epoch 4, Batch 34: Training loss = 0.6700
2025-04-03 18:08:06,858 - INFO - Epoch 4, Batch 35: Training loss = 0.6600
2025-04-03 18:08:06,959 - INFO - Epoch 4, Batch 36: Training loss = 0.6500
2025-04-03 18:08:07,060 - INFO - Epoch 4, Batch 37: Training loss = 0.6400
2025-04-03 18:08:07,161 - INFO - Epoch 4, Batch 38: Training loss = 0.6300
2025-04-03 18:08:07,262 - INFO - Epoch 4, Batch 39: Training loss = 0.6200
2025-04-03 18:08:07,363 - INFO - Epoch 4, Batch 40: Training loss = 0.6100
2025-04-03 18:08:07,463 - INFO - Epoch 4, Batch 41: Training loss = 0.6000
2025-04-03 18:08:07,563 - INFO - Epoch 4, Batch 42: Training loss = 0.5900
2025-04-03 18:08:07,664 - INFO - Epoch 4, Batch 43: Training loss = 0.5800
2025-04-03 18:08:07,765 - INFO - Epoch 4, Batch 44: Training loss = 0.5700
2025-04-03 18:08:07,866 - INFO - Epoch 4, Batch 45: Training loss = 0.5600
2025-04-03 18:08:07,967 - INFO - Epoch 4, Batch 46: Training loss = 0.5500
2025-04-03 18:08:08,067 - INFO - Epoch 4, Batch 47: Training loss = 0.5400
2025-04-03 18:08:08,167 - INFO - Epoch 4, Batch 48: Training loss = 0.5300
2025-04-03 18:08:08,268 - INFO - Epoch 4, Batch 49: Training loss = 0.5200
2025-04-03 18:08:08,368 - INFO - Epoch 4, Batch 50: Training loss = 0.5100
2025-04-03 18:08:08,469 - INFO - Epoch 4, Batch 51: Training loss = 0.5000
2025-04-03 18:08:08,569 - INFO - Epoch 4, Batch 52: Training loss = 0.4900
2025-04-03 18:08:08,670 - INFO - Epoch 4, Batch 53: Training loss = 0.4800
2025-04-03 18:08:08,771 - INFO - Epoch 4, Batch 54: Training loss = 0.4700
2025-04-03 18:08:08,872 - INFO - Epoch 4, Batch 55: Training loss = 0.4600
2025-04-03 18:08:08,973 - INFO - Epoch 4, Batch 56: Training loss = 0.4500
2025-04-03 18:08:09,073 - INFO - Epoch 4, Batch 57: Training loss = 0.4400
2025-04-03 18:08:09,174 - INFO - Epoch 4, Batch 58: Training loss = 0.4300
2025-04-03 18:08:09,274 - INFO - Epoch 4, Batch 59: Training loss = 0.4200
2025-04-03 18:08:09,375 - INFO - Epoch 4, Batch 60: Training loss = 0.4100
2025-04-03 18:08:09,476 - INFO - Epoch 4, Batch 61: Training loss = 0.4000
2025-04-03 18:08:09,577 - INFO - Epoch 4, Batch 62: Training loss = 0.3900
2025-04-03 18:08:09,677 - INFO - Epoch 4, Batch 63: Training loss = 0.3800
2025-04-03 18:08:09,778 - INFO - Epoch 4, Batch 64: Training loss = 0.3700
2025-04-03 18:08:09,879 - INFO - Epoch 4, Batch 65: Training loss = 0.3600
2025-04-03 18:08:09,980 - INFO - Epoch 4, Batch 66: Training loss = 0.3500
2025-04-03 18:08:10,080 - INFO - Epoch 4, Batch 67: Training loss = 0.3400
2025-04-03 18:08:10,181 - INFO - Epoch 4, Batch 68: Training loss = 0.3300
2025-04-03 18:08:10,282 - INFO - Epoch 4, Batch 69: Training loss = 0.3200
2025-04-03 18:08:10,382 - INFO - Epoch 4, Batch 70: Training loss = 0.3100
2025-04-03 18:08:10,483 - INFO - Epoch 4, Batch 71: Training loss = 0.3000
2025-04-03 18:08:10,583 - INFO - Epoch 4, Batch 72: Training loss = 0.2900
2025-04-03 18:08:10,684 - INFO - Epoch 4, Batch 73: Training loss = 0.2800
2025-04-03 18:08:10,785 - INFO - Epoch 4, Batch 74: Training loss = 0.2700
2025-04-03 18:08:10,886 - INFO - Epoch 4, Batch 75: Training loss = 0.2600
2025-04-03 18:08:10,986 - INFO - Epoch 4, Batch 76: Training loss = 0.2500
2025-04-03 18:08:11,087 - INFO - Epoch 4, Batch 77: Training loss = 0.2400
2025-04-03 18:08:11,188 - INFO - Epoch 4, Batch 78: Training loss = 0.2300
2025-04-03 18:08:11,288 - INFO - Epoch 4, Batch 79: Training loss = 0.2200
2025-04-03 18:08:11,389 - INFO - Epoch 4, Batch 80: Training loss = 0.2100
2025-04-03 18:08:11,489 - INFO - Epoch 4, Batch 81: Training loss = 0.2000
2025-04-03 18:08:11,590 - INFO - Epoch 4, Batch 82: Training loss = 0.1900
2025-04-03 18:08:11,691 - INFO - Epoch 4, Batch 83: Training loss = 0.1800
2025-04-03 18:08:11,792 - INFO - Epoch 4, Batch 84: Training loss = 0.1700
2025-04-03 18:08:11,893 - INFO - Epoch 4, Batch 85: Training loss = 0.1600
2025-04-03 18:08:11,994 - INFO - Epoch 4, Batch 86: Training loss = 0.1500
2025-04-03 18:08:12,094 - INFO - Epoch 4, Batch 87: Training loss = 0.1400
2025-04-03 18:08:12,195 - INFO - Epoch 4, Batch 88: Training loss = 0.1300
2025-04-03 18:08:12,295 - INFO - Epoch 4, Batch 89: Training loss = 0.1200
2025-04-03 18:08:12,396 - INFO - Epoch 4, Batch 90: Training loss = 0.1100
2025-04-03 18:08:12,497 - INFO - Epoch 4, Batch 91: Training loss = 0.1000
2025-04-03 18:08:12,598 - INFO - Epoch 4, Batch 92: Training loss = 0.0900
2025-04-03 18:08:12,699 - INFO - Epoch 4, Batch 93: Training loss = 0.0800
2025-04-03 18:08:12,799 - INFO - Epoch 4, Batch 94: Training loss = 0.0700
2025-04-03 18:08:12,900 - INFO - Epoch 4, Batch 95: Training loss = 0.0600
2025-04-03 18:08:13,000 - INFO - Epoch 4, Batch 96: Training loss = 0.0500
2025-04-03 18:08:13,101 - INFO - Epoch 4, Batch 97: Training loss = 0.0400
2025-04-03 18:08:13,201 - INFO - Epoch 4, Batch 98: Training loss = 0.0300
2025-04-03 18:08:13,302 - INFO - Epoch 4, Batch 99: Training loss = 0.0200
2025-04-03 18:08:13,403 - INFO - Epoch 4, Batch 100: Training loss = 0.0100
2025-04-03 18:08:13,503 - INFO - Epoch 4: Validation loss = 0.0700, Validation accuracy = 0.8600
2025-04-03 18:08:13,504 - INFO - Epoch 4: MSE = 0.0700, RMSE = 0.2646, MAE = 0.0560, RÂ² = 0.9300
2025-04-03 18:08:13,505 - INFO - Epoch 5 started.
2025-04-03 18:08:13,505 - INFO - Epoch 5, Batch 1: Training loss = 1.0000
2025-04-03 18:08:13,605 - INFO - Epoch 5, Batch 2: Training loss = 0.9900
2025-04-03 18:08:13,706 - INFO - Epoch 5, Batch 3: Training loss = 0.9800
2025-04-03 18:08:13,806 - INFO - Epoch 5, Batch 4: Training loss = 0.9700
2025-04-03 18:08:13,907 - INFO - Epoch 5, Batch 5: Training loss = 0.9600
2025-04-03 18:08:14,008 - INFO - Epoch 5, Batch 6: Training loss = 0.9500
2025-04-03 18:08:14,109 - INFO - Epoch 5, Batch 7: Training loss = 0.9400
2025-04-03 18:08:14,210 - INFO - Epoch 5, Batch 8: Training loss = 0.9300
2025-04-03 18:08:14,311 - INFO - Epoch 5, Batch 9: Training loss = 0.9200
2025-04-03 18:08:14,412 - INFO - Epoch 5, Batch 10: Training loss = 0.9100
2025-04-03 18:08:14,513 - INFO - Epoch 5, Batch 11: Training loss = 0.9000
2025-04-03 18:08:14,614 - INFO - Epoch 5, Batch 12: Training loss = 0.8900
2025-04-03 18:08:14,714 - INFO - Epoch 5, Batch 13: Training loss = 0.8800
2025-04-03 18:08:14,815 - INFO - Epoch 5, Batch 14: Training loss = 0.8700
2025-04-03 18:08:14,916 - INFO - Epoch 5, Batch 15: Training loss = 0.8600
2025-04-03 18:08:15,016 - INFO - Epoch 5, Batch 16: Training loss = 0.8500
2025-04-03 18:08:15,117 - INFO - Epoch 5, Batch 17: Training loss = 0.8400
2025-04-03 18:08:15,218 - INFO - Epoch 5, Batch 18: Training loss = 0.8300
2025-04-03 18:08:15,319 - INFO - Epoch 5, Batch 19: Training loss = 0.8200
2025-04-03 18:08:15,420 - INFO - Epoch 5, Batch 20: Training loss = 0.8100
2025-04-03 18:08:15,520 - INFO - Epoch 5, Batch 21: Training loss = 0.8000
2025-04-03 18:08:15,621 - INFO - Epoch 5, Batch 22: Training loss = 0.7900
2025-04-03 18:08:15,722 - INFO - Epoch 5, Batch 23: Training loss = 0.7800
2025-04-03 18:08:15,823 - INFO - Epoch 5, Batch 24: Training loss = 0.7700
2025-04-03 18:08:15,923 - INFO - Epoch 5, Batch 25: Training loss = 0.7600
2025-04-03 18:08:16,024 - INFO - Epoch 5, Batch 26: Training loss = 0.7500
2025-04-03 18:08:16,125 - INFO - Epoch 5, Batch 27: Training loss = 0.7400
2025-04-03 18:08:16,225 - INFO - Epoch 5, Batch 28: Training loss = 0.7300
2025-04-03 18:08:16,326 - INFO - Epoch 5, Batch 29: Training loss = 0.7200
2025-04-03 18:08:16,427 - INFO - Epoch 5, Batch 30: Training loss = 0.7100
2025-04-03 18:08:16,528 - INFO - Epoch 5, Batch 31: Training loss = 0.7000
2025-04-03 18:08:16,628 - INFO - Epoch 5, Batch 32: Training loss = 0.6900
2025-04-03 18:08:16,729 - INFO - Epoch 5, Batch 33: Training loss = 0.6800
2025-04-03 18:08:16,830 - INFO - Epoch 5, Batch 34: Training loss = 0.6700
2025-04-03 18:08:16,930 - INFO - Epoch 5, Batch 35: Training loss = 0.6600
2025-04-03 18:08:17,031 - INFO - Epoch 5, Batch 36: Training loss = 0.6500
2025-04-03 18:08:17,132 - INFO - Epoch 5, Batch 37: Training loss = 0.6400
2025-04-03 18:08:17,232 - INFO - Epoch 5, Batch 38: Training loss = 0.6300
2025-04-03 18:08:17,333 - INFO - Epoch 5, Batch 39: Training loss = 0.6200
2025-04-03 18:08:17,434 - INFO - Epoch 5, Batch 40: Training loss = 0.6100
2025-04-03 18:08:17,535 - INFO - Epoch 5, Batch 41: Training loss = 0.6000
2025-04-03 18:08:17,635 - INFO - Epoch 5, Batch 42: Training loss = 0.5900
2025-04-03 18:08:17,736 - INFO - Epoch 5, Batch 43: Training loss = 0.5800
2025-04-03 18:08:17,837 - INFO - Epoch 5, Batch 44: Training loss = 0.5700
2025-04-03 18:08:17,937 - INFO - Epoch 5, Batch 45: Training loss = 0.5600
2025-04-03 18:08:18,038 - INFO - Epoch 5, Batch 46: Training loss = 0.5500
2025-04-03 18:08:18,139 - INFO - Epoch 5, Batch 47: Training loss = 0.5400
2025-04-03 18:08:18,239 - INFO - Epoch 5, Batch 48: Training loss = 0.5300
2025-04-03 18:08:18,340 - INFO - Epoch 5, Batch 49: Training loss = 0.5200
2025-04-03 18:08:18,441 - INFO - Epoch 5, Batch 50: Training loss = 0.5100
2025-04-03 18:08:18,542 - INFO - Epoch 5, Batch 51: Training loss = 0.5000
2025-04-03 18:08:18,643 - INFO - Epoch 5, Batch 52: Training loss = 0.4900
2025-04-03 18:08:18,745 - INFO - Epoch 5, Batch 53: Training loss = 0.4800
2025-04-03 18:08:18,846 - INFO - Epoch 5, Batch 54: Training loss = 0.4700
2025-04-03 18:08:18,946 - INFO - Epoch 5, Batch 55: Training loss = 0.4600
2025-04-03 18:08:19,047 - INFO - Epoch 5, Batch 56: Training loss = 0.4500
2025-04-03 18:08:19,148 - INFO - Epoch 5, Batch 57: Training loss = 0.4400
2025-04-03 18:08:19,248 - INFO - Epoch 5, Batch 58: Training loss = 0.4300
2025-04-03 18:08:19,349 - INFO - Epoch 5, Batch 59: Training loss = 0.4200
2025-04-03 18:08:19,450 - INFO - Epoch 5, Batch 60: Training loss = 0.4100
2025-04-03 18:08:19,551 - INFO - Epoch 5, Batch 61: Training loss = 0.4000
2025-04-03 18:08:19,651 - INFO - Epoch 5, Batch 62: Training loss = 0.3900
2025-04-03 18:08:19,752 - INFO - Epoch 5, Batch 63: Training loss = 0.3800
2025-04-03 18:08:19,853 - INFO - Epoch 5, Batch 64: Training loss = 0.3700
2025-04-03 18:08:19,953 - INFO - Epoch 5, Batch 65: Training loss = 0.3600
2025-04-03 18:08:20,054 - INFO - Epoch 5, Batch 66: Training loss = 0.3500
2025-04-03 18:08:20,155 - INFO - Epoch 5, Batch 67: Training loss = 0.3400
2025-04-03 18:08:20,256 - INFO - Epoch 5, Batch 68: Training loss = 0.3300
2025-04-03 18:08:20,356 - INFO - Epoch 5, Batch 69: Training loss = 0.3200
2025-04-03 18:08:20,457 - INFO - Epoch 5, Batch 70: Training loss = 0.3100
2025-04-03 18:08:20,558 - INFO - Epoch 5, Batch 71: Training loss = 0.3000
2025-04-03 18:08:20,659 - INFO - Epoch 5, Batch 72: Training loss = 0.2900
2025-04-03 18:08:20,760 - INFO - Epoch 5, Batch 73: Training loss = 0.2800
2025-04-03 18:08:20,861 - INFO - Epoch 5, Batch 74: Training loss = 0.2700
2025-04-03 18:08:20,965 - INFO - Epoch 5, Batch 75: Training loss = 0.2600
2025-04-03 18:08:21,066 - INFO - Epoch 5, Batch 76: Training loss = 0.2500
2025-04-03 18:08:21,166 - INFO - Epoch 5, Batch 77: Training loss = 0.2400
2025-04-03 18:08:21,267 - INFO - Epoch 5, Batch 78: Training loss = 0.2300
2025-04-03 18:08:21,368 - INFO - Epoch 5, Batch 79: Training loss = 0.2200
2025-04-03 18:08:21,469 - INFO - Epoch 5, Batch 80: Training loss = 0.2100
2025-04-03 18:08:21,570 - INFO - Epoch 5, Batch 81: Training loss = 0.2000
2025-04-03 18:08:21,671 - INFO - Epoch 5, Batch 82: Training loss = 0.1900
2025-04-03 18:08:21,772 - INFO - Epoch 5, Batch 83: Training loss = 0.1800
2025-04-03 18:08:21,872 - INFO - Epoch 5, Batch 84: Training loss = 0.1700
2025-04-03 18:08:21,973 - INFO - Epoch 5, Batch 85: Training loss = 0.1600
2025-04-03 18:08:22,073 - INFO - Epoch 5, Batch 86: Training loss = 0.1500
2025-04-03 18:08:22,174 - INFO - Epoch 5, Batch 87: Training loss = 0.1400
2025-04-03 18:08:22,275 - INFO - Epoch 5, Batch 88: Training loss = 0.1300
2025-04-03 18:08:22,376 - INFO - Epoch 5, Batch 89: Training loss = 0.1200
2025-04-03 18:08:22,477 - INFO - Epoch 5, Batch 90: Training loss = 0.1100
2025-04-03 18:08:22,578 - INFO - Epoch 5, Batch 91: Training loss = 0.1000
2025-04-03 18:08:22,679 - INFO - Epoch 5, Batch 92: Training loss = 0.0900
2025-04-03 18:08:22,780 - INFO - Epoch 5, Batch 93: Training loss = 0.0800
2025-04-03 18:08:22,880 - INFO - Epoch 5, Batch 94: Training loss = 0.0700
2025-04-03 18:08:22,981 - INFO - Epoch 5, Batch 95: Training loss = 0.0600
2025-04-03 18:08:23,082 - INFO - Epoch 5, Batch 96: Training loss = 0.0500
2025-04-03 18:08:23,183 - INFO - Epoch 5, Batch 97: Training loss = 0.0400
2025-04-03 18:08:23,284 - INFO - Epoch 5, Batch 98: Training loss = 0.0300
2025-04-03 18:08:23,385 - INFO - Epoch 5, Batch 99: Training loss = 0.0200
2025-04-03 18:08:23,486 - INFO - Epoch 5, Batch 100: Training loss = 0.0100
2025-04-03 18:08:23,587 - INFO - Epoch 5: Validation loss = 0.0600, Validation accuracy = 0.8800
2025-04-03 18:08:23,587 - INFO - Epoch 5: MSE = 0.0600, RMSE = 0.2449, MAE = 0.0480, RÂ² = 0.9400
2025-04-03 18:08:23,588 - INFO - Epoch 6 started.
2025-04-03 18:08:23,588 - INFO - Epoch 6, Batch 1: Training loss = 1.0000
2025-04-03 18:08:23,689 - INFO - Epoch 6, Batch 2: Training loss = 0.9900
2025-04-03 18:08:23,790 - INFO - Epoch 6, Batch 3: Training loss = 0.9800
2025-04-03 18:08:23,891 - INFO - Epoch 6, Batch 4: Training loss = 0.9700
2025-04-03 18:08:23,991 - INFO - Epoch 6, Batch 5: Training loss = 0.9600
2025-04-03 18:08:24,092 - INFO - Epoch 6, Batch 6: Training loss = 0.9500
2025-04-03 18:08:24,193 - INFO - Epoch 6, Batch 7: Training loss = 0.9400
2025-04-03 18:08:24,294 - INFO - Epoch 6, Batch 8: Training loss = 0.9300
2025-04-03 18:08:24,395 - INFO - Epoch 6, Batch 9: Training loss = 0.9200
2025-04-03 18:08:24,495 - INFO - Epoch 6, Batch 10: Training loss = 0.9100
2025-04-03 18:08:24,596 - INFO - Epoch 6, Batch 11: Training loss = 0.9000
2025-04-03 18:08:24,697 - INFO - Epoch 6, Batch 12: Training loss = 0.8900
2025-04-03 18:08:24,798 - INFO - Epoch 6, Batch 13: Training loss = 0.8800
2025-04-03 18:08:24,898 - INFO - Epoch 6, Batch 14: Training loss = 0.8700
2025-04-03 18:08:24,999 - INFO - Epoch 6, Batch 15: Training loss = 0.8600
2025-04-03 18:08:25,100 - INFO - Epoch 6, Batch 16: Training loss = 0.8500
2025-04-03 18:08:25,201 - INFO - Epoch 6, Batch 17: Training loss = 0.8400
2025-04-03 18:08:25,302 - INFO - Epoch 6, Batch 18: Training loss = 0.8300
2025-04-03 18:08:25,403 - INFO - Epoch 6, Batch 19: Training loss = 0.8200
2025-04-03 18:08:25,505 - INFO - Epoch 6, Batch 20: Training loss = 0.8100
2025-04-03 18:08:25,605 - INFO - Epoch 6, Batch 21: Training loss = 0.8000
2025-04-03 18:08:25,706 - INFO - Epoch 6, Batch 22: Training loss = 0.7900
2025-04-03 18:08:25,807 - INFO - Epoch 6, Batch 23: Training loss = 0.7800
2025-04-03 18:08:25,907 - INFO - Epoch 6, Batch 24: Training loss = 0.7700
2025-04-03 18:08:26,008 - INFO - Epoch 6, Batch 25: Training loss = 0.7600
2025-04-03 18:08:26,108 - INFO - Epoch 6, Batch 26: Training loss = 0.7500
2025-04-03 18:08:26,209 - INFO - Epoch 6, Batch 27: Training loss = 0.7400
2025-04-03 18:08:26,309 - INFO - Epoch 6, Batch 28: Training loss = 0.7300
2025-04-03 18:08:26,409 - INFO - Epoch 6, Batch 29: Training loss = 0.7200
2025-04-03 18:08:26,510 - INFO - Epoch 6, Batch 30: Training loss = 0.7100
2025-04-03 18:08:26,611 - INFO - Epoch 6, Batch 31: Training loss = 0.7000
2025-04-03 18:08:26,711 - INFO - Epoch 6, Batch 32: Training loss = 0.6900
2025-04-03 18:08:26,811 - INFO - Epoch 6, Batch 33: Training loss = 0.6800
2025-04-03 18:08:26,912 - INFO - Epoch 6, Batch 34: Training loss = 0.6700
2025-04-03 18:08:27,012 - INFO - Epoch 6, Batch 35: Training loss = 0.6600
2025-04-03 18:08:27,113 - INFO - Epoch 6, Batch 36: Training loss = 0.6500
2025-04-03 18:08:27,214 - INFO - Epoch 6, Batch 37: Training loss = 0.6400
2025-04-03 18:08:27,314 - INFO - Epoch 6, Batch 38: Training loss = 0.6300
2025-04-03 18:08:27,415 - INFO - Epoch 6, Batch 39: Training loss = 0.6200
2025-04-03 18:08:27,516 - INFO - Epoch 6, Batch 40: Training loss = 0.6100
2025-04-03 18:08:27,616 - INFO - Epoch 6, Batch 41: Training loss = 0.6000
2025-04-03 18:08:27,717 - INFO - Epoch 6, Batch 42: Training loss = 0.5900
2025-04-03 18:08:27,818 - INFO - Epoch 6, Batch 43: Training loss = 0.5800
2025-04-03 18:08:27,918 - INFO - Epoch 6, Batch 44: Training loss = 0.5700
2025-04-03 18:08:28,019 - INFO - Epoch 6, Batch 45: Training loss = 0.5600
2025-04-03 18:08:28,120 - INFO - Epoch 6, Batch 46: Training loss = 0.5500
2025-04-03 18:08:28,220 - INFO - Epoch 6, Batch 47: Training loss = 0.5400
2025-04-03 18:08:28,321 - INFO - Epoch 6, Batch 48: Training loss = 0.5300
2025-04-03 18:08:28,422 - INFO - Epoch 6, Batch 49: Training loss = 0.5200
2025-04-03 18:08:28,523 - INFO - Epoch 6, Batch 50: Training loss = 0.5100
2025-04-03 18:08:28,624 - INFO - Epoch 6, Batch 51: Training loss = 0.5000
2025-04-03 18:08:28,725 - INFO - Epoch 6, Batch 52: Training loss = 0.4900
2025-04-03 18:08:28,826 - INFO - Epoch 6, Batch 53: Training loss = 0.4800
2025-04-03 18:08:28,926 - INFO - Epoch 6, Batch 54: Training loss = 0.4700
2025-04-03 18:08:29,027 - INFO - Epoch 6, Batch 55: Training loss = 0.4600
2025-04-03 18:08:29,127 - INFO - Epoch 6, Batch 56: Training loss = 0.4500
2025-04-03 18:08:29,228 - INFO - Epoch 6, Batch 57: Training loss = 0.4400
2025-04-03 18:08:29,328 - INFO - Epoch 6, Batch 58: Training loss = 0.4300
2025-04-03 18:08:29,429 - INFO - Epoch 6, Batch 59: Training loss = 0.4200
2025-04-03 18:08:29,530 - INFO - Epoch 6, Batch 60: Training loss = 0.4100
2025-04-03 18:08:29,631 - INFO - Epoch 6, Batch 61: Training loss = 0.4000
2025-04-03 18:08:29,732 - INFO - Epoch 6, Batch 62: Training loss = 0.3900
2025-04-03 18:08:29,832 - INFO - Epoch 6, Batch 63: Training loss = 0.3800
2025-04-03 18:08:29,933 - INFO - Epoch 6, Batch 64: Training loss = 0.3700
2025-04-03 18:08:30,033 - INFO - Epoch 6, Batch 65: Training loss = 0.3600
2025-04-03 18:08:30,134 - INFO - Epoch 6, Batch 66: Training loss = 0.3500
2025-04-03 18:08:30,234 - INFO - Epoch 6, Batch 67: Training loss = 0.3400
2025-04-03 18:08:30,335 - INFO - Epoch 6, Batch 68: Training loss = 0.3300
2025-04-03 18:08:30,436 - INFO - Epoch 6, Batch 69: Training loss = 0.3200
2025-04-03 18:08:30,537 - INFO - Epoch 6, Batch 70: Training loss = 0.3100
2025-04-03 18:08:30,638 - INFO - Epoch 6, Batch 71: Training loss = 0.3000
2025-04-03 18:08:30,739 - INFO - Epoch 6, Batch 72: Training loss = 0.2900
2025-04-03 18:08:30,839 - INFO - Epoch 6, Batch 73: Training loss = 0.2800
2025-04-03 18:08:30,940 - INFO - Epoch 6, Batch 74: Training loss = 0.2700
2025-04-03 18:08:31,041 - INFO - Epoch 6, Batch 75: Training loss = 0.2600
2025-04-03 18:08:31,142 - INFO - Epoch 6, Batch 76: Training loss = 0.2500
2025-04-03 18:08:31,242 - INFO - Epoch 6, Batch 77: Training loss = 0.2400
2025-04-03 18:08:31,343 - INFO - Epoch 6, Batch 78: Training loss = 0.2300
2025-04-03 18:08:31,444 - INFO - Epoch 6, Batch 79: Training loss = 0.2200
2025-04-03 18:08:31,545 - INFO - Epoch 6, Batch 80: Training loss = 0.2100
2025-04-03 18:08:31,646 - INFO - Epoch 6, Batch 81: Training loss = 0.2000
2025-04-03 18:08:31,747 - INFO - Epoch 6, Batch 82: Training loss = 0.1900
2025-04-03 18:08:31,848 - INFO - Epoch 6, Batch 83: Training loss = 0.1800
2025-04-03 18:08:31,948 - INFO - Epoch 6, Batch 84: Training loss = 0.1700
2025-04-03 18:08:32,049 - INFO - Epoch 6, Batch 85: Training loss = 0.1600
2025-04-03 18:08:32,149 - INFO - Epoch 6, Batch 86: Training loss = 0.1500
2025-04-03 18:08:32,250 - INFO - Epoch 6, Batch 87: Training loss = 0.1400
2025-04-03 18:08:32,350 - INFO - Epoch 6, Batch 88: Training loss = 0.1300
2025-04-03 18:08:32,451 - INFO - Epoch 6, Batch 89: Training loss = 0.1200
2025-04-03 18:08:32,552 - INFO - Epoch 6, Batch 90: Training loss = 0.1100
2025-04-03 18:08:32,653 - INFO - Epoch 6, Batch 91: Training loss = 0.1000
2025-04-03 18:08:32,754 - INFO - Epoch 6, Batch 92: Training loss = 0.0900
2025-04-03 18:08:32,854 - INFO - Epoch 6, Batch 93: Training loss = 0.0800
2025-04-03 18:08:32,955 - INFO - Epoch 6, Batch 94: Training loss = 0.0700
2025-04-03 18:08:33,056 - INFO - Epoch 6, Batch 95: Training loss = 0.0600
2025-04-03 18:08:33,156 - INFO - Epoch 6, Batch 96: Training loss = 0.0500
2025-04-03 18:08:33,257 - INFO - Epoch 6, Batch 97: Training loss = 0.0400
2025-04-03 18:08:33,358 - INFO - Epoch 6, Batch 98: Training loss = 0.0300
2025-04-03 18:08:33,459 - INFO - Epoch 6, Batch 99: Training loss = 0.0200
2025-04-03 18:08:33,560 - INFO - Epoch 6, Batch 100: Training loss = 0.0100
2025-04-03 18:08:33,661 - INFO - Epoch 6: Validation loss = 0.0500, Validation accuracy = 0.9000
2025-04-03 18:08:33,662 - INFO - Epoch 6: MSE = 0.0500, RMSE = 0.2236, MAE = 0.0400, RÂ² = 0.9500
2025-04-03 18:08:33,662 - INFO - Epoch 7 started.
2025-04-03 18:08:33,662 - INFO - Epoch 7, Batch 1: Training loss = 1.0000
2025-04-03 18:08:33,763 - INFO - Epoch 7, Batch 2: Training loss = 0.9900
2025-04-03 18:08:33,864 - INFO - Epoch 7, Batch 3: Training loss = 0.9800
2025-04-03 18:08:33,965 - INFO - Epoch 7, Batch 4: Training loss = 0.9700
2025-04-03 18:08:34,066 - INFO - Epoch 7, Batch 5: Training loss = 0.9600
2025-04-03 18:08:34,166 - INFO - Epoch 7, Batch 6: Training loss = 0.9500
2025-04-03 18:08:34,267 - INFO - Epoch 7, Batch 7: Training loss = 0.9400
2025-04-03 18:08:34,368 - INFO - Epoch 7, Batch 8: Training loss = 0.9300
2025-04-03 18:08:34,469 - INFO - Epoch 7, Batch 9: Training loss = 0.9200
2025-04-03 18:08:34,569 - INFO - Epoch 7, Batch 10: Training loss = 0.9100
2025-04-03 18:08:34,670 - INFO - Epoch 7, Batch 11: Training loss = 0.9000
2025-04-03 18:08:34,771 - INFO - Epoch 7, Batch 12: Training loss = 0.8900
2025-04-03 18:08:34,872 - INFO - Epoch 7, Batch 13: Training loss = 0.8800
2025-04-03 18:08:34,974 - INFO - Epoch 7, Batch 14: Training loss = 0.8700
2025-04-03 18:08:35,075 - INFO - Epoch 7, Batch 15: Training loss = 0.8600
2025-04-03 18:08:35,175 - INFO - Epoch 7, Batch 16: Training loss = 0.8500
2025-04-03 18:08:35,276 - INFO - Epoch 7, Batch 17: Training loss = 0.8400
2025-04-03 18:08:35,376 - INFO - Epoch 7, Batch 18: Training loss = 0.8300
2025-04-03 18:08:35,477 - INFO - Epoch 7, Batch 19: Training loss = 0.8200
2025-04-03 18:08:35,577 - INFO - Epoch 7, Batch 20: Training loss = 0.8100
2025-04-03 18:08:35,678 - INFO - Epoch 7, Batch 21: Training loss = 0.8000
2025-04-03 18:08:35,780 - INFO - Epoch 7, Batch 22: Training loss = 0.7900
2025-04-03 18:08:35,880 - INFO - Epoch 7, Batch 23: Training loss = 0.7800
2025-04-03 18:08:35,981 - INFO - Epoch 7, Batch 24: Training loss = 0.7700
2025-04-03 18:08:36,082 - INFO - Epoch 7, Batch 25: Training loss = 0.7600
2025-04-03 18:08:36,182 - INFO - Epoch 7, Batch 26: Training loss = 0.7500
2025-04-03 18:08:36,283 - INFO - Epoch 7, Batch 27: Training loss = 0.7400
2025-04-03 18:08:36,384 - INFO - Epoch 7, Batch 28: Training loss = 0.7300
2025-04-03 18:08:36,485 - INFO - Epoch 7, Batch 29: Training loss = 0.7200
2025-04-03 18:08:36,585 - INFO - Epoch 7, Batch 30: Training loss = 0.7100
2025-04-03 18:08:36,686 - INFO - Epoch 7, Batch 31: Training loss = 0.7000
2025-04-03 18:08:36,787 - INFO - Epoch 7, Batch 32: Training loss = 0.6900
2025-04-03 18:08:36,888 - INFO - Epoch 7, Batch 33: Training loss = 0.6800
2025-04-03 18:08:36,989 - INFO - Epoch 7, Batch 34: Training loss = 0.6700
2025-04-03 18:08:37,090 - INFO - Epoch 7, Batch 35: Training loss = 0.6600
2025-04-03 18:08:37,191 - INFO - Epoch 7, Batch 36: Training loss = 0.6500
2025-04-03 18:08:37,292 - INFO - Epoch 7, Batch 37: Training loss = 0.6400
2025-04-03 18:08:37,393 - INFO - Epoch 7, Batch 38: Training loss = 0.6300
2025-04-03 18:08:37,492 - INFO - Epoch 7, Batch 39: Training loss = 0.6200
2025-04-03 18:08:37,593 - INFO - Epoch 7, Batch 40: Training loss = 0.6100
2025-04-03 18:08:37,693 - INFO - Epoch 7, Batch 41: Training loss = 0.6000
2025-04-03 18:08:37,794 - INFO - Epoch 7, Batch 42: Training loss = 0.5900
2025-04-03 18:08:37,894 - INFO - Epoch 7, Batch 43: Training loss = 0.5800
2025-04-03 18:08:37,995 - INFO - Epoch 7, Batch 44: Training loss = 0.5700
2025-04-03 18:08:38,096 - INFO - Epoch 7, Batch 45: Training loss = 0.5600
2025-04-03 18:08:38,196 - INFO - Epoch 7, Batch 46: Training loss = 0.5500
2025-04-03 18:08:38,297 - INFO - Epoch 7, Batch 47: Training loss = 0.5400
2025-04-03 18:08:38,397 - INFO - Epoch 7, Batch 48: Training loss = 0.5300
2025-04-03 18:08:38,498 - INFO - Epoch 7, Batch 49: Training loss = 0.5200
2025-04-03 18:08:38,598 - INFO - Epoch 7, Batch 50: Training loss = 0.5100
2025-04-03 18:08:38,699 - INFO - Epoch 7, Batch 51: Training loss = 0.5000
2025-04-03 18:08:38,800 - INFO - Epoch 7, Batch 52: Training loss = 0.4900
2025-04-03 18:08:38,900 - INFO - Epoch 7, Batch 53: Training loss = 0.4800
2025-04-03 18:08:39,001 - INFO - Epoch 7, Batch 54: Training loss = 0.4700
2025-04-03 18:08:39,102 - INFO - Epoch 7, Batch 55: Training loss = 0.4600
2025-04-03 18:08:39,203 - INFO - Epoch 7, Batch 56: Training loss = 0.4500
2025-04-03 18:08:39,303 - INFO - Epoch 7, Batch 57: Training loss = 0.4400
2025-04-03 18:08:39,404 - INFO - Epoch 7, Batch 58: Training loss = 0.4300
2025-04-03 18:08:39,505 - INFO - Epoch 7, Batch 59: Training loss = 0.4200
2025-04-03 18:08:39,606 - INFO - Epoch 7, Batch 60: Training loss = 0.4100
2025-04-03 18:08:39,707 - INFO - Epoch 7, Batch 61: Training loss = 0.4000
2025-04-03 18:08:39,808 - INFO - Epoch 7, Batch 62: Training loss = 0.3900
2025-04-03 18:08:39,909 - INFO - Epoch 7, Batch 63: Training loss = 0.3800
2025-04-03 18:08:40,009 - INFO - Epoch 7, Batch 64: Training loss = 0.3700
2025-04-03 18:08:40,110 - INFO - Epoch 7, Batch 65: Training loss = 0.3600
2025-04-03 18:08:40,211 - INFO - Epoch 7, Batch 66: Training loss = 0.3500
2025-04-03 18:08:40,312 - INFO - Epoch 7, Batch 67: Training loss = 0.3400
2025-04-03 18:08:40,413 - INFO - Epoch 7, Batch 68: Training loss = 0.3300
2025-04-03 18:08:40,513 - INFO - Epoch 7, Batch 69: Training loss = 0.3200
2025-04-03 18:08:40,614 - INFO - Epoch 7, Batch 70: Training loss = 0.3100
2025-04-03 18:08:40,714 - INFO - Epoch 7, Batch 71: Training loss = 0.3000
2025-04-03 18:08:40,815 - INFO - Epoch 7, Batch 72: Training loss = 0.2900
2025-04-03 18:08:40,916 - INFO - Epoch 7, Batch 73: Training loss = 0.2800
2025-04-03 18:08:41,017 - INFO - Epoch 7, Batch 74: Training loss = 0.2700
2025-04-03 18:08:41,117 - INFO - Epoch 7, Batch 75: Training loss = 0.2600
2025-04-03 18:08:41,218 - INFO - Epoch 7, Batch 76: Training loss = 0.2500
2025-04-03 18:08:41,319 - INFO - Epoch 7, Batch 77: Training loss = 0.2400
2025-04-03 18:08:41,420 - INFO - Epoch 7, Batch 78: Training loss = 0.2300
2025-04-03 18:08:41,521 - INFO - Epoch 7, Batch 79: Training loss = 0.2200
2025-04-03 18:08:41,622 - INFO - Epoch 7, Batch 80: Training loss = 0.2100
2025-04-03 18:08:41,723 - INFO - Epoch 7, Batch 81: Training loss = 0.2000
2025-04-03 18:08:41,823 - INFO - Epoch 7, Batch 82: Training loss = 0.1900
2025-04-03 18:08:41,924 - INFO - Epoch 7, Batch 83: Training loss = 0.1800
2025-04-03 18:08:42,025 - INFO - Epoch 7, Batch 84: Training loss = 0.1700
2025-04-03 18:08:42,125 - INFO - Epoch 7, Batch 85: Training loss = 0.1600
2025-04-03 18:08:42,226 - INFO - Epoch 7, Batch 86: Training loss = 0.1500
2025-04-03 18:08:42,327 - INFO - Epoch 7, Batch 87: Training loss = 0.1400
2025-04-03 18:08:42,427 - INFO - Epoch 7, Batch 88: Training loss = 0.1300
2025-04-03 18:08:42,528 - INFO - Epoch 7, Batch 89: Training loss = 0.1200
2025-04-03 18:08:42,629 - INFO - Epoch 7, Batch 90: Training loss = 0.1100
2025-04-03 18:08:42,730 - INFO - Epoch 7, Batch 91: Training loss = 0.1000
2025-04-03 18:08:42,831 - INFO - Epoch 7, Batch 92: Training loss = 0.0900
2025-04-03 18:08:42,932 - INFO - Epoch 7, Batch 93: Training loss = 0.0800
2025-04-03 18:08:43,033 - INFO - Epoch 7, Batch 94: Training loss = 0.0700
2025-04-03 18:08:43,134 - INFO - Epoch 7, Batch 95: Training loss = 0.0600
2025-04-03 18:08:43,235 - INFO - Epoch 7, Batch 96: Training loss = 0.0500
2025-04-03 18:08:43,336 - INFO - Epoch 7, Batch 97: Training loss = 0.0400
2025-04-03 18:08:43,437 - INFO - Epoch 7, Batch 98: Training loss = 0.0300
2025-04-03 18:08:43,538 - INFO - Epoch 7, Batch 99: Training loss = 0.0200
2025-04-03 18:08:43,639 - INFO - Epoch 7, Batch 100: Training loss = 0.0100
2025-04-03 18:08:43,740 - INFO - Epoch 7: Validation loss = 0.0400, Validation accuracy = 0.9200
2025-04-03 18:08:43,741 - INFO - Epoch 7: MSE = 0.0400, RMSE = 0.2000, MAE = 0.0320, RÂ² = 0.9600
2025-04-03 18:08:43,741 - INFO - Epoch 8 started.
2025-04-03 18:08:43,742 - INFO - Epoch 8, Batch 1: Training loss = 1.0000
2025-04-03 18:08:43,842 - INFO - Epoch 8, Batch 2: Training loss = 0.9900
2025-04-03 18:08:43,943 - INFO - Epoch 8, Batch 3: Training loss = 0.9800
2025-04-03 18:08:44,044 - INFO - Epoch 8, Batch 4: Training loss = 0.9700
2025-04-03 18:08:44,144 - INFO - Epoch 8, Batch 5: Training loss = 0.9600
2025-04-03 18:08:44,245 - INFO - Epoch 8, Batch 6: Training loss = 0.9500
2025-04-03 18:08:44,346 - INFO - Epoch 8, Batch 7: Training loss = 0.9400
2025-04-03 18:08:44,446 - INFO - Epoch 8, Batch 8: Training loss = 0.9300
2025-04-03 18:08:44,547 - INFO - Epoch 8, Batch 9: Training loss = 0.9200
2025-04-03 18:08:44,648 - INFO - Epoch 8, Batch 10: Training loss = 0.9100
2025-04-03 18:08:44,748 - INFO - Epoch 8, Batch 11: Training loss = 0.9000
2025-04-03 18:08:44,850 - INFO - Epoch 8, Batch 12: Training loss = 0.8900
2025-04-03 18:08:44,951 - INFO - Epoch 8, Batch 13: Training loss = 0.8800
2025-04-03 18:08:45,052 - INFO - Epoch 8, Batch 14: Training loss = 0.8700
2025-04-03 18:08:45,152 - INFO - Epoch 8, Batch 15: Training loss = 0.8600
2025-04-03 18:08:45,253 - INFO - Epoch 8, Batch 16: Training loss = 0.8500
2025-04-03 18:08:45,353 - INFO - Epoch 8, Batch 17: Training loss = 0.8400
2025-04-03 18:08:45,454 - INFO - Epoch 8, Batch 18: Training loss = 0.8300
2025-04-03 18:08:45,555 - INFO - Epoch 8, Batch 19: Training loss = 0.8200
2025-04-03 18:08:45,655 - INFO - Epoch 8, Batch 20: Training loss = 0.8100
2025-04-03 18:08:45,756 - INFO - Epoch 8, Batch 21: Training loss = 0.8000
2025-04-03 18:08:45,856 - INFO - Epoch 8, Batch 22: Training loss = 0.7900
2025-04-03 18:08:45,957 - INFO - Epoch 8, Batch 23: Training loss = 0.7800
2025-04-03 18:08:46,057 - INFO - Epoch 8, Batch 24: Training loss = 0.7700
2025-04-03 18:08:46,158 - INFO - Epoch 8, Batch 25: Training loss = 0.7600
2025-04-03 18:08:46,258 - INFO - Epoch 8, Batch 26: Training loss = 0.7500
2025-04-03 18:08:46,359 - INFO - Epoch 8, Batch 27: Training loss = 0.7400
2025-04-03 18:08:46,460 - INFO - Epoch 8, Batch 28: Training loss = 0.7300
2025-04-03 18:08:46,561 - INFO - Epoch 8, Batch 29: Training loss = 0.7200
2025-04-03 18:08:46,662 - INFO - Epoch 8, Batch 30: Training loss = 0.7100
2025-04-03 18:08:46,763 - INFO - Epoch 8, Batch 31: Training loss = 0.7000
2025-04-03 18:08:46,864 - INFO - Epoch 8, Batch 32: Training loss = 0.6900
2025-04-03 18:08:46,965 - INFO - Epoch 8, Batch 33: Training loss = 0.6800
2025-04-03 18:08:47,066 - INFO - Epoch 8, Batch 34: Training loss = 0.6700
2025-04-03 18:08:47,166 - INFO - Epoch 8, Batch 35: Training loss = 0.6600
2025-04-03 18:08:47,267 - INFO - Epoch 8, Batch 36: Training loss = 0.6500
2025-04-03 18:08:47,368 - INFO - Epoch 8, Batch 37: Training loss = 0.6400
2025-04-03 18:08:47,469 - INFO - Epoch 8, Batch 38: Training loss = 0.6300
2025-04-03 18:08:47,570 - INFO - Epoch 8, Batch 39: Training loss = 0.6200
2025-04-03 18:08:47,671 - INFO - Epoch 8, Batch 40: Training loss = 0.6100
2025-04-03 18:08:47,772 - INFO - Epoch 8, Batch 41: Training loss = 0.6000
2025-04-03 18:08:47,873 - INFO - Epoch 8, Batch 42: Training loss = 0.5900
2025-04-03 18:08:47,973 - INFO - Epoch 8, Batch 43: Training loss = 0.5800
2025-04-03 18:08:48,074 - INFO - Epoch 8, Batch 44: Training loss = 0.5700
2025-04-03 18:08:48,174 - INFO - Epoch 8, Batch 45: Training loss = 0.5600
2025-04-03 18:08:48,275 - INFO - Epoch 8, Batch 46: Training loss = 0.5500
2025-04-03 18:08:48,376 - INFO - Epoch 8, Batch 47: Training loss = 0.5400
2025-04-03 18:08:48,477 - INFO - Epoch 8, Batch 48: Training loss = 0.5300
2025-04-03 18:08:48,577 - INFO - Epoch 8, Batch 49: Training loss = 0.5200
2025-04-03 18:08:48,678 - INFO - Epoch 8, Batch 50: Training loss = 0.5100
2025-04-03 18:08:48,779 - INFO - Epoch 8, Batch 51: Training loss = 0.5000
2025-04-03 18:08:48,880 - INFO - Epoch 8, Batch 52: Training loss = 0.4900
2025-04-03 18:08:48,980 - INFO - Epoch 8, Batch 53: Training loss = 0.4800
2025-04-03 18:08:49,081 - INFO - Epoch 8, Batch 54: Training loss = 0.4700
2025-04-03 18:08:49,182 - INFO - Epoch 8, Batch 55: Training loss = 0.4600
2025-04-03 18:08:49,283 - INFO - Epoch 8, Batch 56: Training loss = 0.4500
2025-04-03 18:08:49,384 - INFO - Epoch 8, Batch 57: Training loss = 0.4400
2025-04-03 18:08:49,486 - INFO - Epoch 8, Batch 58: Training loss = 0.4300
2025-04-03 18:08:49,587 - INFO - Epoch 8, Batch 59: Training loss = 0.4200
2025-04-03 18:08:49,688 - INFO - Epoch 8, Batch 60: Training loss = 0.4100
2025-04-03 18:08:49,789 - INFO - Epoch 8, Batch 61: Training loss = 0.4000
2025-04-03 18:08:49,890 - INFO - Epoch 8, Batch 62: Training loss = 0.3900
2025-04-03 18:08:49,990 - INFO - Epoch 8, Batch 63: Training loss = 0.3800
2025-04-03 18:08:50,091 - INFO - Epoch 8, Batch 64: Training loss = 0.3700
2025-04-03 18:08:50,192 - INFO - Epoch 8, Batch 65: Training loss = 0.3600
2025-04-03 18:08:50,293 - INFO - Epoch 8, Batch 66: Training loss = 0.3500
2025-04-03 18:08:50,394 - INFO - Epoch 8, Batch 67: Training loss = 0.3400
2025-04-03 18:08:50,494 - INFO - Epoch 8, Batch 68: Training loss = 0.3300
2025-04-03 18:08:50,595 - INFO - Epoch 8, Batch 69: Training loss = 0.3200
2025-04-03 18:08:50,695 - INFO - Epoch 8, Batch 70: Training loss = 0.3100
2025-04-03 18:08:50,796 - INFO - Epoch 8, Batch 71: Training loss = 0.3000
2025-04-03 18:08:50,897 - INFO - Epoch 8, Batch 72: Training loss = 0.2900
2025-04-03 18:08:50,997 - INFO - Epoch 8, Batch 73: Training loss = 0.2800
2025-04-03 18:08:51,098 - INFO - Epoch 8, Batch 74: Training loss = 0.2700
2025-04-03 18:08:51,198 - INFO - Epoch 8, Batch 75: Training loss = 0.2600
2025-04-03 18:08:51,299 - INFO - Epoch 8, Batch 76: Training loss = 0.2500
2025-04-03 18:08:51,400 - INFO - Epoch 8, Batch 77: Training loss = 0.2400
2025-04-03 18:08:51,501 - INFO - Epoch 8, Batch 78: Training loss = 0.2300
2025-04-03 18:08:51,602 - INFO - Epoch 8, Batch 79: Training loss = 0.2200
2025-04-03 18:08:51,703 - INFO - Epoch 8, Batch 80: Training loss = 0.2100
2025-04-03 18:08:51,804 - INFO - Epoch 8, Batch 81: Training loss = 0.2000
2025-04-03 18:08:51,904 - INFO - Epoch 8, Batch 82: Training loss = 0.1900
2025-04-03 18:08:52,005 - INFO - Epoch 8, Batch 83: Training loss = 0.1800
2025-04-03 18:08:52,106 - INFO - Epoch 8, Batch 84: Training loss = 0.1700
2025-04-03 18:08:52,207 - INFO - Epoch 8, Batch 85: Training loss = 0.1600
2025-04-03 18:08:52,308 - INFO - Epoch 8, Batch 86: Training loss = 0.1500
2025-04-03 18:08:52,409 - INFO - Epoch 8, Batch 87: Training loss = 0.1400
2025-04-03 18:08:52,510 - INFO - Epoch 8, Batch 88: Training loss = 0.1300
2025-04-03 18:08:52,611 - INFO - Epoch 8, Batch 89: Training loss = 0.1200
2025-04-03 18:08:52,712 - INFO - Epoch 8, Batch 90: Training loss = 0.1100
2025-04-03 18:08:52,813 - INFO - Epoch 8, Batch 91: Training loss = 0.1000
2025-04-03 18:08:52,914 - INFO - Epoch 8, Batch 92: Training loss = 0.0900
2025-04-03 18:08:53,015 - INFO - Epoch 8, Batch 93: Training loss = 0.0800
2025-04-03 18:08:53,115 - INFO - Epoch 8, Batch 94: Training loss = 0.0700
2025-04-03 18:08:53,216 - INFO - Epoch 8, Batch 95: Training loss = 0.0600
2025-04-03 18:08:53,316 - INFO - Epoch 8, Batch 96: Training loss = 0.0500
2025-04-03 18:08:53,418 - INFO - Epoch 8, Batch 97: Training loss = 0.0400
2025-04-03 18:08:53,519 - INFO - Epoch 8, Batch 98: Training loss = 0.0300
2025-04-03 18:08:53,620 - INFO - Epoch 8, Batch 99: Training loss = 0.0200
2025-04-03 18:08:53,721 - INFO - Epoch 8, Batch 100: Training loss = 0.0100
2025-04-03 18:08:53,821 - INFO - Epoch 8: Validation loss = 0.0300, Validation accuracy = 0.9400
2025-04-03 18:08:53,822 - INFO - Epoch 8: MSE = 0.0300, RMSE = 0.1732, MAE = 0.0240, RÂ² = 0.9700
2025-04-03 18:08:53,822 - INFO - Epoch 9 started.
2025-04-03 18:08:53,822 - INFO - Epoch 9, Batch 1: Training loss = 1.0000
2025-04-03 18:08:53,923 - INFO - Epoch 9, Batch 2: Training loss = 0.9900
2025-04-03 18:08:54,024 - INFO - Epoch 9, Batch 3: Training loss = 0.9800
2025-04-03 18:08:54,125 - INFO - Epoch 9, Batch 4: Training loss = 0.9700
2025-04-03 18:08:54,225 - INFO - Epoch 9, Batch 5: Training loss = 0.9600
2025-04-03 18:08:54,326 - INFO - Epoch 9, Batch 6: Training loss = 0.9500
2025-04-03 18:08:54,427 - INFO - Epoch 9, Batch 7: Training loss = 0.9400
2025-04-03 18:08:54,528 - INFO - Epoch 9, Batch 8: Training loss = 0.9300
2025-04-03 18:08:54,629 - INFO - Epoch 9, Batch 9: Training loss = 0.9200
2025-04-03 18:08:54,729 - INFO - Epoch 9, Batch 10: Training loss = 0.9100
2025-04-03 18:08:54,830 - INFO - Epoch 9, Batch 11: Training loss = 0.9000
2025-04-03 18:08:54,931 - INFO - Epoch 9, Batch 12: Training loss = 0.8900
2025-04-03 18:08:55,032 - INFO - Epoch 9, Batch 13: Training loss = 0.8800
2025-04-03 18:08:55,132 - INFO - Epoch 9, Batch 14: Training loss = 0.8700
2025-04-03 18:08:55,233 - INFO - Epoch 9, Batch 15: Training loss = 0.8600
2025-04-03 18:08:55,334 - INFO - Epoch 9, Batch 16: Training loss = 0.8500
2025-04-03 18:08:55,435 - INFO - Epoch 9, Batch 17: Training loss = 0.8400
2025-04-03 18:08:55,536 - INFO - Epoch 9, Batch 18: Training loss = 0.8300
2025-04-03 18:08:55,636 - INFO - Epoch 9, Batch 19: Training loss = 0.8200
2025-04-03 18:08:55,738 - INFO - Epoch 9, Batch 20: Training loss = 0.8100
2025-04-03 18:08:55,838 - INFO - Epoch 9, Batch 21: Training loss = 0.8000
2025-04-03 18:08:55,939 - INFO - Epoch 9, Batch 22: Training loss = 0.7900
2025-04-03 18:08:56,040 - INFO - Epoch 9, Batch 23: Training loss = 0.7800
2025-04-03 18:08:56,141 - INFO - Epoch 9, Batch 24: Training loss = 0.7700
2025-04-03 18:08:56,242 - INFO - Epoch 9, Batch 25: Training loss = 0.7600
2025-04-03 18:08:56,343 - INFO - Epoch 9, Batch 26: Training loss = 0.7500
2025-04-03 18:08:56,444 - INFO - Epoch 9, Batch 27: Training loss = 0.7400
2025-04-03 18:08:56,545 - INFO - Epoch 9, Batch 28: Training loss = 0.7300
2025-04-03 18:08:56,646 - INFO - Epoch 9, Batch 29: Training loss = 0.7200
2025-04-03 18:08:56,746 - INFO - Epoch 9, Batch 30: Training loss = 0.7100
2025-04-03 18:08:56,847 - INFO - Epoch 9, Batch 31: Training loss = 0.7000
2025-04-03 18:08:56,948 - INFO - Epoch 9, Batch 32: Training loss = 0.6900
2025-04-03 18:08:57,048 - INFO - Epoch 9, Batch 33: Training loss = 0.6800
2025-04-03 18:08:57,149 - INFO - Epoch 9, Batch 34: Training loss = 0.6700
2025-04-03 18:08:57,250 - INFO - Epoch 9, Batch 35: Training loss = 0.6600
2025-04-03 18:08:57,351 - INFO - Epoch 9, Batch 36: Training loss = 0.6500
2025-04-03 18:08:57,452 - INFO - Epoch 9, Batch 37: Training loss = 0.6400
2025-04-03 18:08:57,553 - INFO - Epoch 9, Batch 38: Training loss = 0.6300
2025-04-03 18:08:57,654 - INFO - Epoch 9, Batch 39: Training loss = 0.6200
2025-04-03 18:08:57,754 - INFO - Epoch 9, Batch 40: Training loss = 0.6100
2025-04-03 18:08:57,855 - INFO - Epoch 9, Batch 41: Training loss = 0.6000
2025-04-03 18:08:57,956 - INFO - Epoch 9, Batch 42: Training loss = 0.5900
2025-04-03 18:08:58,057 - INFO - Epoch 9, Batch 43: Training loss = 0.5800
2025-04-03 18:08:58,158 - INFO - Epoch 9, Batch 44: Training loss = 0.5700
2025-04-03 18:08:58,258 - INFO - Epoch 9, Batch 45: Training loss = 0.5600
2025-04-03 18:08:58,359 - INFO - Epoch 9, Batch 46: Training loss = 0.5500
2025-04-03 18:08:58,460 - INFO - Epoch 9, Batch 47: Training loss = 0.5400
2025-04-03 18:08:58,561 - INFO - Epoch 9, Batch 48: Training loss = 0.5300
2025-04-03 18:08:58,662 - INFO - Epoch 9, Batch 49: Training loss = 0.5200
2025-04-03 18:08:58,763 - INFO - Epoch 9, Batch 50: Training loss = 0.5100
2025-04-03 18:08:58,863 - INFO - Epoch 9, Batch 51: Training loss = 0.5000
2025-04-03 18:08:58,964 - INFO - Epoch 9, Batch 52: Training loss = 0.4900
2025-04-03 18:08:59,065 - INFO - Epoch 9, Batch 53: Training loss = 0.4800
2025-04-03 18:08:59,166 - INFO - Epoch 9, Batch 54: Training loss = 0.4700
2025-04-03 18:08:59,267 - INFO - Epoch 9, Batch 55: Training loss = 0.4600
2025-04-03 18:08:59,367 - INFO - Epoch 9, Batch 56: Training loss = 0.4500
2025-04-03 18:08:59,468 - INFO - Epoch 9, Batch 57: Training loss = 0.4400
2025-04-03 18:08:59,570 - INFO - Epoch 9, Batch 58: Training loss = 0.4300
2025-04-03 18:08:59,670 - INFO - Epoch 9, Batch 59: Training loss = 0.4200
2025-04-03 18:08:59,771 - INFO - Epoch 9, Batch 60: Training loss = 0.4100
2025-04-03 18:08:59,872 - INFO - Epoch 9, Batch 61: Training loss = 0.4000
2025-04-03 18:08:59,973 - INFO - Epoch 9, Batch 62: Training loss = 0.3900
2025-04-03 18:09:00,074 - INFO - Epoch 9, Batch 63: Training loss = 0.3800
2025-04-03 18:09:00,174 - INFO - Epoch 9, Batch 64: Training loss = 0.3700
2025-04-03 18:09:00,275 - INFO - Epoch 9, Batch 65: Training loss = 0.3600
2025-04-03 18:09:00,376 - INFO - Epoch 9, Batch 66: Training loss = 0.3500
2025-04-03 18:09:00,477 - INFO - Epoch 9, Batch 67: Training loss = 0.3400
2025-04-03 18:09:00,578 - INFO - Epoch 9, Batch 68: Training loss = 0.3300
2025-04-03 18:09:00,678 - INFO - Epoch 9, Batch 69: Training loss = 0.3200
2025-04-03 18:09:00,779 - INFO - Epoch 9, Batch 70: Training loss = 0.3100
2025-04-03 18:09:00,880 - INFO - Epoch 9, Batch 71: Training loss = 0.3000
2025-04-03 18:09:00,981 - INFO - Epoch 9, Batch 72: Training loss = 0.2900
2025-04-03 18:09:01,081 - INFO - Epoch 9, Batch 73: Training loss = 0.2800
2025-04-03 18:09:01,182 - INFO - Epoch 9, Batch 74: Training loss = 0.2700
2025-04-03 18:09:01,283 - INFO - Epoch 9, Batch 75: Training loss = 0.2600
2025-04-03 18:09:01,383 - INFO - Epoch 9, Batch 76: Training loss = 0.2500
2025-04-03 18:09:01,484 - INFO - Epoch 9, Batch 77: Training loss = 0.2400
2025-04-03 18:09:01,584 - INFO - Epoch 9, Batch 78: Training loss = 0.2300
2025-04-03 18:09:01,686 - INFO - Epoch 9, Batch 79: Training loss = 0.2200
2025-04-03 18:09:01,787 - INFO - Epoch 9, Batch 80: Training loss = 0.2100
2025-04-03 18:09:01,887 - INFO - Epoch 9, Batch 81: Training loss = 0.2000
2025-04-03 18:09:01,988 - INFO - Epoch 9, Batch 82: Training loss = 0.1900
2025-04-03 18:09:02,089 - INFO - Epoch 9, Batch 83: Training loss = 0.1800
2025-04-03 18:09:02,189 - INFO - Epoch 9, Batch 84: Training loss = 0.1700
2025-04-03 18:09:02,290 - INFO - Epoch 9, Batch 85: Training loss = 0.1600
2025-04-03 18:09:02,391 - INFO - Epoch 9, Batch 86: Training loss = 0.1500
2025-04-03 18:09:02,492 - INFO - Epoch 9, Batch 87: Training loss = 0.1400
2025-04-03 18:09:02,593 - INFO - Epoch 9, Batch 88: Training loss = 0.1300
2025-04-03 18:09:02,693 - INFO - Epoch 9, Batch 89: Training loss = 0.1200
2025-04-03 18:09:02,794 - INFO - Epoch 9, Batch 90: Training loss = 0.1100
2025-04-03 18:09:02,895 - INFO - Epoch 9, Batch 91: Training loss = 0.1000
2025-04-03 18:09:02,996 - INFO - Epoch 9, Batch 92: Training loss = 0.0900
2025-04-03 18:09:03,097 - INFO - Epoch 9, Batch 93: Training loss = 0.0800
2025-04-03 18:09:03,198 - INFO - Epoch 9, Batch 94: Training loss = 0.0700
2025-04-03 18:09:03,299 - INFO - Epoch 9, Batch 95: Training loss = 0.0600
2025-04-03 18:09:03,400 - INFO - Epoch 9, Batch 96: Training loss = 0.0500
2025-04-03 18:09:03,500 - INFO - Epoch 9, Batch 97: Training loss = 0.0400
2025-04-03 18:09:03,601 - INFO - Epoch 9, Batch 98: Training loss = 0.0300
2025-04-03 18:09:03,702 - INFO - Epoch 9, Batch 99: Training loss = 0.0200
2025-04-03 18:09:03,804 - INFO - Epoch 9, Batch 100: Training loss = 0.0100
2025-04-03 18:09:03,905 - INFO - Epoch 9: Validation loss = 0.0200, Validation accuracy = 0.9600
2025-04-03 18:09:03,905 - INFO - Epoch 9: MSE = 0.0200, RMSE = 0.1414, MAE = 0.0160, RÂ² = 0.9800
2025-04-03 18:09:03,906 - INFO - Epoch 10 started.
2025-04-03 18:09:03,906 - INFO - Epoch 10, Batch 1: Training loss = 1.0000
2025-04-03 18:09:04,007 - INFO - Epoch 10, Batch 2: Training loss = 0.9900
2025-04-03 18:09:04,108 - INFO - Epoch 10, Batch 3: Training loss = 0.9800
2025-04-03 18:09:04,208 - INFO - Epoch 10, Batch 4: Training loss = 0.9700
2025-04-03 18:09:04,310 - INFO - Epoch 10, Batch 5: Training loss = 0.9600
2025-04-03 18:09:04,410 - INFO - Epoch 10, Batch 6: Training loss = 0.9500
2025-04-03 18:09:04,511 - INFO - Epoch 10, Batch 7: Training loss = 0.9400
2025-04-03 18:09:04,612 - INFO - Epoch 10, Batch 8: Training loss = 0.9300
2025-04-03 18:09:04,713 - INFO - Epoch 10, Batch 9: Training loss = 0.9200
2025-04-03 18:09:04,814 - INFO - Epoch 10, Batch 10: Training loss = 0.9100
2025-04-03 18:09:04,915 - INFO - Epoch 10, Batch 11: Training loss = 0.9000
2025-04-03 18:09:05,015 - INFO - Epoch 10, Batch 12: Training loss = 0.8900
2025-04-03 18:09:05,116 - INFO - Epoch 10, Batch 13: Training loss = 0.8800
2025-04-03 18:09:05,217 - INFO - Epoch 10, Batch 14: Training loss = 0.8700
2025-04-03 18:09:05,317 - INFO - Epoch 10, Batch 15: Training loss = 0.8600
2025-04-03 18:09:05,418 - INFO - Epoch 10, Batch 16: Training loss = 0.8500
2025-04-03 18:09:05,519 - INFO - Epoch 10, Batch 17: Training loss = 0.8400
2025-04-03 18:09:05,620 - INFO - Epoch 10, Batch 18: Training loss = 0.8300
2025-04-03 18:09:05,721 - INFO - Epoch 10, Batch 19: Training loss = 0.8200
2025-04-03 18:09:05,822 - INFO - Epoch 10, Batch 20: Training loss = 0.8100
2025-04-03 18:09:05,923 - INFO - Epoch 10, Batch 21: Training loss = 0.8000
2025-04-03 18:09:06,023 - INFO - Epoch 10, Batch 22: Training loss = 0.7900
2025-04-03 18:09:06,124 - INFO - Epoch 10, Batch 23: Training loss = 0.7800
2025-04-03 18:09:06,225 - INFO - Epoch 10, Batch 24: Training loss = 0.7700
2025-04-03 18:09:06,326 - INFO - Epoch 10, Batch 25: Training loss = 0.7600
2025-04-03 18:09:06,427 - INFO - Epoch 10, Batch 26: Training loss = 0.7500
2025-04-03 18:09:06,528 - INFO - Epoch 10, Batch 27: Training loss = 0.7400
2025-04-03 18:09:06,629 - INFO - Epoch 10, Batch 28: Training loss = 0.7300
2025-04-03 18:09:06,730 - INFO - Epoch 10, Batch 29: Training loss = 0.7200
2025-04-03 18:09:06,831 - INFO - Epoch 10, Batch 30: Training loss = 0.7100
2025-04-03 18:09:06,931 - INFO - Epoch 10, Batch 31: Training loss = 0.7000
2025-04-03 18:09:07,032 - INFO - Epoch 10, Batch 32: Training loss = 0.6900
2025-04-03 18:09:07,133 - INFO - Epoch 10, Batch 33: Training loss = 0.6800
2025-04-03 18:09:07,233 - INFO - Epoch 10, Batch 34: Training loss = 0.6700
2025-04-03 18:09:07,334 - INFO - Epoch 10, Batch 35: Training loss = 0.6600
2025-04-03 18:09:07,433 - INFO - Epoch 10, Batch 36: Training loss = 0.6500
2025-04-03 18:09:07,534 - INFO - Epoch 10, Batch 37: Training loss = 0.6400
2025-04-03 18:09:07,634 - INFO - Epoch 10, Batch 38: Training loss = 0.6300
2025-04-03 18:09:07,735 - INFO - Epoch 10, Batch 39: Training loss = 0.6200
2025-04-03 18:09:07,836 - INFO - Epoch 10, Batch 40: Training loss = 0.6100
2025-04-03 18:09:07,937 - INFO - Epoch 10, Batch 41: Training loss = 0.6000
2025-04-03 18:09:08,037 - INFO - Epoch 10, Batch 42: Training loss = 0.5900
2025-04-03 18:09:08,138 - INFO - Epoch 10, Batch 43: Training loss = 0.5800
2025-04-03 18:09:08,239 - INFO - Epoch 10, Batch 44: Training loss = 0.5700
2025-04-03 18:09:08,339 - INFO - Epoch 10, Batch 45: Training loss = 0.5600
2025-04-03 18:09:08,440 - INFO - Epoch 10, Batch 46: Training loss = 0.5500
2025-04-03 18:09:08,541 - INFO - Epoch 10, Batch 47: Training loss = 0.5400
2025-04-03 18:09:08,641 - INFO - Epoch 10, Batch 48: Training loss = 0.5300
2025-04-03 18:09:08,742 - INFO - Epoch 10, Batch 49: Training loss = 0.5200
2025-04-03 18:09:08,843 - INFO - Epoch 10, Batch 50: Training loss = 0.5100
2025-04-03 18:09:08,944 - INFO - Epoch 10, Batch 51: Training loss = 0.5000
2025-04-03 18:09:09,045 - INFO - Epoch 10, Batch 52: Training loss = 0.4900
2025-04-03 18:09:09,145 - INFO - Epoch 10, Batch 53: Training loss = 0.4800
2025-04-03 18:09:09,246 - INFO - Epoch 10, Batch 54: Training loss = 0.4700
2025-04-03 18:09:09,347 - INFO - Epoch 10, Batch 55: Training loss = 0.4600
2025-04-03 18:09:09,448 - INFO - Epoch 10, Batch 56: Training loss = 0.4500
2025-04-03 18:09:09,548 - INFO - Epoch 10, Batch 57: Training loss = 0.4400
2025-04-03 18:09:09,649 - INFO - Epoch 10, Batch 58: Training loss = 0.4300
2025-04-03 18:09:09,750 - INFO - Epoch 10, Batch 59: Training loss = 0.4200
2025-04-03 18:09:09,850 - INFO - Epoch 10, Batch 60: Training loss = 0.4100
2025-04-03 18:09:09,951 - INFO - Epoch 10, Batch 61: Training loss = 0.4000
2025-04-03 18:09:10,052 - INFO - Epoch 10, Batch 62: Training loss = 0.3900
2025-04-03 18:09:10,153 - INFO - Epoch 10, Batch 63: Training loss = 0.3800
2025-04-03 18:09:10,253 - INFO - Epoch 10, Batch 64: Training loss = 0.3700
2025-04-03 18:09:10,354 - INFO - Epoch 10, Batch 65: Training loss = 0.3600
2025-04-03 18:09:10,455 - INFO - Epoch 10, Batch 66: Training loss = 0.3500
2025-04-03 18:09:10,556 - INFO - Epoch 10, Batch 67: Training loss = 0.3400
2025-04-03 18:09:10,657 - INFO - Epoch 10, Batch 68: Training loss = 0.3300
2025-04-03 18:09:10,758 - INFO - Epoch 10, Batch 69: Training loss = 0.3200
2025-04-03 18:09:10,859 - INFO - Epoch 10, Batch 70: Training loss = 0.3100
2025-04-03 18:09:10,959 - INFO - Epoch 10, Batch 71: Training loss = 0.3000
2025-04-03 18:09:11,060 - INFO - Epoch 10, Batch 72: Training loss = 0.2900
2025-04-03 18:09:11,161 - INFO - Epoch 10, Batch 73: Training loss = 0.2800
2025-04-03 18:09:11,261 - INFO - Epoch 10, Batch 74: Training loss = 0.2700
2025-04-03 18:09:11,362 - INFO - Epoch 10, Batch 75: Training loss = 0.2600
2025-04-03 18:09:11,463 - INFO - Epoch 10, Batch 76: Training loss = 0.2500
2025-04-03 18:09:11,565 - INFO - Epoch 10, Batch 77: Training loss = 0.2400
2025-04-03 18:09:11,666 - INFO - Epoch 10, Batch 78: Training loss = 0.2300
2025-04-03 18:09:11,767 - INFO - Epoch 10, Batch 79: Training loss = 0.2200
2025-04-03 18:09:11,867 - INFO - Epoch 10, Batch 80: Training loss = 0.2100
2025-04-03 18:09:11,968 - INFO - Epoch 10, Batch 81: Training loss = 0.2000
2025-04-03 18:09:12,069 - INFO - Epoch 10, Batch 82: Training loss = 0.1900
2025-04-03 18:09:12,170 - INFO - Epoch 10, Batch 83: Training loss = 0.1800
2025-04-03 18:09:12,270 - INFO - Epoch 10, Batch 84: Training loss = 0.1700
2025-04-03 18:09:12,372 - INFO - Epoch 10, Batch 85: Training loss = 0.1600
2025-04-03 18:09:12,472 - INFO - Epoch 10, Batch 86: Training loss = 0.1500
2025-04-03 18:09:12,573 - INFO - Epoch 10, Batch 87: Training loss = 0.1400
2025-04-03 18:09:12,674 - INFO - Epoch 10, Batch 88: Training loss = 0.1300
2025-04-03 18:09:12,775 - INFO - Epoch 10, Batch 89: Training loss = 0.1200
2025-04-03 18:09:12,876 - INFO - Epoch 10, Batch 90: Training loss = 0.1100
2025-04-03 18:09:12,976 - INFO - Epoch 10, Batch 91: Training loss = 0.1000
2025-04-03 18:09:13,076 - INFO - Epoch 10, Batch 92: Training loss = 0.0900
2025-04-03 18:09:13,177 - INFO - Epoch 10, Batch 93: Training loss = 0.0800
2025-04-03 18:09:13,278 - INFO - Epoch 10, Batch 94: Training loss = 0.0700
2025-04-03 18:09:13,379 - INFO - Epoch 10, Batch 95: Training loss = 0.0600
2025-04-03 18:09:13,480 - INFO - Epoch 10, Batch 96: Training loss = 0.0500
2025-04-03 18:09:13,581 - INFO - Epoch 10, Batch 97: Training loss = 0.0400
2025-04-03 18:09:13,682 - INFO - Epoch 10, Batch 98: Training loss = 0.0300
2025-04-03 18:09:13,783 - INFO - Epoch 10, Batch 99: Training loss = 0.0200
2025-04-03 18:09:13,884 - INFO - Epoch 10, Batch 100: Training loss = 0.0100
2025-04-03 18:09:13,985 - INFO - Epoch 10: Validation loss = 0.0100, Validation accuracy = 0.9800
2025-04-03 18:09:13,986 - INFO - Epoch 10: MSE = 0.0100, RMSE = 0.1000, MAE = 0.0080, RÂ² = 0.9900
2025-04-03 18:09:13,986 - INFO - Regression training process completed.
2025-04-03 18:20:01,546 - INFO - Starting regression training process...
2025-04-03 18:20:01,546 - INFO - Epoch 1 started.
2025-04-03 18:20:01,546 - INFO - Epoch 1, Batch 1: Training loss = 1.0000
2025-04-03 18:20:01,647 - INFO - Epoch 1, Batch 2: Training loss = 0.9900
2025-04-03 18:20:01,747 - INFO - Epoch 1, Batch 3: Training loss = 0.9800
2025-04-03 18:20:01,848 - INFO - Epoch 1, Batch 4: Training loss = 0.9700
2025-04-03 18:20:01,949 - INFO - Epoch 1, Batch 5: Training loss = 0.9600
2025-04-03 18:20:02,049 - INFO - Epoch 1, Batch 6: Training loss = 0.9500
2025-04-03 18:20:02,150 - INFO - Epoch 1, Batch 7: Training loss = 0.9400
2025-04-03 18:20:02,250 - INFO - Epoch 1, Batch 8: Training loss = 0.9300
2025-04-03 18:20:02,351 - INFO - Epoch 1, Batch 9: Training loss = 0.9200
2025-04-03 18:20:02,452 - INFO - Epoch 1, Batch 10: Training loss = 0.9100
2025-04-03 18:20:02,553 - INFO - Epoch 1, Batch 11: Training loss = 0.9000
2025-04-03 18:20:02,654 - INFO - Epoch 1, Batch 12: Training loss = 0.8900
2025-04-03 18:20:02,754 - INFO - Epoch 1, Batch 13: Training loss = 0.8800
2025-04-03 18:20:02,855 - INFO - Epoch 1, Batch 14: Training loss = 0.8700
2025-04-03 18:20:02,956 - INFO - Epoch 1, Batch 15: Training loss = 0.8600
2025-04-03 18:20:03,057 - INFO - Epoch 1, Batch 16: Training loss = 0.8500
2025-04-03 18:20:03,157 - INFO - Epoch 1, Batch 17: Training loss = 0.8400
2025-04-03 18:20:03,258 - INFO - Epoch 1, Batch 18: Training loss = 0.8300
2025-04-03 18:20:03,358 - INFO - Epoch 1, Batch 19: Training loss = 0.8200
2025-04-03 18:20:03,459 - INFO - Epoch 1, Batch 20: Training loss = 0.8100
2025-04-03 18:20:03,559 - INFO - Epoch 1, Batch 21: Training loss = 0.8000
2025-04-03 18:20:03,660 - INFO - Epoch 1, Batch 22: Training loss = 0.7900
2025-04-03 18:20:03,761 - INFO - Epoch 1, Batch 23: Training loss = 0.7800
2025-04-03 18:20:03,861 - INFO - Epoch 1, Batch 24: Training loss = 0.7700
2025-04-03 18:20:03,962 - INFO - Epoch 1, Batch 25: Training loss = 0.7600
2025-04-03 18:20:04,063 - INFO - Epoch 1, Batch 26: Training loss = 0.7500
2025-04-03 18:20:04,164 - INFO - Epoch 1, Batch 27: Training loss = 0.7400
2025-04-03 18:20:04,265 - INFO - Epoch 1, Batch 28: Training loss = 0.7300
2025-04-03 18:20:04,365 - INFO - Epoch 1, Batch 29: Training loss = 0.7200
2025-04-03 18:20:04,466 - INFO - Epoch 1, Batch 30: Training loss = 0.7100
2025-04-03 18:20:04,567 - INFO - Epoch 1, Batch 31: Training loss = 0.7000
2025-04-03 18:20:04,668 - INFO - Epoch 1, Batch 32: Training loss = 0.6900
2025-04-03 18:20:04,769 - INFO - Epoch 1, Batch 33: Training loss = 0.6800
2025-04-03 18:20:04,869 - INFO - Epoch 1, Batch 34: Training loss = 0.6700
2025-04-03 18:20:04,970 - INFO - Epoch 1, Batch 35: Training loss = 0.6600
2025-04-03 18:20:05,070 - INFO - Epoch 1, Batch 36: Training loss = 0.6500
2025-04-03 18:20:05,171 - INFO - Epoch 1, Batch 37: Training loss = 0.6400
2025-04-03 18:20:05,272 - INFO - Epoch 1, Batch 38: Training loss = 0.6300
2025-04-03 18:20:05,372 - INFO - Epoch 1, Batch 39: Training loss = 0.6200
2025-04-03 18:20:05,473 - INFO - Epoch 1, Batch 40: Training loss = 0.6100
2025-04-03 18:20:05,574 - INFO - Epoch 1, Batch 41: Training loss = 0.6000
2025-04-03 18:20:05,675 - INFO - Epoch 1, Batch 42: Training loss = 0.5900
2025-04-03 18:20:05,775 - INFO - Epoch 1, Batch 43: Training loss = 0.5800
2025-04-03 18:20:05,876 - INFO - Epoch 1, Batch 44: Training loss = 0.5700
2025-04-03 18:20:05,977 - INFO - Epoch 1, Batch 45: Training loss = 0.5600
2025-04-03 18:20:06,077 - INFO - Epoch 1, Batch 46: Training loss = 0.5500
2025-04-03 18:20:06,178 - INFO - Epoch 1, Batch 47: Training loss = 0.5400
2025-04-03 18:20:06,279 - INFO - Epoch 1, Batch 48: Training loss = 0.5300
2025-04-03 18:20:06,379 - INFO - Epoch 1, Batch 49: Training loss = 0.5200
2025-04-03 18:20:06,480 - INFO - Epoch 1, Batch 50: Training loss = 0.5100
2025-04-03 18:20:06,581 - INFO - Epoch 1, Batch 51: Training loss = 0.5000
2025-04-03 18:20:06,681 - INFO - Epoch 1, Batch 52: Training loss = 0.4900
2025-04-03 18:20:06,782 - INFO - Epoch 1, Batch 53: Training loss = 0.4800
2025-04-03 18:20:06,882 - INFO - Epoch 1, Batch 54: Training loss = 0.4700
2025-04-03 18:20:06,983 - INFO - Epoch 1, Batch 55: Training loss = 0.4600
2025-04-03 18:20:07,084 - INFO - Epoch 1, Batch 56: Training loss = 0.4500
2025-04-03 18:20:07,184 - INFO - Epoch 1, Batch 57: Training loss = 0.4400
2025-04-03 18:20:07,285 - INFO - Epoch 1, Batch 58: Training loss = 0.4300
2025-04-03 18:20:07,386 - INFO - Epoch 1, Batch 59: Training loss = 0.4200
2025-04-03 18:20:07,486 - INFO - Epoch 1, Batch 60: Training loss = 0.4100
2025-04-03 18:20:07,585 - INFO - Epoch 1, Batch 61: Training loss = 0.4000
2025-04-03 18:20:07,686 - INFO - Epoch 1, Batch 62: Training loss = 0.3900
2025-04-03 18:20:07,787 - INFO - Epoch 1, Batch 63: Training loss = 0.3800
2025-04-03 18:20:07,887 - INFO - Epoch 1, Batch 64: Training loss = 0.3700
2025-04-03 18:20:07,988 - INFO - Epoch 1, Batch 65: Training loss = 0.3600
2025-04-03 18:20:08,089 - INFO - Epoch 1, Batch 66: Training loss = 0.3500
2025-04-03 18:20:08,190 - INFO - Epoch 1, Batch 67: Training loss = 0.3400
2025-04-03 18:20:08,290 - INFO - Epoch 1, Batch 68: Training loss = 0.3300
2025-04-03 18:20:08,391 - INFO - Epoch 1, Batch 69: Training loss = 0.3200
2025-04-03 18:20:08,492 - INFO - Epoch 1, Batch 70: Training loss = 0.3100
2025-04-03 18:20:08,593 - INFO - Epoch 1, Batch 71: Training loss = 0.3000
2025-04-03 18:20:08,693 - INFO - Epoch 1, Batch 72: Training loss = 0.2900
2025-04-03 18:20:08,794 - INFO - Epoch 1, Batch 73: Training loss = 0.2800
2025-04-03 18:20:08,894 - INFO - Epoch 1, Batch 74: Training loss = 0.2700
2025-04-03 18:20:08,995 - INFO - Epoch 1, Batch 75: Training loss = 0.2600
2025-04-03 18:20:09,095 - INFO - Epoch 1, Batch 76: Training loss = 0.2500
2025-04-03 18:20:09,196 - INFO - Epoch 1, Batch 77: Training loss = 0.2400
2025-04-03 18:20:09,297 - INFO - Epoch 1, Batch 78: Training loss = 0.2300
2025-04-03 18:20:09,398 - INFO - Epoch 1, Batch 79: Training loss = 0.2200
2025-04-03 18:20:09,498 - INFO - Epoch 1, Batch 80: Training loss = 0.2100
2025-04-03 18:20:09,599 - INFO - Epoch 1, Batch 81: Training loss = 0.2000
2025-04-03 18:20:09,699 - INFO - Epoch 1, Batch 82: Training loss = 0.1900
2025-04-03 18:20:09,800 - INFO - Epoch 1, Batch 83: Training loss = 0.1800
2025-04-03 18:20:09,900 - INFO - Epoch 1, Batch 84: Training loss = 0.1700
2025-04-03 18:20:10,000 - INFO - Epoch 1, Batch 85: Training loss = 0.1600
2025-04-03 18:20:10,101 - INFO - Epoch 1, Batch 86: Training loss = 0.1500
2025-04-03 18:20:10,202 - INFO - Epoch 1, Batch 87: Training loss = 0.1400
2025-04-03 18:20:10,302 - INFO - Epoch 1, Batch 88: Training loss = 0.1300
2025-04-03 18:20:10,403 - INFO - Epoch 1, Batch 89: Training loss = 0.1200
2025-04-03 18:20:10,504 - INFO - Epoch 1, Batch 90: Training loss = 0.1100
2025-04-03 18:20:10,604 - INFO - Epoch 1, Batch 91: Training loss = 0.1000
2025-04-03 18:20:10,705 - INFO - Epoch 1, Batch 92: Training loss = 0.0900
2025-04-03 18:20:10,806 - INFO - Epoch 1, Batch 93: Training loss = 0.0800
2025-04-03 18:20:10,907 - INFO - Epoch 1, Batch 94: Training loss = 0.0700
2025-04-03 18:20:11,007 - INFO - Epoch 1, Batch 95: Training loss = 0.0600
2025-04-03 18:20:11,108 - INFO - Epoch 1, Batch 96: Training loss = 0.0500
2025-04-03 18:20:11,208 - INFO - Epoch 1, Batch 97: Training loss = 0.0400
2025-04-03 18:20:11,310 - INFO - Epoch 1, Batch 98: Training loss = 0.0300
2025-04-03 18:20:11,410 - INFO - Epoch 1, Batch 99: Training loss = 0.0200
2025-04-03 18:20:11,511 - INFO - Epoch 1, Batch 100: Training loss = 0.0100
2025-04-03 18:20:11,612 - INFO - Epoch 1: Validation loss = 0.1000, Validation accuracy = 0.8000
2025-04-03 18:20:11,612 - INFO - Epoch 1: MSE = 0.1000, RMSE = 0.3162, MAE = 0.0800, RÂ² = 0.9000
2025-04-03 18:20:11,612 - INFO - Epoch 2 started.
2025-04-03 18:20:11,612 - INFO - Epoch 2, Batch 1: Training loss = 1.0000
2025-04-03 18:20:11,713 - INFO - Epoch 2, Batch 2: Training loss = 0.9900
2025-04-03 18:20:11,813 - INFO - Epoch 2, Batch 3: Training loss = 0.9800
2025-04-03 18:20:11,914 - INFO - Epoch 2, Batch 4: Training loss = 0.9700
2025-04-03 18:20:12,015 - INFO - Epoch 2, Batch 5: Training loss = 0.9600
2025-04-03 18:20:12,116 - INFO - Epoch 2, Batch 6: Training loss = 0.9500
2025-04-03 18:20:12,217 - INFO - Epoch 2, Batch 7: Training loss = 0.9400
2025-04-03 18:20:12,318 - INFO - Epoch 2, Batch 8: Training loss = 0.9300
2025-04-03 18:20:12,418 - INFO - Epoch 2, Batch 9: Training loss = 0.9200
2025-04-03 18:20:12,519 - INFO - Epoch 2, Batch 10: Training loss = 0.9100
2025-04-03 18:20:12,620 - INFO - Epoch 2, Batch 11: Training loss = 0.9000
2025-04-03 18:20:12,720 - INFO - Epoch 2, Batch 12: Training loss = 0.8900
2025-04-03 18:20:12,821 - INFO - Epoch 2, Batch 13: Training loss = 0.8800
2025-04-03 18:20:12,922 - INFO - Epoch 2, Batch 14: Training loss = 0.8700
2025-04-03 18:20:13,023 - INFO - Epoch 2, Batch 15: Training loss = 0.8600
2025-04-03 18:20:13,123 - INFO - Epoch 2, Batch 16: Training loss = 0.8500
2025-04-03 18:20:13,224 - INFO - Epoch 2, Batch 17: Training loss = 0.8400
2025-04-03 18:20:13,325 - INFO - Epoch 2, Batch 18: Training loss = 0.8300
2025-04-03 18:20:13,425 - INFO - Epoch 2, Batch 19: Training loss = 0.8200
2025-04-03 18:20:13,526 - INFO - Epoch 2, Batch 20: Training loss = 0.8100
2025-04-03 18:20:13,626 - INFO - Epoch 2, Batch 21: Training loss = 0.8000
2025-04-03 18:20:13,727 - INFO - Epoch 2, Batch 22: Training loss = 0.7900
2025-04-03 18:20:13,828 - INFO - Epoch 2, Batch 23: Training loss = 0.7800
2025-04-03 18:20:13,928 - INFO - Epoch 2, Batch 24: Training loss = 0.7700
2025-04-03 18:20:14,028 - INFO - Epoch 2, Batch 25: Training loss = 0.7600
2025-04-03 18:20:14,129 - INFO - Epoch 2, Batch 26: Training loss = 0.7500
2025-04-03 18:20:14,229 - INFO - Epoch 2, Batch 27: Training loss = 0.7400
2025-04-03 18:20:14,330 - INFO - Epoch 2, Batch 28: Training loss = 0.7300
2025-04-03 18:20:14,431 - INFO - Epoch 2, Batch 29: Training loss = 0.7200
2025-04-03 18:20:14,531 - INFO - Epoch 2, Batch 30: Training loss = 0.7100
2025-04-03 18:20:14,632 - INFO - Epoch 2, Batch 31: Training loss = 0.7000
2025-04-03 18:20:14,733 - INFO - Epoch 2, Batch 32: Training loss = 0.6900
2025-04-03 18:20:14,834 - INFO - Epoch 2, Batch 33: Training loss = 0.6800
2025-04-03 18:20:14,934 - INFO - Epoch 2, Batch 34: Training loss = 0.6700
2025-04-03 18:20:15,035 - INFO - Epoch 2, Batch 35: Training loss = 0.6600
2025-04-03 18:20:15,135 - INFO - Epoch 2, Batch 36: Training loss = 0.6500
2025-04-03 18:20:15,236 - INFO - Epoch 2, Batch 37: Training loss = 0.6400
2025-04-03 18:20:15,336 - INFO - Epoch 2, Batch 38: Training loss = 0.6300
2025-04-03 18:20:15,437 - INFO - Epoch 2, Batch 39: Training loss = 0.6200
2025-04-03 18:20:15,537 - INFO - Epoch 2, Batch 40: Training loss = 0.6100
2025-04-03 18:20:15,638 - INFO - Epoch 2, Batch 41: Training loss = 0.6000
2025-04-03 18:20:15,738 - INFO - Epoch 2, Batch 42: Training loss = 0.5900
2025-04-03 18:20:15,839 - INFO - Epoch 2, Batch 43: Training loss = 0.5800
2025-04-03 18:20:15,939 - INFO - Epoch 2, Batch 44: Training loss = 0.5700
2025-04-03 18:20:16,040 - INFO - Epoch 2, Batch 45: Training loss = 0.5600
2025-04-03 18:20:16,140 - INFO - Epoch 2, Batch 46: Training loss = 0.5500
2025-04-03 18:20:16,241 - INFO - Epoch 2, Batch 47: Training loss = 0.5400
2025-04-03 18:20:16,341 - INFO - Epoch 2, Batch 48: Training loss = 0.5300
2025-04-03 18:20:16,442 - INFO - Epoch 2, Batch 49: Training loss = 0.5200
2025-04-03 18:20:16,542 - INFO - Epoch 2, Batch 50: Training loss = 0.5100
2025-04-03 18:20:16,643 - INFO - Epoch 2, Batch 51: Training loss = 0.5000
2025-04-03 18:20:16,743 - INFO - Epoch 2, Batch 52: Training loss = 0.4900
2025-04-03 18:20:16,844 - INFO - Epoch 2, Batch 53: Training loss = 0.4800
2025-04-03 18:20:16,944 - INFO - Epoch 2, Batch 54: Training loss = 0.4700
2025-04-03 18:20:17,045 - INFO - Epoch 2, Batch 55: Training loss = 0.4600
2025-04-03 18:20:17,145 - INFO - Epoch 2, Batch 56: Training loss = 0.4500
2025-04-03 18:20:17,246 - INFO - Epoch 2, Batch 57: Training loss = 0.4400
2025-04-03 18:20:17,346 - INFO - Epoch 2, Batch 58: Training loss = 0.4300
2025-04-03 18:20:17,447 - INFO - Epoch 2, Batch 59: Training loss = 0.4200
2025-04-03 18:20:17,547 - INFO - Epoch 2, Batch 60: Training loss = 0.4100
2025-04-03 18:20:17,648 - INFO - Epoch 2, Batch 61: Training loss = 0.4000
2025-04-03 18:20:17,748 - INFO - Epoch 2, Batch 62: Training loss = 0.3900
2025-04-03 18:20:17,849 - INFO - Epoch 2, Batch 63: Training loss = 0.3800
2025-04-03 18:20:17,949 - INFO - Epoch 2, Batch 64: Training loss = 0.3700
2025-04-03 18:20:18,050 - INFO - Epoch 2, Batch 65: Training loss = 0.3600
2025-04-03 18:20:18,151 - INFO - Epoch 2, Batch 66: Training loss = 0.3500
2025-04-03 18:20:18,251 - INFO - Epoch 2, Batch 67: Training loss = 0.3400
2025-04-03 18:20:18,352 - INFO - Epoch 2, Batch 68: Training loss = 0.3300
2025-04-03 18:20:18,453 - INFO - Epoch 2, Batch 69: Training loss = 0.3200
2025-04-03 18:20:18,553 - INFO - Epoch 2, Batch 70: Training loss = 0.3100
2025-04-03 18:20:18,654 - INFO - Epoch 2, Batch 71: Training loss = 0.3000
2025-04-03 18:20:18,754 - INFO - Epoch 2, Batch 72: Training loss = 0.2900
2025-04-03 18:20:18,855 - INFO - Epoch 2, Batch 73: Training loss = 0.2800
2025-04-03 18:20:18,956 - INFO - Epoch 2, Batch 74: Training loss = 0.2700
2025-04-03 18:20:19,057 - INFO - Epoch 2, Batch 75: Training loss = 0.2600
2025-04-03 18:20:19,158 - INFO - Epoch 2, Batch 76: Training loss = 0.2500
2025-04-03 18:20:19,258 - INFO - Epoch 2, Batch 77: Training loss = 0.2400
2025-04-03 18:20:19,360 - INFO - Epoch 2, Batch 78: Training loss = 0.2300
2025-04-03 18:20:19,461 - INFO - Epoch 2, Batch 79: Training loss = 0.2200
2025-04-03 18:20:19,562 - INFO - Epoch 2, Batch 80: Training loss = 0.2100
2025-04-03 18:20:19,662 - INFO - Epoch 2, Batch 81: Training loss = 0.2000
2025-04-03 18:20:19,763 - INFO - Epoch 2, Batch 82: Training loss = 0.1900
2025-04-03 18:20:19,864 - INFO - Epoch 2, Batch 83: Training loss = 0.1800
2025-04-03 18:20:19,964 - INFO - Epoch 2, Batch 84: Training loss = 0.1700
2025-04-03 18:20:20,065 - INFO - Epoch 2, Batch 85: Training loss = 0.1600
2025-04-03 18:20:20,166 - INFO - Epoch 2, Batch 86: Training loss = 0.1500
2025-04-03 18:20:20,267 - INFO - Epoch 2, Batch 87: Training loss = 0.1400
2025-04-03 18:20:20,368 - INFO - Epoch 2, Batch 88: Training loss = 0.1300
2025-04-03 18:20:20,468 - INFO - Epoch 2, Batch 89: Training loss = 0.1200
2025-04-03 18:20:20,569 - INFO - Epoch 2, Batch 90: Training loss = 0.1100
2025-04-03 18:20:20,670 - INFO - Epoch 2, Batch 91: Training loss = 0.1000
2025-04-03 18:20:20,771 - INFO - Epoch 2, Batch 92: Training loss = 0.0900
2025-04-03 18:20:20,872 - INFO - Epoch 2, Batch 93: Training loss = 0.0800
2025-04-03 18:20:20,973 - INFO - Epoch 2, Batch 94: Training loss = 0.0700
2025-04-03 18:20:21,073 - INFO - Epoch 2, Batch 95: Training loss = 0.0600
2025-04-03 18:20:21,174 - INFO - Epoch 2, Batch 96: Training loss = 0.0500
2025-04-03 18:20:21,274 - INFO - Epoch 2, Batch 97: Training loss = 0.0400
2025-04-03 18:20:21,375 - INFO - Epoch 2, Batch 98: Training loss = 0.0300
2025-04-03 18:20:21,475 - INFO - Epoch 2, Batch 99: Training loss = 0.0200
2025-04-03 18:20:21,575 - INFO - Epoch 2, Batch 100: Training loss = 0.0100
2025-04-03 18:20:21,676 - INFO - Epoch 2: Validation loss = 0.0900, Validation accuracy = 0.8200
2025-04-03 18:20:21,676 - INFO - Epoch 2: MSE = 0.0900, RMSE = 0.3000, MAE = 0.0720, RÂ² = 0.9100
2025-04-03 18:20:21,676 - INFO - Epoch 3 started.
2025-04-03 18:20:21,676 - INFO - Epoch 3, Batch 1: Training loss = 1.0000
2025-04-03 18:20:21,776 - INFO - Epoch 3, Batch 2: Training loss = 0.9900
2025-04-03 18:20:21,877 - INFO - Epoch 3, Batch 3: Training loss = 0.9800
2025-04-03 18:20:21,977 - INFO - Epoch 3, Batch 4: Training loss = 0.9700
2025-04-03 18:20:22,078 - INFO - Epoch 3, Batch 5: Training loss = 0.9600
2025-04-03 18:20:22,178 - INFO - Epoch 3, Batch 6: Training loss = 0.9500
2025-04-03 18:20:22,279 - INFO - Epoch 3, Batch 7: Training loss = 0.9400
2025-04-03 18:20:22,380 - INFO - Epoch 3, Batch 8: Training loss = 0.9300
2025-04-03 18:20:22,481 - INFO - Epoch 3, Batch 9: Training loss = 0.9200
2025-04-03 18:20:22,581 - INFO - Epoch 3, Batch 10: Training loss = 0.9100
2025-04-03 18:20:22,682 - INFO - Epoch 3, Batch 11: Training loss = 0.9000
2025-04-03 18:20:22,782 - INFO - Epoch 3, Batch 12: Training loss = 0.8900
2025-04-03 18:20:22,883 - INFO - Epoch 3, Batch 13: Training loss = 0.8800
2025-04-03 18:20:22,984 - INFO - Epoch 3, Batch 14: Training loss = 0.8700
2025-04-03 18:20:23,084 - INFO - Epoch 3, Batch 15: Training loss = 0.8600
2025-04-03 18:20:23,185 - INFO - Epoch 3, Batch 16: Training loss = 0.8500
2025-04-03 18:20:23,285 - INFO - Epoch 3, Batch 17: Training loss = 0.8400
2025-04-03 18:20:23,386 - INFO - Epoch 3, Batch 18: Training loss = 0.8300
2025-04-03 18:20:23,487 - INFO - Epoch 3, Batch 19: Training loss = 0.8200
2025-04-03 18:20:23,587 - INFO - Epoch 3, Batch 20: Training loss = 0.8100
2025-04-03 18:20:23,688 - INFO - Epoch 3, Batch 21: Training loss = 0.8000
2025-04-03 18:20:23,788 - INFO - Epoch 3, Batch 22: Training loss = 0.7900
2025-04-03 18:20:23,889 - INFO - Epoch 3, Batch 23: Training loss = 0.7800
2025-04-03 18:20:23,990 - INFO - Epoch 3, Batch 24: Training loss = 0.7700
2025-04-03 18:20:24,090 - INFO - Epoch 3, Batch 25: Training loss = 0.7600
2025-04-03 18:20:24,191 - INFO - Epoch 3, Batch 26: Training loss = 0.7500
2025-04-03 18:20:24,292 - INFO - Epoch 3, Batch 27: Training loss = 0.7400
2025-04-03 18:20:24,393 - INFO - Epoch 3, Batch 28: Training loss = 0.7300
2025-04-03 18:20:24,495 - INFO - Epoch 3, Batch 29: Training loss = 0.7200
2025-04-03 18:20:24,596 - INFO - Epoch 3, Batch 30: Training loss = 0.7100
2025-04-03 18:20:24,697 - INFO - Epoch 3, Batch 31: Training loss = 0.7000
2025-04-03 18:20:24,798 - INFO - Epoch 3, Batch 32: Training loss = 0.6900
2025-04-03 18:20:24,898 - INFO - Epoch 3, Batch 33: Training loss = 0.6800
2025-04-03 18:20:24,999 - INFO - Epoch 3, Batch 34: Training loss = 0.6700
2025-04-03 18:20:25,099 - INFO - Epoch 3, Batch 35: Training loss = 0.6600
2025-04-03 18:20:25,199 - INFO - Epoch 3, Batch 36: Training loss = 0.6500
2025-04-03 18:20:25,300 - INFO - Epoch 3, Batch 37: Training loss = 0.6400
2025-04-03 18:20:25,401 - INFO - Epoch 3, Batch 38: Training loss = 0.6300
2025-04-03 18:20:25,502 - INFO - Epoch 3, Batch 39: Training loss = 0.6200
2025-04-03 18:20:25,603 - INFO - Epoch 3, Batch 40: Training loss = 0.6100
2025-04-03 18:20:25,703 - INFO - Epoch 3, Batch 41: Training loss = 0.6000
2025-04-03 18:20:25,804 - INFO - Epoch 3, Batch 42: Training loss = 0.5900
2025-04-03 18:20:25,905 - INFO - Epoch 3, Batch 43: Training loss = 0.5800
2025-04-03 18:20:26,006 - INFO - Epoch 3, Batch 44: Training loss = 0.5700
2025-04-03 18:20:26,106 - INFO - Epoch 3, Batch 45: Training loss = 0.5600
2025-04-03 18:20:26,207 - INFO - Epoch 3, Batch 46: Training loss = 0.5500
2025-04-03 18:20:26,308 - INFO - Epoch 3, Batch 47: Training loss = 0.5400
2025-04-03 18:20:26,409 - INFO - Epoch 3, Batch 48: Training loss = 0.5300
2025-04-03 18:20:26,510 - INFO - Epoch 3, Batch 49: Training loss = 0.5200
2025-04-03 18:20:26,610 - INFO - Epoch 3, Batch 50: Training loss = 0.5100
2025-04-03 18:20:26,711 - INFO - Epoch 3, Batch 51: Training loss = 0.5000
2025-04-03 18:20:26,812 - INFO - Epoch 3, Batch 52: Training loss = 0.4900
2025-04-03 18:20:26,912 - INFO - Epoch 3, Batch 53: Training loss = 0.4800
2025-04-03 18:20:27,013 - INFO - Epoch 3, Batch 54: Training loss = 0.4700
2025-04-03 18:20:27,114 - INFO - Epoch 3, Batch 55: Training loss = 0.4600
2025-04-03 18:20:27,214 - INFO - Epoch 3, Batch 56: Training loss = 0.4500
2025-04-03 18:20:27,315 - INFO - Epoch 3, Batch 57: Training loss = 0.4400
2025-04-03 18:20:27,415 - INFO - Epoch 3, Batch 58: Training loss = 0.4300
2025-04-03 18:20:27,516 - INFO - Epoch 3, Batch 59: Training loss = 0.4200
2025-04-03 18:20:27,617 - INFO - Epoch 3, Batch 60: Training loss = 0.4100
2025-04-03 18:20:27,718 - INFO - Epoch 3, Batch 61: Training loss = 0.4000
2025-04-03 18:20:27,819 - INFO - Epoch 3, Batch 62: Training loss = 0.3900
2025-04-03 18:20:27,920 - INFO - Epoch 3, Batch 63: Training loss = 0.3800
2025-04-03 18:20:28,020 - INFO - Epoch 3, Batch 64: Training loss = 0.3700
2025-04-03 18:20:28,121 - INFO - Epoch 3, Batch 65: Training loss = 0.3600
2025-04-03 18:20:28,221 - INFO - Epoch 3, Batch 66: Training loss = 0.3500
2025-04-03 18:20:28,322 - INFO - Epoch 3, Batch 67: Training loss = 0.3400
2025-04-03 18:20:28,423 - INFO - Epoch 3, Batch 68: Training loss = 0.3300
2025-04-03 18:20:28,524 - INFO - Epoch 3, Batch 69: Training loss = 0.3200
2025-04-03 18:20:28,625 - INFO - Epoch 3, Batch 70: Training loss = 0.3100
2025-04-03 18:20:28,726 - INFO - Epoch 3, Batch 71: Training loss = 0.3000
2025-04-03 18:20:28,827 - INFO - Epoch 3, Batch 72: Training loss = 0.2900
2025-04-03 18:20:28,927 - INFO - Epoch 3, Batch 73: Training loss = 0.2800
2025-04-03 18:20:29,028 - INFO - Epoch 3, Batch 74: Training loss = 0.2700
2025-04-03 18:20:29,129 - INFO - Epoch 3, Batch 75: Training loss = 0.2600
2025-04-03 18:20:29,229 - INFO - Epoch 3, Batch 76: Training loss = 0.2500
2025-04-03 18:20:29,330 - INFO - Epoch 3, Batch 77: Training loss = 0.2400
2025-04-03 18:20:29,430 - INFO - Epoch 3, Batch 78: Training loss = 0.2300
2025-04-03 18:20:29,531 - INFO - Epoch 3, Batch 79: Training loss = 0.2200
2025-04-03 18:20:29,632 - INFO - Epoch 3, Batch 80: Training loss = 0.2100
2025-04-03 18:20:29,733 - INFO - Epoch 3, Batch 81: Training loss = 0.2000
2025-04-03 18:20:29,833 - INFO - Epoch 3, Batch 82: Training loss = 0.1900
2025-04-03 18:20:29,934 - INFO - Epoch 3, Batch 83: Training loss = 0.1800
2025-04-03 18:20:30,035 - INFO - Epoch 3, Batch 84: Training loss = 0.1700
2025-04-03 18:20:30,136 - INFO - Epoch 3, Batch 85: Training loss = 0.1600
2025-04-03 18:20:30,237 - INFO - Epoch 3, Batch 86: Training loss = 0.1500
2025-04-03 18:20:30,337 - INFO - Epoch 3, Batch 87: Training loss = 0.1400
2025-04-03 18:20:30,438 - INFO - Epoch 3, Batch 88: Training loss = 0.1300
2025-04-03 18:20:30,539 - INFO - Epoch 3, Batch 89: Training loss = 0.1200
2025-04-03 18:20:30,640 - INFO - Epoch 3, Batch 90: Training loss = 0.1100
2025-04-03 18:20:30,741 - INFO - Epoch 3, Batch 91: Training loss = 0.1000
2025-04-03 18:20:30,842 - INFO - Epoch 3, Batch 92: Training loss = 0.0900
2025-04-03 18:20:30,942 - INFO - Epoch 3, Batch 93: Training loss = 0.0800
2025-04-03 18:20:31,043 - INFO - Epoch 3, Batch 94: Training loss = 0.0700
2025-04-03 18:20:31,144 - INFO - Epoch 3, Batch 95: Training loss = 0.0600
2025-04-03 18:20:31,245 - INFO - Epoch 3, Batch 96: Training loss = 0.0500
2025-04-03 18:20:31,346 - INFO - Epoch 3, Batch 97: Training loss = 0.0400
2025-04-03 18:20:31,447 - INFO - Epoch 3, Batch 98: Training loss = 0.0300
2025-04-03 18:20:31,548 - INFO - Epoch 3, Batch 99: Training loss = 0.0200
2025-04-03 18:20:31,649 - INFO - Epoch 3, Batch 100: Training loss = 0.0100
2025-04-03 18:20:31,750 - INFO - Epoch 3: Validation loss = 0.0800, Validation accuracy = 0.8400
2025-04-03 18:20:31,750 - INFO - Epoch 3: MSE = 0.0800, RMSE = 0.2828, MAE = 0.0640, RÂ² = 0.9200
2025-04-03 18:20:31,751 - INFO - Epoch 4 started.
2025-04-03 18:20:31,751 - INFO - Epoch 4, Batch 1: Training loss = 1.0000
2025-04-03 18:20:31,851 - INFO - Epoch 4, Batch 2: Training loss = 0.9900
2025-04-03 18:20:31,952 - INFO - Epoch 4, Batch 3: Training loss = 0.9800
2025-04-03 18:20:32,053 - INFO - Epoch 4, Batch 4: Training loss = 0.9700
2025-04-03 18:20:32,153 - INFO - Epoch 4, Batch 5: Training loss = 0.9600
2025-04-03 18:20:32,254 - INFO - Epoch 4, Batch 6: Training loss = 0.9500
2025-04-03 18:20:32,355 - INFO - Epoch 4, Batch 7: Training loss = 0.9400
2025-04-03 18:20:32,455 - INFO - Epoch 4, Batch 8: Training loss = 0.9300
2025-04-03 18:20:32,556 - INFO - Epoch 4, Batch 9: Training loss = 0.9200
2025-04-03 18:20:32,657 - INFO - Epoch 4, Batch 10: Training loss = 0.9100
2025-04-03 18:20:32,757 - INFO - Epoch 4, Batch 11: Training loss = 0.9000
2025-04-03 18:20:32,858 - INFO - Epoch 4, Batch 12: Training loss = 0.8900
2025-04-03 18:20:32,959 - INFO - Epoch 4, Batch 13: Training loss = 0.8800
2025-04-03 18:20:33,060 - INFO - Epoch 4, Batch 14: Training loss = 0.8700
2025-04-03 18:20:33,161 - INFO - Epoch 4, Batch 15: Training loss = 0.8600
2025-04-03 18:20:33,262 - INFO - Epoch 4, Batch 16: Training loss = 0.8500
2025-04-03 18:20:33,363 - INFO - Epoch 4, Batch 17: Training loss = 0.8400
2025-04-03 18:20:33,464 - INFO - Epoch 4, Batch 18: Training loss = 0.8300
2025-04-03 18:20:33,565 - INFO - Epoch 4, Batch 19: Training loss = 0.8200
2025-04-03 18:20:33,666 - INFO - Epoch 4, Batch 20: Training loss = 0.8100
2025-04-03 18:20:33,768 - INFO - Epoch 4, Batch 21: Training loss = 0.8000
2025-04-03 18:20:33,868 - INFO - Epoch 4, Batch 22: Training loss = 0.7900
2025-04-03 18:20:33,969 - INFO - Epoch 4, Batch 23: Training loss = 0.7800
2025-04-03 18:20:34,070 - INFO - Epoch 4, Batch 24: Training loss = 0.7700
2025-04-03 18:20:34,171 - INFO - Epoch 4, Batch 25: Training loss = 0.7600
2025-04-03 18:20:34,272 - INFO - Epoch 4, Batch 26: Training loss = 0.7500
2025-04-03 18:20:34,372 - INFO - Epoch 4, Batch 27: Training loss = 0.7400
2025-04-03 18:20:34,473 - INFO - Epoch 4, Batch 28: Training loss = 0.7300
2025-04-03 18:20:34,574 - INFO - Epoch 4, Batch 29: Training loss = 0.7200
2025-04-03 18:20:34,675 - INFO - Epoch 4, Batch 30: Training loss = 0.7100
2025-04-03 18:20:34,776 - INFO - Epoch 4, Batch 31: Training loss = 0.7000
2025-04-03 18:20:34,877 - INFO - Epoch 4, Batch 32: Training loss = 0.6900
2025-04-03 18:20:34,978 - INFO - Epoch 4, Batch 33: Training loss = 0.6800
2025-04-03 18:20:35,079 - INFO - Epoch 4, Batch 34: Training loss = 0.6700
2025-04-03 18:20:35,180 - INFO - Epoch 4, Batch 35: Training loss = 0.6600
2025-04-03 18:20:35,281 - INFO - Epoch 4, Batch 36: Training loss = 0.6500
2025-04-03 18:20:35,382 - INFO - Epoch 4, Batch 37: Training loss = 0.6400
2025-04-03 18:20:35,483 - INFO - Epoch 4, Batch 38: Training loss = 0.6300
2025-04-03 18:20:35,584 - INFO - Epoch 4, Batch 39: Training loss = 0.6200
2025-04-03 18:20:35,685 - INFO - Epoch 4, Batch 40: Training loss = 0.6100
2025-04-03 18:20:35,787 - INFO - Epoch 4, Batch 41: Training loss = 0.6000
2025-04-03 18:20:35,888 - INFO - Epoch 4, Batch 42: Training loss = 0.5900
2025-04-03 18:20:35,988 - INFO - Epoch 4, Batch 43: Training loss = 0.5800
2025-04-03 18:20:36,089 - INFO - Epoch 4, Batch 44: Training loss = 0.5700
2025-04-03 18:20:36,190 - INFO - Epoch 4, Batch 45: Training loss = 0.5600
2025-04-03 18:20:36,290 - INFO - Epoch 4, Batch 46: Training loss = 0.5500
2025-04-03 18:20:36,391 - INFO - Epoch 4, Batch 47: Training loss = 0.5400
2025-04-03 18:20:36,491 - INFO - Epoch 4, Batch 48: Training loss = 0.5300
2025-04-03 18:20:36,593 - INFO - Epoch 4, Batch 49: Training loss = 0.5200
2025-04-03 18:20:36,694 - INFO - Epoch 4, Batch 50: Training loss = 0.5100
2025-04-03 18:20:36,795 - INFO - Epoch 4, Batch 51: Training loss = 0.5000
2025-04-03 18:20:36,896 - INFO - Epoch 4, Batch 52: Training loss = 0.4900
2025-04-03 18:20:36,997 - INFO - Epoch 4, Batch 53: Training loss = 0.4800
2025-04-03 18:20:37,098 - INFO - Epoch 4, Batch 54: Training loss = 0.4700
2025-04-03 18:20:37,199 - INFO - Epoch 4, Batch 55: Training loss = 0.4600
2025-04-03 18:20:37,300 - INFO - Epoch 4, Batch 56: Training loss = 0.4500
2025-04-03 18:20:37,401 - INFO - Epoch 4, Batch 57: Training loss = 0.4400
2025-04-03 18:20:37,500 - INFO - Epoch 4, Batch 58: Training loss = 0.4300
2025-04-03 18:20:37,600 - INFO - Epoch 4, Batch 59: Training loss = 0.4200
2025-04-03 18:20:37,701 - INFO - Epoch 4, Batch 60: Training loss = 0.4100
2025-04-03 18:20:37,803 - INFO - Epoch 4, Batch 61: Training loss = 0.4000
2025-04-03 18:20:37,904 - INFO - Epoch 4, Batch 62: Training loss = 0.3900
2025-04-03 18:20:38,005 - INFO - Epoch 4, Batch 63: Training loss = 0.3800
2025-04-03 18:20:38,106 - INFO - Epoch 4, Batch 64: Training loss = 0.3700
2025-04-03 18:20:38,206 - INFO - Epoch 4, Batch 65: Training loss = 0.3600
2025-04-03 18:20:38,307 - INFO - Epoch 4, Batch 66: Training loss = 0.3500
2025-04-03 18:20:38,408 - INFO - Epoch 4, Batch 67: Training loss = 0.3400
2025-04-03 18:20:38,508 - INFO - Epoch 4, Batch 68: Training loss = 0.3300
2025-04-03 18:20:38,609 - INFO - Epoch 4, Batch 69: Training loss = 0.3200
2025-04-03 18:20:38,710 - INFO - Epoch 4, Batch 70: Training loss = 0.3100
2025-04-03 18:20:38,811 - INFO - Epoch 4, Batch 71: Training loss = 0.3000
2025-04-03 18:20:38,912 - INFO - Epoch 4, Batch 72: Training loss = 0.2900
2025-04-03 18:20:39,012 - INFO - Epoch 4, Batch 73: Training loss = 0.2800
2025-04-03 18:20:39,113 - INFO - Epoch 4, Batch 74: Training loss = 0.2700
2025-04-03 18:20:39,214 - INFO - Epoch 4, Batch 75: Training loss = 0.2600
2025-04-03 18:20:39,315 - INFO - Epoch 4, Batch 76: Training loss = 0.2500
2025-04-03 18:20:39,416 - INFO - Epoch 4, Batch 77: Training loss = 0.2400
2025-04-03 18:20:39,517 - INFO - Epoch 4, Batch 78: Training loss = 0.2300
2025-04-03 18:20:39,618 - INFO - Epoch 4, Batch 79: Training loss = 0.2200
2025-04-03 18:20:39,719 - INFO - Epoch 4, Batch 80: Training loss = 0.2100
2025-04-03 18:20:39,820 - INFO - Epoch 4, Batch 81: Training loss = 0.2000
2025-04-03 18:20:39,921 - INFO - Epoch 4, Batch 82: Training loss = 0.1900
2025-04-03 18:20:40,022 - INFO - Epoch 4, Batch 83: Training loss = 0.1800
2025-04-03 18:20:40,123 - INFO - Epoch 4, Batch 84: Training loss = 0.1700
2025-04-03 18:20:40,224 - INFO - Epoch 4, Batch 85: Training loss = 0.1600
2025-04-03 18:20:40,325 - INFO - Epoch 4, Batch 86: Training loss = 0.1500
2025-04-03 18:20:40,426 - INFO - Epoch 4, Batch 87: Training loss = 0.1400
2025-04-03 18:20:40,526 - INFO - Epoch 4, Batch 88: Training loss = 0.1300
2025-04-03 18:20:40,627 - INFO - Epoch 4, Batch 89: Training loss = 0.1200
2025-04-03 18:20:40,728 - INFO - Epoch 4, Batch 90: Training loss = 0.1100
2025-04-03 18:20:40,828 - INFO - Epoch 4, Batch 91: Training loss = 0.1000
2025-04-03 18:20:40,929 - INFO - Epoch 4, Batch 92: Training loss = 0.0900
2025-04-03 18:20:41,030 - INFO - Epoch 4, Batch 93: Training loss = 0.0800
2025-04-03 18:20:41,131 - INFO - Epoch 4, Batch 94: Training loss = 0.0700
2025-04-03 18:20:41,231 - INFO - Epoch 4, Batch 95: Training loss = 0.0600
2025-04-03 18:20:41,332 - INFO - Epoch 4, Batch 96: Training loss = 0.0500
2025-04-03 18:20:41,432 - INFO - Epoch 4, Batch 97: Training loss = 0.0400
2025-04-03 18:20:41,533 - INFO - Epoch 4, Batch 98: Training loss = 0.0300
2025-04-03 18:20:41,634 - INFO - Epoch 4, Batch 99: Training loss = 0.0200
2025-04-03 18:20:41,735 - INFO - Epoch 4, Batch 100: Training loss = 0.0100
2025-04-03 18:20:41,836 - INFO - Epoch 4: Validation loss = 0.0700, Validation accuracy = 0.8600
2025-04-03 18:20:41,836 - INFO - Epoch 4: MSE = 0.0700, RMSE = 0.2646, MAE = 0.0560, RÂ² = 0.9300
2025-04-03 18:20:41,836 - INFO - Epoch 5 started.
2025-04-03 18:20:41,836 - INFO - Epoch 5, Batch 1: Training loss = 1.0000
2025-04-03 18:20:41,937 - INFO - Epoch 5, Batch 2: Training loss = 0.9900
2025-04-03 18:20:42,037 - INFO - Epoch 5, Batch 3: Training loss = 0.9800
2025-04-03 18:20:42,138 - INFO - Epoch 5, Batch 4: Training loss = 0.9700
2025-04-03 18:20:42,239 - INFO - Epoch 5, Batch 5: Training loss = 0.9600
2025-04-03 18:20:42,340 - INFO - Epoch 5, Batch 6: Training loss = 0.9500
2025-04-03 18:20:42,441 - INFO - Epoch 5, Batch 7: Training loss = 0.9400
2025-04-03 18:20:42,542 - INFO - Epoch 5, Batch 8: Training loss = 0.9300
2025-04-03 18:20:42,643 - INFO - Epoch 5, Batch 9: Training loss = 0.9200
2025-04-03 18:20:42,744 - INFO - Epoch 5, Batch 10: Training loss = 0.9100
2025-04-03 18:20:42,844 - INFO - Epoch 5, Batch 11: Training loss = 0.9000
2025-04-03 18:20:42,945 - INFO - Epoch 5, Batch 12: Training loss = 0.8900
2025-04-03 18:20:43,046 - INFO - Epoch 5, Batch 13: Training loss = 0.8800
2025-04-03 18:20:43,147 - INFO - Epoch 5, Batch 14: Training loss = 0.8700
2025-04-03 18:20:43,248 - INFO - Epoch 5, Batch 15: Training loss = 0.8600
2025-04-03 18:20:43,349 - INFO - Epoch 5, Batch 16: Training loss = 0.8500
2025-04-03 18:20:43,450 - INFO - Epoch 5, Batch 17: Training loss = 0.8400
2025-04-03 18:20:43,551 - INFO - Epoch 5, Batch 18: Training loss = 0.8300
2025-04-03 18:20:43,652 - INFO - Epoch 5, Batch 19: Training loss = 0.8200
2025-04-03 18:20:43,753 - INFO - Epoch 5, Batch 20: Training loss = 0.8100
2025-04-03 18:20:43,854 - INFO - Epoch 5, Batch 21: Training loss = 0.8000
2025-04-03 18:20:43,955 - INFO - Epoch 5, Batch 22: Training loss = 0.7900
2025-04-03 18:20:44,055 - INFO - Epoch 5, Batch 23: Training loss = 0.7800
2025-04-03 18:20:44,156 - INFO - Epoch 5, Batch 24: Training loss = 0.7700
2025-04-03 18:20:44,257 - INFO - Epoch 5, Batch 25: Training loss = 0.7600
2025-04-03 18:20:44,358 - INFO - Epoch 5, Batch 26: Training loss = 0.7500
2025-04-03 18:20:44,459 - INFO - Epoch 5, Batch 27: Training loss = 0.7400
2025-04-03 18:20:44,560 - INFO - Epoch 5, Batch 28: Training loss = 0.7300
2025-04-03 18:20:44,661 - INFO - Epoch 5, Batch 29: Training loss = 0.7200
2025-04-03 18:20:44,762 - INFO - Epoch 5, Batch 30: Training loss = 0.7100
2025-04-03 18:20:44,863 - INFO - Epoch 5, Batch 31: Training loss = 0.7000
2025-04-03 18:20:44,964 - INFO - Epoch 5, Batch 32: Training loss = 0.6900
2025-04-03 18:20:45,064 - INFO - Epoch 5, Batch 33: Training loss = 0.6800
2025-04-03 18:20:45,165 - INFO - Epoch 5, Batch 34: Training loss = 0.6700
2025-04-03 18:20:45,266 - INFO - Epoch 5, Batch 35: Training loss = 0.6600
2025-04-03 18:20:45,367 - INFO - Epoch 5, Batch 36: Training loss = 0.6500
2025-04-03 18:20:45,468 - INFO - Epoch 5, Batch 37: Training loss = 0.6400
2025-04-03 18:20:45,569 - INFO - Epoch 5, Batch 38: Training loss = 0.6300
2025-04-03 18:20:45,670 - INFO - Epoch 5, Batch 39: Training loss = 0.6200
2025-04-03 18:20:45,771 - INFO - Epoch 5, Batch 40: Training loss = 0.6100
2025-04-03 18:20:45,872 - INFO - Epoch 5, Batch 41: Training loss = 0.6000
2025-04-03 18:20:45,973 - INFO - Epoch 5, Batch 42: Training loss = 0.5900
2025-04-03 18:20:46,074 - INFO - Epoch 5, Batch 43: Training loss = 0.5800
2025-04-03 18:20:46,175 - INFO - Epoch 5, Batch 44: Training loss = 0.5700
2025-04-03 18:20:46,276 - INFO - Epoch 5, Batch 45: Training loss = 0.5600
2025-04-03 18:20:46,377 - INFO - Epoch 5, Batch 46: Training loss = 0.5500
2025-04-03 18:20:46,478 - INFO - Epoch 5, Batch 47: Training loss = 0.5400
2025-04-03 18:20:46,579 - INFO - Epoch 5, Batch 48: Training loss = 0.5300
2025-04-03 18:20:46,680 - INFO - Epoch 5, Batch 49: Training loss = 0.5200
2025-04-03 18:20:46,780 - INFO - Epoch 5, Batch 50: Training loss = 0.5100
2025-04-03 18:20:46,881 - INFO - Epoch 5, Batch 51: Training loss = 0.5000
2025-04-03 18:20:46,981 - INFO - Epoch 5, Batch 52: Training loss = 0.4900
2025-04-03 18:20:47,082 - INFO - Epoch 5, Batch 53: Training loss = 0.4800
2025-04-03 18:20:47,183 - INFO - Epoch 5, Batch 54: Training loss = 0.4700
2025-04-03 18:20:47,283 - INFO - Epoch 5, Batch 55: Training loss = 0.4600
2025-04-03 18:20:47,384 - INFO - Epoch 5, Batch 56: Training loss = 0.4500
2025-04-03 18:20:47,485 - INFO - Epoch 5, Batch 57: Training loss = 0.4400
2025-04-03 18:20:47,585 - INFO - Epoch 5, Batch 58: Training loss = 0.4300
2025-04-03 18:20:47,685 - INFO - Epoch 5, Batch 59: Training loss = 0.4200
2025-04-03 18:20:47,786 - INFO - Epoch 5, Batch 60: Training loss = 0.4100
2025-04-03 18:20:47,886 - INFO - Epoch 5, Batch 61: Training loss = 0.4000
2025-04-03 18:20:47,987 - INFO - Epoch 5, Batch 62: Training loss = 0.3900
2025-04-03 18:20:48,088 - INFO - Epoch 5, Batch 63: Training loss = 0.3800
2025-04-03 18:20:48,189 - INFO - Epoch 5, Batch 64: Training loss = 0.3700
2025-04-03 18:20:48,290 - INFO - Epoch 5, Batch 65: Training loss = 0.3600
2025-04-03 18:20:48,391 - INFO - Epoch 5, Batch 66: Training loss = 0.3500
2025-04-03 18:20:48,492 - INFO - Epoch 5, Batch 67: Training loss = 0.3400
2025-04-03 18:20:48,593 - INFO - Epoch 5, Batch 68: Training loss = 0.3300
2025-04-03 18:20:48,694 - INFO - Epoch 5, Batch 69: Training loss = 0.3200
2025-04-03 18:20:48,794 - INFO - Epoch 5, Batch 70: Training loss = 0.3100
2025-04-03 18:20:48,895 - INFO - Epoch 5, Batch 71: Training loss = 0.3000
2025-04-03 18:20:48,996 - INFO - Epoch 5, Batch 72: Training loss = 0.2900
2025-04-03 18:20:49,096 - INFO - Epoch 5, Batch 73: Training loss = 0.2800
2025-04-03 18:20:49,197 - INFO - Epoch 5, Batch 74: Training loss = 0.2700
2025-04-03 18:20:49,297 - INFO - Epoch 5, Batch 75: Training loss = 0.2600
2025-04-03 18:20:49,398 - INFO - Epoch 5, Batch 76: Training loss = 0.2500
2025-04-03 18:20:49,499 - INFO - Epoch 5, Batch 77: Training loss = 0.2400
2025-04-03 18:20:49,599 - INFO - Epoch 5, Batch 78: Training loss = 0.2300
2025-04-03 18:20:49,700 - INFO - Epoch 5, Batch 79: Training loss = 0.2200
2025-04-03 18:20:49,800 - INFO - Epoch 5, Batch 80: Training loss = 0.2100
2025-04-03 18:20:49,901 - INFO - Epoch 5, Batch 81: Training loss = 0.2000
2025-04-03 18:20:50,001 - INFO - Epoch 5, Batch 82: Training loss = 0.1900
2025-04-03 18:20:50,102 - INFO - Epoch 5, Batch 83: Training loss = 0.1800
2025-04-03 18:20:50,202 - INFO - Epoch 5, Batch 84: Training loss = 0.1700
2025-04-03 18:20:50,303 - INFO - Epoch 5, Batch 85: Training loss = 0.1600
2025-04-03 18:20:50,403 - INFO - Epoch 5, Batch 86: Training loss = 0.1500
2025-04-03 18:20:50,504 - INFO - Epoch 5, Batch 87: Training loss = 0.1400
2025-04-03 18:20:50,605 - INFO - Epoch 5, Batch 88: Training loss = 0.1300
2025-04-03 18:20:50,705 - INFO - Epoch 5, Batch 89: Training loss = 0.1200
2025-04-03 18:20:50,806 - INFO - Epoch 5, Batch 90: Training loss = 0.1100
2025-04-03 18:20:50,906 - INFO - Epoch 5, Batch 91: Training loss = 0.1000
2025-04-03 18:20:51,007 - INFO - Epoch 5, Batch 92: Training loss = 0.0900
2025-04-03 18:20:51,107 - INFO - Epoch 5, Batch 93: Training loss = 0.0800
2025-04-03 18:20:51,208 - INFO - Epoch 5, Batch 94: Training loss = 0.0700
2025-04-03 18:20:51,309 - INFO - Epoch 5, Batch 95: Training loss = 0.0600
2025-04-03 18:20:51,409 - INFO - Epoch 5, Batch 96: Training loss = 0.0500
2025-04-03 18:20:51,510 - INFO - Epoch 5, Batch 97: Training loss = 0.0400
2025-04-03 18:20:51,610 - INFO - Epoch 5, Batch 98: Training loss = 0.0300
2025-04-03 18:20:51,711 - INFO - Epoch 5, Batch 99: Training loss = 0.0200
2025-04-03 18:20:51,811 - INFO - Epoch 5, Batch 100: Training loss = 0.0100
2025-04-03 18:20:51,912 - INFO - Epoch 5: Validation loss = 0.0600, Validation accuracy = 0.8800
2025-04-03 18:20:51,912 - INFO - Epoch 5: MSE = 0.0600, RMSE = 0.2449, MAE = 0.0480, RÂ² = 0.9400
2025-04-03 18:20:51,912 - INFO - Epoch 6 started.
2025-04-03 18:20:51,912 - INFO - Epoch 6, Batch 1: Training loss = 1.0000
2025-04-03 18:20:52,013 - INFO - Epoch 6, Batch 2: Training loss = 0.9900
2025-04-03 18:20:52,113 - INFO - Epoch 6, Batch 3: Training loss = 0.9800
2025-04-03 18:20:52,214 - INFO - Epoch 6, Batch 4: Training loss = 0.9700
2025-04-03 18:20:52,314 - INFO - Epoch 6, Batch 5: Training loss = 0.9600
2025-04-03 18:20:52,415 - INFO - Epoch 6, Batch 6: Training loss = 0.9500
2025-04-03 18:20:52,515 - INFO - Epoch 6, Batch 7: Training loss = 0.9400
2025-04-03 18:20:52,616 - INFO - Epoch 6, Batch 8: Training loss = 0.9300
2025-04-03 18:20:52,716 - INFO - Epoch 6, Batch 9: Training loss = 0.9200
2025-04-03 18:20:52,817 - INFO - Epoch 6, Batch 10: Training loss = 0.9100
2025-04-03 18:20:52,918 - INFO - Epoch 6, Batch 11: Training loss = 0.9000
2025-04-03 18:20:53,019 - INFO - Epoch 6, Batch 12: Training loss = 0.8900
2025-04-03 18:20:53,119 - INFO - Epoch 6, Batch 13: Training loss = 0.8800
2025-04-03 18:20:53,220 - INFO - Epoch 6, Batch 14: Training loss = 0.8700
2025-04-03 18:20:53,321 - INFO - Epoch 6, Batch 15: Training loss = 0.8600
2025-04-03 18:20:53,422 - INFO - Epoch 6, Batch 16: Training loss = 0.8500
2025-04-03 18:20:53,523 - INFO - Epoch 6, Batch 17: Training loss = 0.8400
2025-04-03 18:20:53,624 - INFO - Epoch 6, Batch 18: Training loss = 0.8300
2025-04-03 18:20:53,724 - INFO - Epoch 6, Batch 19: Training loss = 0.8200
2025-04-03 18:20:53,825 - INFO - Epoch 6, Batch 20: Training loss = 0.8100
2025-04-03 18:20:53,925 - INFO - Epoch 6, Batch 21: Training loss = 0.8000
2025-04-03 18:20:54,026 - INFO - Epoch 6, Batch 22: Training loss = 0.7900
2025-04-03 18:20:54,127 - INFO - Epoch 6, Batch 23: Training loss = 0.7800
2025-04-03 18:20:54,227 - INFO - Epoch 6, Batch 24: Training loss = 0.7700
2025-04-03 18:20:54,328 - INFO - Epoch 6, Batch 25: Training loss = 0.7600
2025-04-03 18:20:54,428 - INFO - Epoch 6, Batch 26: Training loss = 0.7500
2025-04-03 18:20:54,529 - INFO - Epoch 6, Batch 27: Training loss = 0.7400
2025-04-03 18:20:54,629 - INFO - Epoch 6, Batch 28: Training loss = 0.7300
2025-04-03 18:20:54,730 - INFO - Epoch 6, Batch 29: Training loss = 0.7200
2025-04-03 18:20:54,830 - INFO - Epoch 6, Batch 30: Training loss = 0.7100
2025-04-03 18:20:54,931 - INFO - Epoch 6, Batch 31: Training loss = 0.7000
2025-04-03 18:20:55,032 - INFO - Epoch 6, Batch 32: Training loss = 0.6900
2025-04-03 18:20:55,133 - INFO - Epoch 6, Batch 33: Training loss = 0.6800
2025-04-03 18:20:55,233 - INFO - Epoch 6, Batch 34: Training loss = 0.6700
2025-04-03 18:20:55,334 - INFO - Epoch 6, Batch 35: Training loss = 0.6600
2025-04-03 18:20:55,435 - INFO - Epoch 6, Batch 36: Training loss = 0.6500
2025-04-03 18:20:55,536 - INFO - Epoch 6, Batch 37: Training loss = 0.6400
2025-04-03 18:20:55,637 - INFO - Epoch 6, Batch 38: Training loss = 0.6300
2025-04-03 18:20:55,738 - INFO - Epoch 6, Batch 39: Training loss = 0.6200
2025-04-03 18:20:55,839 - INFO - Epoch 6, Batch 40: Training loss = 0.6100
2025-04-03 18:20:55,939 - INFO - Epoch 6, Batch 41: Training loss = 0.6000
2025-04-03 18:20:56,040 - INFO - Epoch 6, Batch 42: Training loss = 0.5900
2025-04-03 18:20:56,141 - INFO - Epoch 6, Batch 43: Training loss = 0.5800
2025-04-03 18:20:56,242 - INFO - Epoch 6, Batch 44: Training loss = 0.5700
2025-04-03 18:20:56,343 - INFO - Epoch 6, Batch 45: Training loss = 0.5600
2025-04-03 18:20:56,444 - INFO - Epoch 6, Batch 46: Training loss = 0.5500
2025-04-03 18:20:56,545 - INFO - Epoch 6, Batch 47: Training loss = 0.5400
2025-04-03 18:20:56,646 - INFO - Epoch 6, Batch 48: Training loss = 0.5300
2025-04-03 18:20:56,747 - INFO - Epoch 6, Batch 49: Training loss = 0.5200
2025-04-03 18:20:56,848 - INFO - Epoch 6, Batch 50: Training loss = 0.5100
2025-04-03 18:20:56,949 - INFO - Epoch 6, Batch 51: Training loss = 0.5000
2025-04-03 18:20:57,049 - INFO - Epoch 6, Batch 52: Training loss = 0.4900
2025-04-03 18:20:57,150 - INFO - Epoch 6, Batch 53: Training loss = 0.4800
2025-04-03 18:20:57,251 - INFO - Epoch 6, Batch 54: Training loss = 0.4700
2025-04-03 18:20:57,352 - INFO - Epoch 6, Batch 55: Training loss = 0.4600
2025-04-03 18:20:57,454 - INFO - Epoch 6, Batch 56: Training loss = 0.4500
2025-04-03 18:20:57,555 - INFO - Epoch 6, Batch 57: Training loss = 0.4400
2025-04-03 18:20:57,656 - INFO - Epoch 6, Batch 58: Training loss = 0.4300
2025-04-03 18:20:57,757 - INFO - Epoch 6, Batch 59: Training loss = 0.4200
2025-04-03 18:20:57,858 - INFO - Epoch 6, Batch 60: Training loss = 0.4100
2025-04-03 18:20:57,959 - INFO - Epoch 6, Batch 61: Training loss = 0.4000
2025-04-03 18:20:58,060 - INFO - Epoch 6, Batch 62: Training loss = 0.3900
2025-04-03 18:20:58,161 - INFO - Epoch 6, Batch 63: Training loss = 0.3800
2025-04-03 18:20:58,261 - INFO - Epoch 6, Batch 64: Training loss = 0.3700
2025-04-03 18:20:58,362 - INFO - Epoch 6, Batch 65: Training loss = 0.3600
2025-04-03 18:20:58,463 - INFO - Epoch 6, Batch 66: Training loss = 0.3500
2025-04-03 18:20:58,565 - INFO - Epoch 6, Batch 67: Training loss = 0.3400
2025-04-03 18:20:58,666 - INFO - Epoch 6, Batch 68: Training loss = 0.3300
2025-04-03 18:20:58,767 - INFO - Epoch 6, Batch 69: Training loss = 0.3200
2025-04-03 18:20:58,869 - INFO - Epoch 6, Batch 70: Training loss = 0.3100
2025-04-03 18:20:58,969 - INFO - Epoch 6, Batch 71: Training loss = 0.3000
2025-04-03 18:20:59,070 - INFO - Epoch 6, Batch 72: Training loss = 0.2900
2025-04-03 18:20:59,171 - INFO - Epoch 6, Batch 73: Training loss = 0.2800
2025-04-03 18:20:59,272 - INFO - Epoch 6, Batch 74: Training loss = 0.2700
2025-04-03 18:20:59,373 - INFO - Epoch 6, Batch 75: Training loss = 0.2600
2025-04-03 18:20:59,474 - INFO - Epoch 6, Batch 76: Training loss = 0.2500
2025-04-03 18:20:59,575 - INFO - Epoch 6, Batch 77: Training loss = 0.2400
2025-04-03 18:20:59,676 - INFO - Epoch 6, Batch 78: Training loss = 0.2300
2025-04-03 18:20:59,777 - INFO - Epoch 6, Batch 79: Training loss = 0.2200
2025-04-03 18:20:59,878 - INFO - Epoch 6, Batch 80: Training loss = 0.2100
2025-04-03 18:20:59,979 - INFO - Epoch 6, Batch 81: Training loss = 0.2000
2025-04-03 18:21:00,079 - INFO - Epoch 6, Batch 82: Training loss = 0.1900
2025-04-03 18:21:00,180 - INFO - Epoch 6, Batch 83: Training loss = 0.1800
2025-04-03 18:21:00,281 - INFO - Epoch 6, Batch 84: Training loss = 0.1700
2025-04-03 18:21:00,381 - INFO - Epoch 6, Batch 85: Training loss = 0.1600
2025-04-03 18:21:00,482 - INFO - Epoch 6, Batch 86: Training loss = 0.1500
2025-04-03 18:21:00,582 - INFO - Epoch 6, Batch 87: Training loss = 0.1400
2025-04-03 18:21:00,683 - INFO - Epoch 6, Batch 88: Training loss = 0.1300
2025-04-03 18:21:00,784 - INFO - Epoch 6, Batch 89: Training loss = 0.1200
2025-04-03 18:21:00,885 - INFO - Epoch 6, Batch 90: Training loss = 0.1100
2025-04-03 18:21:00,985 - INFO - Epoch 6, Batch 91: Training loss = 0.1000
2025-04-03 18:21:01,086 - INFO - Epoch 6, Batch 92: Training loss = 0.0900
2025-04-03 18:21:01,187 - INFO - Epoch 6, Batch 93: Training loss = 0.0800
2025-04-03 18:21:01,288 - INFO - Epoch 6, Batch 94: Training loss = 0.0700
2025-04-03 18:21:01,389 - INFO - Epoch 6, Batch 95: Training loss = 0.0600
2025-04-03 18:21:01,489 - INFO - Epoch 6, Batch 96: Training loss = 0.0500
2025-04-03 18:21:01,590 - INFO - Epoch 6, Batch 97: Training loss = 0.0400
2025-04-03 18:21:01,690 - INFO - Epoch 6, Batch 98: Training loss = 0.0300
2025-04-03 18:21:01,791 - INFO - Epoch 6, Batch 99: Training loss = 0.0200
2025-04-03 18:21:01,892 - INFO - Epoch 6, Batch 100: Training loss = 0.0100
2025-04-03 18:21:01,993 - INFO - Epoch 6: Validation loss = 0.0500, Validation accuracy = 0.9000
2025-04-03 18:21:01,993 - INFO - Epoch 6: MSE = 0.0500, RMSE = 0.2236, MAE = 0.0400, RÂ² = 0.9500
2025-04-03 18:21:01,993 - INFO - Epoch 7 started.
2025-04-03 18:21:01,993 - INFO - Epoch 7, Batch 1: Training loss = 1.0000
2025-04-03 18:21:02,094 - INFO - Epoch 7, Batch 2: Training loss = 0.9900
2025-04-03 18:21:02,194 - INFO - Epoch 7, Batch 3: Training loss = 0.9800
2025-04-03 18:21:02,296 - INFO - Epoch 7, Batch 4: Training loss = 0.9700
2025-04-03 18:21:02,396 - INFO - Epoch 7, Batch 5: Training loss = 0.9600
2025-04-03 18:21:02,496 - INFO - Epoch 7, Batch 6: Training loss = 0.9500
2025-04-03 18:21:02,597 - INFO - Epoch 7, Batch 7: Training loss = 0.9400
2025-04-03 18:21:02,698 - INFO - Epoch 7, Batch 8: Training loss = 0.9300
2025-04-03 18:21:02,798 - INFO - Epoch 7, Batch 9: Training loss = 0.9200
2025-04-03 18:21:02,899 - INFO - Epoch 7, Batch 10: Training loss = 0.9100
2025-04-03 18:21:03,000 - INFO - Epoch 7, Batch 11: Training loss = 0.9000
2025-04-03 18:21:03,100 - INFO - Epoch 7, Batch 12: Training loss = 0.8900
2025-04-03 18:21:03,201 - INFO - Epoch 7, Batch 13: Training loss = 0.8800
2025-04-03 18:21:03,302 - INFO - Epoch 7, Batch 14: Training loss = 0.8700
2025-04-03 18:21:03,402 - INFO - Epoch 7, Batch 15: Training loss = 0.8600
2025-04-03 18:21:03,503 - INFO - Epoch 7, Batch 16: Training loss = 0.8500
2025-04-03 18:21:03,604 - INFO - Epoch 7, Batch 17: Training loss = 0.8400
2025-04-03 18:21:03,704 - INFO - Epoch 7, Batch 18: Training loss = 0.8300
2025-04-03 18:21:03,805 - INFO - Epoch 7, Batch 19: Training loss = 0.8200
2025-04-03 18:21:03,906 - INFO - Epoch 7, Batch 20: Training loss = 0.8100
2025-04-03 18:21:04,007 - INFO - Epoch 7, Batch 21: Training loss = 0.8000
2025-04-03 18:21:04,108 - INFO - Epoch 7, Batch 22: Training loss = 0.7900
2025-04-03 18:21:04,209 - INFO - Epoch 7, Batch 23: Training loss = 0.7800
2025-04-03 18:21:04,309 - INFO - Epoch 7, Batch 24: Training loss = 0.7700
2025-04-03 18:21:04,410 - INFO - Epoch 7, Batch 25: Training loss = 0.7600
2025-04-03 18:21:04,511 - INFO - Epoch 7, Batch 26: Training loss = 0.7500
2025-04-03 18:21:04,612 - INFO - Epoch 7, Batch 27: Training loss = 0.7400
2025-04-03 18:21:04,713 - INFO - Epoch 7, Batch 28: Training loss = 0.7300
2025-04-03 18:21:04,813 - INFO - Epoch 7, Batch 29: Training loss = 0.7200
2025-04-03 18:21:04,914 - INFO - Epoch 7, Batch 30: Training loss = 0.7100
2025-04-03 18:21:05,015 - INFO - Epoch 7, Batch 31: Training loss = 0.7000
2025-04-03 18:21:05,116 - INFO - Epoch 7, Batch 32: Training loss = 0.6900
2025-04-03 18:21:05,216 - INFO - Epoch 7, Batch 33: Training loss = 0.6800
2025-04-03 18:21:05,317 - INFO - Epoch 7, Batch 34: Training loss = 0.6700
2025-04-03 18:21:05,417 - INFO - Epoch 7, Batch 35: Training loss = 0.6600
2025-04-03 18:21:05,518 - INFO - Epoch 7, Batch 36: Training loss = 0.6500
2025-04-03 18:21:05,619 - INFO - Epoch 7, Batch 37: Training loss = 0.6400
2025-04-03 18:21:05,719 - INFO - Epoch 7, Batch 38: Training loss = 0.6300
2025-04-03 18:21:05,820 - INFO - Epoch 7, Batch 39: Training loss = 0.6200
2025-04-03 18:21:05,921 - INFO - Epoch 7, Batch 40: Training loss = 0.6100
2025-04-03 18:21:06,021 - INFO - Epoch 7, Batch 41: Training loss = 0.6000
2025-04-03 18:21:06,121 - INFO - Epoch 7, Batch 42: Training loss = 0.5900
2025-04-03 18:21:06,222 - INFO - Epoch 7, Batch 43: Training loss = 0.5800
2025-04-03 18:21:06,323 - INFO - Epoch 7, Batch 44: Training loss = 0.5700
2025-04-03 18:21:06,423 - INFO - Epoch 7, Batch 45: Training loss = 0.5600
2025-04-03 18:21:06,524 - INFO - Epoch 7, Batch 46: Training loss = 0.5500
2025-04-03 18:21:06,624 - INFO - Epoch 7, Batch 47: Training loss = 0.5400
2025-04-03 18:21:06,725 - INFO - Epoch 7, Batch 48: Training loss = 0.5300
2025-04-03 18:21:06,826 - INFO - Epoch 7, Batch 49: Training loss = 0.5200
2025-04-03 18:21:06,926 - INFO - Epoch 7, Batch 50: Training loss = 0.5100
2025-04-03 18:21:07,027 - INFO - Epoch 7, Batch 51: Training loss = 0.5000
2025-04-03 18:21:07,127 - INFO - Epoch 7, Batch 52: Training loss = 0.4900
2025-04-03 18:21:07,228 - INFO - Epoch 7, Batch 53: Training loss = 0.4800
2025-04-03 18:21:07,328 - INFO - Epoch 7, Batch 54: Training loss = 0.4700
2025-04-03 18:21:07,429 - INFO - Epoch 7, Batch 55: Training loss = 0.4600
2025-04-03 18:21:07,529 - INFO - Epoch 7, Batch 56: Training loss = 0.4500
2025-04-03 18:21:07,629 - INFO - Epoch 7, Batch 57: Training loss = 0.4400
2025-04-03 18:21:07,730 - INFO - Epoch 7, Batch 58: Training loss = 0.4300
2025-04-03 18:21:07,831 - INFO - Epoch 7, Batch 59: Training loss = 0.4200
2025-04-03 18:21:07,931 - INFO - Epoch 7, Batch 60: Training loss = 0.4100
2025-04-03 18:21:08,032 - INFO - Epoch 7, Batch 61: Training loss = 0.4000
2025-04-03 18:21:08,133 - INFO - Epoch 7, Batch 62: Training loss = 0.3900
2025-04-03 18:21:08,234 - INFO - Epoch 7, Batch 63: Training loss = 0.3800
2025-04-03 18:21:08,334 - INFO - Epoch 7, Batch 64: Training loss = 0.3700
2025-04-03 18:21:08,435 - INFO - Epoch 7, Batch 65: Training loss = 0.3600
2025-04-03 18:21:08,535 - INFO - Epoch 7, Batch 66: Training loss = 0.3500
2025-04-03 18:21:08,636 - INFO - Epoch 7, Batch 67: Training loss = 0.3400
2025-04-03 18:21:08,737 - INFO - Epoch 7, Batch 68: Training loss = 0.3300
2025-04-03 18:21:08,837 - INFO - Epoch 7, Batch 69: Training loss = 0.3200
2025-04-03 18:21:08,938 - INFO - Epoch 7, Batch 70: Training loss = 0.3100
2025-04-03 18:21:09,038 - INFO - Epoch 7, Batch 71: Training loss = 0.3000
2025-04-03 18:21:09,139 - INFO - Epoch 7, Batch 72: Training loss = 0.2900
2025-04-03 18:21:09,239 - INFO - Epoch 7, Batch 73: Training loss = 0.2800
2025-04-03 18:21:09,339 - INFO - Epoch 7, Batch 74: Training loss = 0.2700
2025-04-03 18:21:09,440 - INFO - Epoch 7, Batch 75: Training loss = 0.2600
2025-04-03 18:21:09,540 - INFO - Epoch 7, Batch 76: Training loss = 0.2500
2025-04-03 18:21:09,641 - INFO - Epoch 7, Batch 77: Training loss = 0.2400
2025-04-03 18:21:09,742 - INFO - Epoch 7, Batch 78: Training loss = 0.2300
2025-04-03 18:21:09,842 - INFO - Epoch 7, Batch 79: Training loss = 0.2200
2025-04-03 18:21:09,943 - INFO - Epoch 7, Batch 80: Training loss = 0.2100
2025-04-03 18:21:10,044 - INFO - Epoch 7, Batch 81: Training loss = 0.2000
2025-04-03 18:21:10,145 - INFO - Epoch 7, Batch 82: Training loss = 0.1900
2025-04-03 18:21:10,246 - INFO - Epoch 7, Batch 83: Training loss = 0.1800
2025-04-03 18:21:10,347 - INFO - Epoch 7, Batch 84: Training loss = 0.1700
2025-04-03 18:21:10,447 - INFO - Epoch 7, Batch 85: Training loss = 0.1600
2025-04-03 18:21:10,549 - INFO - Epoch 7, Batch 86: Training loss = 0.1500
2025-04-03 18:21:10,650 - INFO - Epoch 7, Batch 87: Training loss = 0.1400
2025-04-03 18:21:10,751 - INFO - Epoch 7, Batch 88: Training loss = 0.1300
2025-04-03 18:21:10,851 - INFO - Epoch 7, Batch 89: Training loss = 0.1200
2025-04-03 18:21:10,952 - INFO - Epoch 7, Batch 90: Training loss = 0.1100
2025-04-03 18:21:11,053 - INFO - Epoch 7, Batch 91: Training loss = 0.1000
2025-04-03 18:21:11,154 - INFO - Epoch 7, Batch 92: Training loss = 0.0900
2025-04-03 18:21:11,255 - INFO - Epoch 7, Batch 93: Training loss = 0.0800
2025-04-03 18:21:11,355 - INFO - Epoch 7, Batch 94: Training loss = 0.0700
2025-04-03 18:21:11,456 - INFO - Epoch 7, Batch 95: Training loss = 0.0600
2025-04-03 18:21:11,557 - INFO - Epoch 7, Batch 96: Training loss = 0.0500
2025-04-03 18:21:11,658 - INFO - Epoch 7, Batch 97: Training loss = 0.0400
2025-04-03 18:21:11,758 - INFO - Epoch 7, Batch 98: Training loss = 0.0300
2025-04-03 18:21:11,859 - INFO - Epoch 7, Batch 99: Training loss = 0.0200
2025-04-03 18:21:11,960 - INFO - Epoch 7, Batch 100: Training loss = 0.0100
2025-04-03 18:21:12,061 - INFO - Epoch 7: Validation loss = 0.0400, Validation accuracy = 0.9200
2025-04-03 18:21:12,062 - INFO - Epoch 7: MSE = 0.0400, RMSE = 0.2000, MAE = 0.0320, RÂ² = 0.9600
2025-04-03 18:21:12,062 - INFO - Epoch 8 started.
2025-04-03 18:21:12,063 - INFO - Epoch 8, Batch 1: Training loss = 1.0000
2025-04-03 18:21:12,163 - INFO - Epoch 8, Batch 2: Training loss = 0.9900
2025-04-03 18:21:12,264 - INFO - Epoch 8, Batch 3: Training loss = 0.9800
2025-04-03 18:21:12,365 - INFO - Epoch 8, Batch 4: Training loss = 0.9700
2025-04-03 18:21:12,466 - INFO - Epoch 8, Batch 5: Training loss = 0.9600
2025-04-03 18:21:12,567 - INFO - Epoch 8, Batch 6: Training loss = 0.9500
2025-04-03 18:21:12,668 - INFO - Epoch 8, Batch 7: Training loss = 0.9400
2025-04-03 18:21:12,770 - INFO - Epoch 8, Batch 8: Training loss = 0.9300
2025-04-03 18:21:12,870 - INFO - Epoch 8, Batch 9: Training loss = 0.9200
2025-04-03 18:21:12,971 - INFO - Epoch 8, Batch 10: Training loss = 0.9100
2025-04-03 18:21:13,072 - INFO - Epoch 8, Batch 11: Training loss = 0.9000
2025-04-03 18:21:13,173 - INFO - Epoch 8, Batch 12: Training loss = 0.8900
2025-04-03 18:21:13,274 - INFO - Epoch 8, Batch 13: Training loss = 0.8800
2025-04-03 18:21:13,375 - INFO - Epoch 8, Batch 14: Training loss = 0.8700
2025-04-03 18:21:13,475 - INFO - Epoch 8, Batch 15: Training loss = 0.8600
2025-04-03 18:21:13,576 - INFO - Epoch 8, Batch 16: Training loss = 0.8500
2025-04-03 18:21:13,677 - INFO - Epoch 8, Batch 17: Training loss = 0.8400
2025-04-03 18:21:13,778 - INFO - Epoch 8, Batch 18: Training loss = 0.8300
2025-04-03 18:21:13,879 - INFO - Epoch 8, Batch 19: Training loss = 0.8200
2025-04-03 18:21:13,979 - INFO - Epoch 8, Batch 20: Training loss = 0.8100
2025-04-03 18:21:14,081 - INFO - Epoch 8, Batch 21: Training loss = 0.8000
2025-04-03 18:21:14,182 - INFO - Epoch 8, Batch 22: Training loss = 0.7900
2025-04-03 18:21:14,282 - INFO - Epoch 8, Batch 23: Training loss = 0.7800
2025-04-03 18:21:14,384 - INFO - Epoch 8, Batch 24: Training loss = 0.7700
2025-04-03 18:21:14,485 - INFO - Epoch 8, Batch 25: Training loss = 0.7600
2025-04-03 18:21:14,586 - INFO - Epoch 8, Batch 26: Training loss = 0.7500
2025-04-03 18:21:14,686 - INFO - Epoch 8, Batch 27: Training loss = 0.7400
2025-04-03 18:21:14,787 - INFO - Epoch 8, Batch 28: Training loss = 0.7300
2025-04-03 18:21:14,888 - INFO - Epoch 8, Batch 29: Training loss = 0.7200
2025-04-03 18:21:14,989 - INFO - Epoch 8, Batch 30: Training loss = 0.7100
2025-04-03 18:21:15,090 - INFO - Epoch 8, Batch 31: Training loss = 0.7000
2025-04-03 18:21:15,191 - INFO - Epoch 8, Batch 32: Training loss = 0.6900
2025-04-03 18:21:15,292 - INFO - Epoch 8, Batch 33: Training loss = 0.6800
2025-04-03 18:21:15,392 - INFO - Epoch 8, Batch 34: Training loss = 0.6700
2025-04-03 18:21:15,493 - INFO - Epoch 8, Batch 35: Training loss = 0.6600
2025-04-03 18:21:15,594 - INFO - Epoch 8, Batch 36: Training loss = 0.6500
2025-04-03 18:21:15,695 - INFO - Epoch 8, Batch 37: Training loss = 0.6400
2025-04-03 18:21:15,795 - INFO - Epoch 8, Batch 38: Training loss = 0.6300
2025-04-03 18:21:15,896 - INFO - Epoch 8, Batch 39: Training loss = 0.6200
2025-04-03 18:21:15,996 - INFO - Epoch 8, Batch 40: Training loss = 0.6100
2025-04-03 18:21:16,097 - INFO - Epoch 8, Batch 41: Training loss = 0.6000
2025-04-03 18:21:16,198 - INFO - Epoch 8, Batch 42: Training loss = 0.5900
2025-04-03 18:21:16,299 - INFO - Epoch 8, Batch 43: Training loss = 0.5800
2025-04-03 18:21:16,400 - INFO - Epoch 8, Batch 44: Training loss = 0.5700
2025-04-03 18:21:16,501 - INFO - Epoch 8, Batch 45: Training loss = 0.5600
2025-04-03 18:21:16,602 - INFO - Epoch 8, Batch 46: Training loss = 0.5500
2025-04-03 18:21:16,703 - INFO - Epoch 8, Batch 47: Training loss = 0.5400
2025-04-03 18:21:16,805 - INFO - Epoch 8, Batch 48: Training loss = 0.5300
2025-04-03 18:21:16,906 - INFO - Epoch 8, Batch 49: Training loss = 0.5200
2025-04-03 18:21:17,006 - INFO - Epoch 8, Batch 50: Training loss = 0.5100
2025-04-03 18:21:17,107 - INFO - Epoch 8, Batch 51: Training loss = 0.5000
2025-04-03 18:21:17,208 - INFO - Epoch 8, Batch 52: Training loss = 0.4900
2025-04-03 18:21:17,309 - INFO - Epoch 8, Batch 53: Training loss = 0.4800
2025-04-03 18:21:17,410 - INFO - Epoch 8, Batch 54: Training loss = 0.4700
2025-04-03 18:21:17,511 - INFO - Epoch 8, Batch 55: Training loss = 0.4600
2025-04-03 18:21:17,612 - INFO - Epoch 8, Batch 56: Training loss = 0.4500
2025-04-03 18:21:17,713 - INFO - Epoch 8, Batch 57: Training loss = 0.4400
2025-04-03 18:21:17,813 - INFO - Epoch 8, Batch 58: Training loss = 0.4300
2025-04-03 18:21:17,914 - INFO - Epoch 8, Batch 59: Training loss = 0.4200
2025-04-03 18:21:18,015 - INFO - Epoch 8, Batch 60: Training loss = 0.4100
2025-04-03 18:21:18,115 - INFO - Epoch 8, Batch 61: Training loss = 0.4000
2025-04-03 18:21:18,216 - INFO - Epoch 8, Batch 62: Training loss = 0.3900
2025-04-03 18:21:18,316 - INFO - Epoch 8, Batch 63: Training loss = 0.3800
2025-04-03 18:21:18,417 - INFO - Epoch 8, Batch 64: Training loss = 0.3700
2025-04-03 18:21:18,518 - INFO - Epoch 8, Batch 65: Training loss = 0.3600
2025-04-03 18:21:18,619 - INFO - Epoch 8, Batch 66: Training loss = 0.3500
2025-04-03 18:21:18,720 - INFO - Epoch 8, Batch 67: Training loss = 0.3400
2025-04-03 18:21:18,820 - INFO - Epoch 8, Batch 68: Training loss = 0.3300
2025-04-03 18:21:18,921 - INFO - Epoch 8, Batch 69: Training loss = 0.3200
2025-04-03 18:21:19,021 - INFO - Epoch 8, Batch 70: Training loss = 0.3100
2025-04-03 18:21:19,123 - INFO - Epoch 8, Batch 71: Training loss = 0.3000
2025-04-03 18:21:19,223 - INFO - Epoch 8, Batch 72: Training loss = 0.2900
2025-04-03 18:21:19,324 - INFO - Epoch 8, Batch 73: Training loss = 0.2800
2025-04-03 18:21:19,424 - INFO - Epoch 8, Batch 74: Training loss = 0.2700
2025-04-03 18:21:19,525 - INFO - Epoch 8, Batch 75: Training loss = 0.2600
2025-04-03 18:21:19,626 - INFO - Epoch 8, Batch 76: Training loss = 0.2500
2025-04-03 18:21:19,726 - INFO - Epoch 8, Batch 77: Training loss = 0.2400
2025-04-03 18:21:19,827 - INFO - Epoch 8, Batch 78: Training loss = 0.2300
2025-04-03 18:21:19,927 - INFO - Epoch 8, Batch 79: Training loss = 0.2200
2025-04-03 18:21:20,028 - INFO - Epoch 8, Batch 80: Training loss = 0.2100
2025-04-03 18:21:20,129 - INFO - Epoch 8, Batch 81: Training loss = 0.2000
2025-04-03 18:21:20,229 - INFO - Epoch 8, Batch 82: Training loss = 0.1900
2025-04-03 18:21:20,330 - INFO - Epoch 8, Batch 83: Training loss = 0.1800
2025-04-03 18:21:20,431 - INFO - Epoch 8, Batch 84: Training loss = 0.1700
2025-04-03 18:21:20,531 - INFO - Epoch 8, Batch 85: Training loss = 0.1600
2025-04-03 18:21:20,632 - INFO - Epoch 8, Batch 86: Training loss = 0.1500
2025-04-03 18:21:20,733 - INFO - Epoch 8, Batch 87: Training loss = 0.1400
2025-04-03 18:21:20,834 - INFO - Epoch 8, Batch 88: Training loss = 0.1300
2025-04-03 18:21:20,935 - INFO - Epoch 8, Batch 89: Training loss = 0.1200
2025-04-03 18:21:21,035 - INFO - Epoch 8, Batch 90: Training loss = 0.1100
2025-04-03 18:21:21,136 - INFO - Epoch 8, Batch 91: Training loss = 0.1000
2025-04-03 18:21:21,237 - INFO - Epoch 8, Batch 92: Training loss = 0.0900
2025-04-03 18:21:21,337 - INFO - Epoch 8, Batch 93: Training loss = 0.0800
2025-04-03 18:21:21,438 - INFO - Epoch 8, Batch 94: Training loss = 0.0700
2025-04-03 18:21:21,539 - INFO - Epoch 8, Batch 95: Training loss = 0.0600
2025-04-03 18:21:21,639 - INFO - Epoch 8, Batch 96: Training loss = 0.0500
2025-04-03 18:21:21,740 - INFO - Epoch 8, Batch 97: Training loss = 0.0400
2025-04-03 18:21:21,841 - INFO - Epoch 8, Batch 98: Training loss = 0.0300
2025-04-03 18:21:21,942 - INFO - Epoch 8, Batch 99: Training loss = 0.0200
2025-04-03 18:21:22,043 - INFO - Epoch 8, Batch 100: Training loss = 0.0100
2025-04-03 18:21:22,143 - INFO - Epoch 8: Validation loss = 0.0300, Validation accuracy = 0.9400
2025-04-03 18:21:22,144 - INFO - Epoch 8: MSE = 0.0300, RMSE = 0.1732, MAE = 0.0240, RÂ² = 0.9700
2025-04-03 18:21:22,144 - INFO - Epoch 9 started.
2025-04-03 18:21:22,144 - INFO - Epoch 9, Batch 1: Training loss = 1.0000
2025-04-03 18:21:22,244 - INFO - Epoch 9, Batch 2: Training loss = 0.9900
2025-04-03 18:21:22,345 - INFO - Epoch 9, Batch 3: Training loss = 0.9800
2025-04-03 18:21:22,446 - INFO - Epoch 9, Batch 4: Training loss = 0.9700
2025-04-03 18:21:22,546 - INFO - Epoch 9, Batch 5: Training loss = 0.9600
2025-04-03 18:21:22,647 - INFO - Epoch 9, Batch 6: Training loss = 0.9500
2025-04-03 18:21:22,748 - INFO - Epoch 9, Batch 7: Training loss = 0.9400
2025-04-03 18:21:22,849 - INFO - Epoch 9, Batch 8: Training loss = 0.9300
2025-04-03 18:21:22,950 - INFO - Epoch 9, Batch 9: Training loss = 0.9200
2025-04-03 18:21:23,051 - INFO - Epoch 9, Batch 10: Training loss = 0.9100
2025-04-03 18:21:23,152 - INFO - Epoch 9, Batch 11: Training loss = 0.9000
2025-04-03 18:21:23,253 - INFO - Epoch 9, Batch 12: Training loss = 0.8900
2025-04-03 18:21:23,354 - INFO - Epoch 9, Batch 13: Training loss = 0.8800
2025-04-03 18:21:23,455 - INFO - Epoch 9, Batch 14: Training loss = 0.8700
2025-04-03 18:21:23,555 - INFO - Epoch 9, Batch 15: Training loss = 0.8600
2025-04-03 18:21:23,656 - INFO - Epoch 9, Batch 16: Training loss = 0.8500
2025-04-03 18:21:23,756 - INFO - Epoch 9, Batch 17: Training loss = 0.8400
2025-04-03 18:21:23,857 - INFO - Epoch 9, Batch 18: Training loss = 0.8300
2025-04-03 18:21:23,958 - INFO - Epoch 9, Batch 19: Training loss = 0.8200
2025-04-03 18:21:24,059 - INFO - Epoch 9, Batch 20: Training loss = 0.8100
2025-04-03 18:21:24,160 - INFO - Epoch 9, Batch 21: Training loss = 0.8000
2025-04-03 18:21:24,261 - INFO - Epoch 9, Batch 22: Training loss = 0.7900
2025-04-03 18:21:24,361 - INFO - Epoch 9, Batch 23: Training loss = 0.7800
2025-04-03 18:21:24,462 - INFO - Epoch 9, Batch 24: Training loss = 0.7700
2025-04-03 18:21:24,562 - INFO - Epoch 9, Batch 25: Training loss = 0.7600
2025-04-03 18:21:24,663 - INFO - Epoch 9, Batch 26: Training loss = 0.7500
2025-04-03 18:21:24,764 - INFO - Epoch 9, Batch 27: Training loss = 0.7400
2025-04-03 18:21:24,864 - INFO - Epoch 9, Batch 28: Training loss = 0.7300
2025-04-03 18:21:24,965 - INFO - Epoch 9, Batch 29: Training loss = 0.7200
2025-04-03 18:21:25,066 - INFO - Epoch 9, Batch 30: Training loss = 0.7100
2025-04-03 18:21:25,167 - INFO - Epoch 9, Batch 31: Training loss = 0.7000
2025-04-03 18:21:25,267 - INFO - Epoch 9, Batch 32: Training loss = 0.6900
2025-04-03 18:21:25,368 - INFO - Epoch 9, Batch 33: Training loss = 0.6800
2025-04-03 18:21:25,469 - INFO - Epoch 9, Batch 34: Training loss = 0.6700
2025-04-03 18:21:25,570 - INFO - Epoch 9, Batch 35: Training loss = 0.6600
2025-04-03 18:21:25,671 - INFO - Epoch 9, Batch 36: Training loss = 0.6500
2025-04-03 18:21:25,772 - INFO - Epoch 9, Batch 37: Training loss = 0.6400
2025-04-03 18:21:25,872 - INFO - Epoch 9, Batch 38: Training loss = 0.6300
2025-04-03 18:21:25,973 - INFO - Epoch 9, Batch 39: Training loss = 0.6200
2025-04-03 18:21:26,074 - INFO - Epoch 9, Batch 40: Training loss = 0.6100
2025-04-03 18:21:26,175 - INFO - Epoch 9, Batch 41: Training loss = 0.6000
2025-04-03 18:21:26,276 - INFO - Epoch 9, Batch 42: Training loss = 0.5900
2025-04-03 18:21:26,377 - INFO - Epoch 9, Batch 43: Training loss = 0.5800
2025-04-03 18:21:26,478 - INFO - Epoch 9, Batch 44: Training loss = 0.5700
2025-04-03 18:21:26,578 - INFO - Epoch 9, Batch 45: Training loss = 0.5600
2025-04-03 18:21:26,679 - INFO - Epoch 9, Batch 46: Training loss = 0.5500
2025-04-03 18:21:26,779 - INFO - Epoch 9, Batch 47: Training loss = 0.5400
2025-04-03 18:21:26,880 - INFO - Epoch 9, Batch 48: Training loss = 0.5300
2025-04-03 18:21:26,981 - INFO - Epoch 9, Batch 49: Training loss = 0.5200
2025-04-03 18:21:27,081 - INFO - Epoch 9, Batch 50: Training loss = 0.5100
2025-04-03 18:21:27,182 - INFO - Epoch 9, Batch 51: Training loss = 0.5000
2025-04-03 18:21:27,283 - INFO - Epoch 9, Batch 52: Training loss = 0.4900
2025-04-03 18:21:27,384 - INFO - Epoch 9, Batch 53: Training loss = 0.4800
2025-04-03 18:21:27,485 - INFO - Epoch 9, Batch 54: Training loss = 0.4700
2025-04-03 18:21:27,586 - INFO - Epoch 9, Batch 55: Training loss = 0.4600
2025-04-03 18:21:27,687 - INFO - Epoch 9, Batch 56: Training loss = 0.4500
2025-04-03 18:21:27,787 - INFO - Epoch 9, Batch 57: Training loss = 0.4400
2025-04-03 18:21:27,888 - INFO - Epoch 9, Batch 58: Training loss = 0.4300
2025-04-03 18:21:27,989 - INFO - Epoch 9, Batch 59: Training loss = 0.4200
2025-04-03 18:21:28,090 - INFO - Epoch 9, Batch 60: Training loss = 0.4100
2025-04-03 18:21:28,191 - INFO - Epoch 9, Batch 61: Training loss = 0.4000
2025-04-03 18:21:28,292 - INFO - Epoch 9, Batch 62: Training loss = 0.3900
2025-04-03 18:21:28,392 - INFO - Epoch 9, Batch 63: Training loss = 0.3800
2025-04-03 18:21:28,493 - INFO - Epoch 9, Batch 64: Training loss = 0.3700
2025-04-03 18:21:28,594 - INFO - Epoch 9, Batch 65: Training loss = 0.3600
2025-04-03 18:21:28,695 - INFO - Epoch 9, Batch 66: Training loss = 0.3500
2025-04-03 18:21:28,796 - INFO - Epoch 9, Batch 67: Training loss = 0.3400
2025-04-03 18:21:28,897 - INFO - Epoch 9, Batch 68: Training loss = 0.3300
2025-04-03 18:21:28,997 - INFO - Epoch 9, Batch 69: Training loss = 0.3200
2025-04-03 18:21:29,098 - INFO - Epoch 9, Batch 70: Training loss = 0.3100
2025-04-03 18:21:29,199 - INFO - Epoch 9, Batch 71: Training loss = 0.3000
2025-04-03 18:21:29,299 - INFO - Epoch 9, Batch 72: Training loss = 0.2900
2025-04-03 18:21:29,400 - INFO - Epoch 9, Batch 73: Training loss = 0.2800
2025-04-03 18:21:29,501 - INFO - Epoch 9, Batch 74: Training loss = 0.2700
2025-04-03 18:21:29,602 - INFO - Epoch 9, Batch 75: Training loss = 0.2600
2025-04-03 18:21:29,704 - INFO - Epoch 9, Batch 76: Training loss = 0.2500
2025-04-03 18:21:29,804 - INFO - Epoch 9, Batch 77: Training loss = 0.2400
2025-04-03 18:21:29,905 - INFO - Epoch 9, Batch 78: Training loss = 0.2300
2025-04-03 18:21:30,006 - INFO - Epoch 9, Batch 79: Training loss = 0.2200
2025-04-03 18:21:30,106 - INFO - Epoch 9, Batch 80: Training loss = 0.2100
2025-04-03 18:21:30,207 - INFO - Epoch 9, Batch 81: Training loss = 0.2000
2025-04-03 18:21:30,308 - INFO - Epoch 9, Batch 82: Training loss = 0.1900
2025-04-03 18:21:30,409 - INFO - Epoch 9, Batch 83: Training loss = 0.1800
2025-04-03 18:21:30,510 - INFO - Epoch 9, Batch 84: Training loss = 0.1700
2025-04-03 18:21:30,610 - INFO - Epoch 9, Batch 85: Training loss = 0.1600
2025-04-03 18:21:30,711 - INFO - Epoch 9, Batch 86: Training loss = 0.1500
2025-04-03 18:21:30,812 - INFO - Epoch 9, Batch 87: Training loss = 0.1400
2025-04-03 18:21:30,913 - INFO - Epoch 9, Batch 88: Training loss = 0.1300
2025-04-03 18:21:31,014 - INFO - Epoch 9, Batch 89: Training loss = 0.1200
2025-04-03 18:21:31,114 - INFO - Epoch 9, Batch 90: Training loss = 0.1100
2025-04-03 18:21:31,215 - INFO - Epoch 9, Batch 91: Training loss = 0.1000
2025-04-03 18:21:31,316 - INFO - Epoch 9, Batch 92: Training loss = 0.0900
2025-04-03 18:21:31,417 - INFO - Epoch 9, Batch 93: Training loss = 0.0800
2025-04-03 18:21:31,517 - INFO - Epoch 9, Batch 94: Training loss = 0.0700
2025-04-03 18:21:31,618 - INFO - Epoch 9, Batch 95: Training loss = 0.0600
2025-04-03 18:21:31,718 - INFO - Epoch 9, Batch 96: Training loss = 0.0500
2025-04-03 18:21:31,819 - INFO - Epoch 9, Batch 97: Training loss = 0.0400
2025-04-03 18:21:31,919 - INFO - Epoch 9, Batch 98: Training loss = 0.0300
2025-04-03 18:21:32,020 - INFO - Epoch 9, Batch 99: Training loss = 0.0200
2025-04-03 18:21:32,121 - INFO - Epoch 9, Batch 100: Training loss = 0.0100
2025-04-03 18:21:32,221 - INFO - Epoch 9: Validation loss = 0.0200, Validation accuracy = 0.9600
2025-04-03 18:21:32,222 - INFO - Epoch 9: MSE = 0.0200, RMSE = 0.1414, MAE = 0.0160, RÂ² = 0.9800
2025-04-03 18:21:32,222 - INFO - Epoch 10 started.
2025-04-03 18:21:32,222 - INFO - Epoch 10, Batch 1: Training loss = 1.0000
2025-04-03 18:21:32,323 - INFO - Epoch 10, Batch 2: Training loss = 0.9900
2025-04-03 18:21:32,424 - INFO - Epoch 10, Batch 3: Training loss = 0.9800
2025-04-03 18:21:32,524 - INFO - Epoch 10, Batch 4: Training loss = 0.9700
2025-04-03 18:21:32,625 - INFO - Epoch 10, Batch 5: Training loss = 0.9600
2025-04-03 18:21:32,726 - INFO - Epoch 10, Batch 6: Training loss = 0.9500
2025-04-03 18:21:32,826 - INFO - Epoch 10, Batch 7: Training loss = 0.9400
2025-04-03 18:21:32,927 - INFO - Epoch 10, Batch 8: Training loss = 0.9300
2025-04-03 18:21:33,028 - INFO - Epoch 10, Batch 9: Training loss = 0.9200
2025-04-03 18:21:33,129 - INFO - Epoch 10, Batch 10: Training loss = 0.9100
2025-04-03 18:21:33,230 - INFO - Epoch 10, Batch 11: Training loss = 0.9000
2025-04-03 18:21:33,332 - INFO - Epoch 10, Batch 12: Training loss = 0.8900
2025-04-03 18:21:33,432 - INFO - Epoch 10, Batch 13: Training loss = 0.8800
2025-04-03 18:21:33,533 - INFO - Epoch 10, Batch 14: Training loss = 0.8700
2025-04-03 18:21:33,634 - INFO - Epoch 10, Batch 15: Training loss = 0.8600
2025-04-03 18:21:33,735 - INFO - Epoch 10, Batch 16: Training loss = 0.8500
2025-04-03 18:21:33,835 - INFO - Epoch 10, Batch 17: Training loss = 0.8400
2025-04-03 18:21:33,936 - INFO - Epoch 10, Batch 18: Training loss = 0.8300
2025-04-03 18:21:34,037 - INFO - Epoch 10, Batch 19: Training loss = 0.8200
2025-04-03 18:21:34,138 - INFO - Epoch 10, Batch 20: Training loss = 0.8100
2025-04-03 18:21:34,239 - INFO - Epoch 10, Batch 21: Training loss = 0.8000
2025-04-03 18:21:34,340 - INFO - Epoch 10, Batch 22: Training loss = 0.7900
2025-04-03 18:21:34,440 - INFO - Epoch 10, Batch 23: Training loss = 0.7800
2025-04-03 18:21:34,541 - INFO - Epoch 10, Batch 24: Training loss = 0.7700
2025-04-03 18:21:34,642 - INFO - Epoch 10, Batch 25: Training loss = 0.7600
2025-04-03 18:21:34,743 - INFO - Epoch 10, Batch 26: Training loss = 0.7500
2025-04-03 18:21:34,844 - INFO - Epoch 10, Batch 27: Training loss = 0.7400
2025-04-03 18:21:34,945 - INFO - Epoch 10, Batch 28: Training loss = 0.7300
2025-04-03 18:21:35,046 - INFO - Epoch 10, Batch 29: Training loss = 0.7200
2025-04-03 18:21:35,147 - INFO - Epoch 10, Batch 30: Training loss = 0.7100
2025-04-03 18:21:35,248 - INFO - Epoch 10, Batch 31: Training loss = 0.7000
2025-04-03 18:21:35,349 - INFO - Epoch 10, Batch 32: Training loss = 0.6900
2025-04-03 18:21:35,449 - INFO - Epoch 10, Batch 33: Training loss = 0.6800
2025-04-03 18:21:35,550 - INFO - Epoch 10, Batch 34: Training loss = 0.6700
2025-04-03 18:21:35,651 - INFO - Epoch 10, Batch 35: Training loss = 0.6600
2025-04-03 18:21:35,752 - INFO - Epoch 10, Batch 36: Training loss = 0.6500
2025-04-03 18:21:35,853 - INFO - Epoch 10, Batch 37: Training loss = 0.6400
2025-04-03 18:21:35,953 - INFO - Epoch 10, Batch 38: Training loss = 0.6300
2025-04-03 18:21:36,054 - INFO - Epoch 10, Batch 39: Training loss = 0.6200
2025-04-03 18:21:36,155 - INFO - Epoch 10, Batch 40: Training loss = 0.6100
2025-04-03 18:21:36,256 - INFO - Epoch 10, Batch 41: Training loss = 0.6000
2025-04-03 18:21:36,357 - INFO - Epoch 10, Batch 42: Training loss = 0.5900
2025-04-03 18:21:36,458 - INFO - Epoch 10, Batch 43: Training loss = 0.5800
2025-04-03 18:21:36,559 - INFO - Epoch 10, Batch 44: Training loss = 0.5700
2025-04-03 18:21:36,660 - INFO - Epoch 10, Batch 45: Training loss = 0.5600
2025-04-03 18:21:36,760 - INFO - Epoch 10, Batch 46: Training loss = 0.5500
2025-04-03 18:21:36,861 - INFO - Epoch 10, Batch 47: Training loss = 0.5400
2025-04-03 18:21:36,962 - INFO - Epoch 10, Batch 48: Training loss = 0.5300
2025-04-03 18:21:37,063 - INFO - Epoch 10, Batch 49: Training loss = 0.5200
2025-04-03 18:21:37,164 - INFO - Epoch 10, Batch 50: Training loss = 0.5100
2025-04-03 18:21:37,265 - INFO - Epoch 10, Batch 51: Training loss = 0.5000
2025-04-03 18:21:37,366 - INFO - Epoch 10, Batch 52: Training loss = 0.4900
2025-04-03 18:21:37,467 - INFO - Epoch 10, Batch 53: Training loss = 0.4800
2025-04-03 18:21:37,566 - INFO - Epoch 10, Batch 54: Training loss = 0.4700
2025-04-03 18:21:37,667 - INFO - Epoch 10, Batch 55: Training loss = 0.4600
2025-04-03 18:21:37,768 - INFO - Epoch 10, Batch 56: Training loss = 0.4500
2025-04-03 18:21:37,869 - INFO - Epoch 10, Batch 57: Training loss = 0.4400
2025-04-03 18:21:37,969 - INFO - Epoch 10, Batch 58: Training loss = 0.4300
2025-04-03 18:21:38,070 - INFO - Epoch 10, Batch 59: Training loss = 0.4200
2025-04-03 18:21:38,171 - INFO - Epoch 10, Batch 60: Training loss = 0.4100
2025-04-03 18:21:38,272 - INFO - Epoch 10, Batch 61: Training loss = 0.4000
2025-04-03 18:21:38,373 - INFO - Epoch 10, Batch 62: Training loss = 0.3900
2025-04-03 18:21:38,474 - INFO - Epoch 10, Batch 63: Training loss = 0.3800
2025-04-03 18:21:38,575 - INFO - Epoch 10, Batch 64: Training loss = 0.3700
2025-04-03 18:21:38,677 - INFO - Epoch 10, Batch 65: Training loss = 0.3600
2025-04-03 18:21:38,778 - INFO - Epoch 10, Batch 66: Training loss = 0.3500
2025-04-03 18:21:38,879 - INFO - Epoch 10, Batch 67: Training loss = 0.3400
2025-04-03 18:21:38,979 - INFO - Epoch 10, Batch 68: Training loss = 0.3300
2025-04-03 18:21:39,080 - INFO - Epoch 10, Batch 69: Training loss = 0.3200
2025-04-03 18:21:39,181 - INFO - Epoch 10, Batch 70: Training loss = 0.3100
2025-04-03 18:21:39,281 - INFO - Epoch 10, Batch 71: Training loss = 0.3000
2025-04-03 18:21:39,382 - INFO - Epoch 10, Batch 72: Training loss = 0.2900
2025-04-03 18:21:39,483 - INFO - Epoch 10, Batch 73: Training loss = 0.2800
2025-04-03 18:21:39,585 - INFO - Epoch 10, Batch 74: Training loss = 0.2700
2025-04-03 18:21:39,686 - INFO - Epoch 10, Batch 75: Training loss = 0.2600
2025-04-03 18:21:39,787 - INFO - Epoch 10, Batch 76: Training loss = 0.2500
2025-04-03 18:21:39,888 - INFO - Epoch 10, Batch 77: Training loss = 0.2400
2025-04-03 18:21:39,989 - INFO - Epoch 10, Batch 78: Training loss = 0.2300
2025-04-03 18:21:40,090 - INFO - Epoch 10, Batch 79: Training loss = 0.2200
2025-04-03 18:21:40,190 - INFO - Epoch 10, Batch 80: Training loss = 0.2100
2025-04-03 18:21:40,291 - INFO - Epoch 10, Batch 81: Training loss = 0.2000
2025-04-03 18:21:40,392 - INFO - Epoch 10, Batch 82: Training loss = 0.1900
2025-04-03 18:21:40,493 - INFO - Epoch 10, Batch 83: Training loss = 0.1800
2025-04-03 18:21:40,594 - INFO - Epoch 10, Batch 84: Training loss = 0.1700
2025-04-03 18:21:40,695 - INFO - Epoch 10, Batch 85: Training loss = 0.1600
2025-04-03 18:21:40,796 - INFO - Epoch 10, Batch 86: Training loss = 0.1500
2025-04-03 18:21:40,896 - INFO - Epoch 10, Batch 87: Training loss = 0.1400
2025-04-03 18:21:40,997 - INFO - Epoch 10, Batch 88: Training loss = 0.1300
2025-04-03 18:21:41,098 - INFO - Epoch 10, Batch 89: Training loss = 0.1200
2025-04-03 18:21:41,198 - INFO - Epoch 10, Batch 90: Training loss = 0.1100
2025-04-03 18:21:41,299 - INFO - Epoch 10, Batch 91: Training loss = 0.1000
2025-04-03 18:21:41,400 - INFO - Epoch 10, Batch 92: Training loss = 0.0900
2025-04-03 18:21:41,501 - INFO - Epoch 10, Batch 93: Training loss = 0.0800
2025-04-03 18:21:41,602 - INFO - Epoch 10, Batch 94: Training loss = 0.0700
2025-04-03 18:21:41,703 - INFO - Epoch 10, Batch 95: Training loss = 0.0600
2025-04-03 18:21:41,803 - INFO - Epoch 10, Batch 96: Training loss = 0.0500
2025-04-03 18:21:41,904 - INFO - Epoch 10, Batch 97: Training loss = 0.0400
2025-04-03 18:21:42,005 - INFO - Epoch 10, Batch 98: Training loss = 0.0300
2025-04-03 18:21:42,106 - INFO - Epoch 10, Batch 99: Training loss = 0.0200
2025-04-03 18:21:42,206 - INFO - Epoch 10, Batch 100: Training loss = 0.0100
2025-04-03 18:21:42,307 - INFO - Epoch 10: Validation loss = 0.0100, Validation accuracy = 0.9800
2025-04-03 18:21:42,307 - INFO - Epoch 10: MSE = 0.0100, RMSE = 0.1000, MAE = 0.0080, RÂ² = 0.9900
2025-04-03 18:21:42,307 - INFO - Regression training process completed.
2025-04-03 18:26:32,819 - INFO - Starting regression training process...
2025-04-03 18:26:32,819 - INFO - Epoch 1 started.
2025-04-03 18:26:32,820 - INFO - Epoch 1, Batch 1: Training loss = 1.0000
2025-04-03 18:26:32,920 - INFO - Epoch 1, Batch 2: Training loss = 0.9900
2025-04-03 18:26:33,021 - INFO - Epoch 1, Batch 3: Training loss = 0.9800
2025-04-03 18:26:33,121 - INFO - Epoch 1, Batch 4: Training loss = 0.9700
2025-04-03 18:26:33,222 - INFO - Epoch 1, Batch 5: Training loss = 0.9600
2025-04-03 18:26:33,323 - INFO - Epoch 1, Batch 6: Training loss = 0.9500
2025-04-03 18:26:33,424 - INFO - Epoch 1, Batch 7: Training loss = 0.9400
2025-04-03 18:26:33,525 - INFO - Epoch 1, Batch 8: Training loss = 0.9300
2025-04-03 18:26:33,626 - INFO - Epoch 1, Batch 9: Training loss = 0.9200
2025-04-03 18:26:33,727 - INFO - Epoch 1, Batch 10: Training loss = 0.9100
2025-04-03 18:26:33,828 - INFO - Epoch 1, Batch 11: Training loss = 0.9000
2025-04-03 18:26:33,929 - INFO - Epoch 1, Batch 12: Training loss = 0.8900
2025-04-03 18:26:34,029 - INFO - Epoch 1, Batch 13: Training loss = 0.8800
2025-04-03 18:26:34,130 - INFO - Epoch 1, Batch 14: Training loss = 0.8700
2025-04-03 18:26:34,231 - INFO - Epoch 1, Batch 15: Training loss = 0.8600
2025-04-03 18:26:34,332 - INFO - Epoch 1, Batch 16: Training loss = 0.8500
2025-04-03 18:26:34,433 - INFO - Epoch 1, Batch 17: Training loss = 0.8400
2025-04-03 18:26:34,534 - INFO - Epoch 1, Batch 18: Training loss = 0.8300
2025-04-03 18:26:34,634 - INFO - Epoch 1, Batch 19: Training loss = 0.8200
2025-04-03 18:26:34,735 - INFO - Epoch 1, Batch 20: Training loss = 0.8100
2025-04-03 18:26:34,836 - INFO - Epoch 1, Batch 21: Training loss = 0.8000
2025-04-03 18:26:34,937 - INFO - Epoch 1, Batch 22: Training loss = 0.7900
2025-04-03 18:26:35,038 - INFO - Epoch 1, Batch 23: Training loss = 0.7800
2025-04-03 18:26:35,139 - INFO - Epoch 1, Batch 24: Training loss = 0.7700
2025-04-03 18:26:35,240 - INFO - Epoch 1, Batch 25: Training loss = 0.7600
2025-04-03 18:26:35,341 - INFO - Epoch 1, Batch 26: Training loss = 0.7500
2025-04-03 18:26:35,441 - INFO - Epoch 1, Batch 27: Training loss = 0.7400
2025-04-03 18:26:35,542 - INFO - Epoch 1, Batch 28: Training loss = 0.7300
2025-04-03 18:26:35,643 - INFO - Epoch 1, Batch 29: Training loss = 0.7200
2025-04-03 18:26:35,743 - INFO - Epoch 1, Batch 30: Training loss = 0.7100
2025-04-03 18:26:35,844 - INFO - Epoch 1, Batch 31: Training loss = 0.7000
2025-04-03 18:26:35,945 - INFO - Epoch 1, Batch 32: Training loss = 0.6900
2025-04-03 18:26:36,046 - INFO - Epoch 1, Batch 33: Training loss = 0.6800
2025-04-03 18:26:36,146 - INFO - Epoch 1, Batch 34: Training loss = 0.6700
2025-04-03 18:26:36,247 - INFO - Epoch 1, Batch 35: Training loss = 0.6600
2025-04-03 18:26:36,347 - INFO - Epoch 1, Batch 36: Training loss = 0.6500
2025-04-03 18:26:36,449 - INFO - Epoch 1, Batch 37: Training loss = 0.6400
2025-04-03 18:26:36,550 - INFO - Epoch 1, Batch 38: Training loss = 0.6300
2025-04-03 18:26:36,651 - INFO - Epoch 1, Batch 39: Training loss = 0.6200
2025-04-03 18:26:36,752 - INFO - Epoch 1, Batch 40: Training loss = 0.6100
2025-04-03 18:26:36,853 - INFO - Epoch 1, Batch 41: Training loss = 0.6000
2025-04-03 18:26:36,953 - INFO - Epoch 1, Batch 42: Training loss = 0.5900
2025-04-03 18:26:37,054 - INFO - Epoch 1, Batch 43: Training loss = 0.5800
2025-04-03 18:26:37,155 - INFO - Epoch 1, Batch 44: Training loss = 0.5700
2025-04-03 18:26:37,255 - INFO - Epoch 1, Batch 45: Training loss = 0.5600
2025-04-03 18:26:37,357 - INFO - Epoch 1, Batch 46: Training loss = 0.5500
2025-04-03 18:26:37,458 - INFO - Epoch 1, Batch 47: Training loss = 0.5400
2025-04-03 18:26:37,557 - INFO - Epoch 1, Batch 48: Training loss = 0.5300
2025-04-03 18:26:37,657 - INFO - Epoch 1, Batch 49: Training loss = 0.5200
2025-04-03 18:26:37,758 - INFO - Epoch 1, Batch 50: Training loss = 0.5100
2025-04-03 18:26:37,859 - INFO - Epoch 1, Batch 51: Training loss = 0.5000
2025-04-03 18:26:37,960 - INFO - Epoch 1, Batch 52: Training loss = 0.4900
2025-04-03 18:26:38,061 - INFO - Epoch 1, Batch 53: Training loss = 0.4800
2025-04-03 18:26:38,161 - INFO - Epoch 1, Batch 54: Training loss = 0.4700
2025-04-03 18:26:38,262 - INFO - Epoch 1, Batch 55: Training loss = 0.4600
2025-04-03 18:26:38,363 - INFO - Epoch 1, Batch 56: Training loss = 0.4500
2025-04-03 18:26:38,464 - INFO - Epoch 1, Batch 57: Training loss = 0.4400
2025-04-03 18:26:38,565 - INFO - Epoch 1, Batch 58: Training loss = 0.4300
2025-04-03 18:26:38,665 - INFO - Epoch 1, Batch 59: Training loss = 0.4200
2025-04-03 18:26:38,766 - INFO - Epoch 1, Batch 60: Training loss = 0.4100
2025-04-03 18:26:38,867 - INFO - Epoch 1, Batch 61: Training loss = 0.4000
2025-04-03 18:26:38,968 - INFO - Epoch 1, Batch 62: Training loss = 0.3900
2025-04-03 18:26:39,068 - INFO - Epoch 1, Batch 63: Training loss = 0.3800
2025-04-03 18:26:39,169 - INFO - Epoch 1, Batch 64: Training loss = 0.3700
2025-04-03 18:26:39,269 - INFO - Epoch 1, Batch 65: Training loss = 0.3600
2025-04-03 18:26:39,370 - INFO - Epoch 1, Batch 66: Training loss = 0.3500
2025-04-03 18:26:39,470 - INFO - Epoch 1, Batch 67: Training loss = 0.3400
2025-04-03 18:26:39,571 - INFO - Epoch 1, Batch 68: Training loss = 0.3300
2025-04-03 18:26:39,671 - INFO - Epoch 1, Batch 69: Training loss = 0.3200
2025-04-03 18:26:39,772 - INFO - Epoch 1, Batch 70: Training loss = 0.3100
2025-04-03 18:26:39,872 - INFO - Epoch 1, Batch 71: Training loss = 0.3000
2025-04-03 18:26:39,973 - INFO - Epoch 1, Batch 72: Training loss = 0.2900
2025-04-03 18:26:40,074 - INFO - Epoch 1, Batch 73: Training loss = 0.2800
2025-04-03 18:26:40,175 - INFO - Epoch 1, Batch 74: Training loss = 0.2700
2025-04-03 18:26:40,276 - INFO - Epoch 1, Batch 75: Training loss = 0.2600
2025-04-03 18:26:40,376 - INFO - Epoch 1, Batch 76: Training loss = 0.2500
2025-04-03 18:26:40,477 - INFO - Epoch 1, Batch 77: Training loss = 0.2400
2025-04-03 18:26:40,577 - INFO - Epoch 1, Batch 78: Training loss = 0.2300
2025-04-03 18:26:40,678 - INFO - Epoch 1, Batch 79: Training loss = 0.2200
2025-04-03 18:26:40,779 - INFO - Epoch 1, Batch 80: Training loss = 0.2100
2025-04-03 18:26:40,880 - INFO - Epoch 1, Batch 81: Training loss = 0.2000
2025-04-03 18:26:40,980 - INFO - Epoch 1, Batch 82: Training loss = 0.1900
2025-04-03 18:26:41,081 - INFO - Epoch 1, Batch 83: Training loss = 0.1800
2025-04-03 18:26:41,181 - INFO - Epoch 1, Batch 84: Training loss = 0.1700
2025-04-03 18:26:41,282 - INFO - Epoch 1, Batch 85: Training loss = 0.1600
2025-04-03 18:26:41,382 - INFO - Epoch 1, Batch 86: Training loss = 0.1500
2025-04-03 18:26:41,483 - INFO - Epoch 1, Batch 87: Training loss = 0.1400
2025-04-03 18:26:41,584 - INFO - Epoch 1, Batch 88: Training loss = 0.1300
2025-04-03 18:26:41,685 - INFO - Epoch 1, Batch 89: Training loss = 0.1200
2025-04-03 18:26:41,785 - INFO - Epoch 1, Batch 90: Training loss = 0.1100
2025-04-03 18:26:41,886 - INFO - Epoch 1, Batch 91: Training loss = 0.1000
2025-04-03 18:26:41,987 - INFO - Epoch 1, Batch 92: Training loss = 0.0900
2025-04-03 18:26:42,088 - INFO - Epoch 1, Batch 93: Training loss = 0.0800
2025-04-03 18:26:42,188 - INFO - Epoch 1, Batch 94: Training loss = 0.0700
2025-04-03 18:26:42,289 - INFO - Epoch 1, Batch 95: Training loss = 0.0600
2025-04-03 18:26:42,390 - INFO - Epoch 1, Batch 96: Training loss = 0.0500
2025-04-03 18:26:42,490 - INFO - Epoch 1, Batch 97: Training loss = 0.0400
2025-04-03 18:26:42,591 - INFO - Epoch 1, Batch 98: Training loss = 0.0300
2025-04-03 18:26:42,692 - INFO - Epoch 1, Batch 99: Training loss = 0.0200
2025-04-03 18:26:42,793 - INFO - Epoch 1, Batch 100: Training loss = 0.0100
2025-04-03 18:26:42,894 - INFO - Epoch 1: Validation loss = 0.1000, Validation accuracy = 0.8000
2025-04-03 18:26:42,894 - INFO - Epoch 1: MSE = 0.1000, RMSE = 0.3162, MAE = 0.0800, RÂ² = 0.9000
2025-04-03 18:26:42,894 - INFO - Epoch 2 started.
2025-04-03 18:26:42,895 - INFO - Epoch 2, Batch 1: Training loss = 1.0000
2025-04-03 18:26:42,995 - INFO - Epoch 2, Batch 2: Training loss = 0.9900
2025-04-03 18:26:43,096 - INFO - Epoch 2, Batch 3: Training loss = 0.9800
2025-04-03 18:26:43,197 - INFO - Epoch 2, Batch 4: Training loss = 0.9700
2025-04-03 18:26:43,298 - INFO - Epoch 2, Batch 5: Training loss = 0.9600
2025-04-03 18:26:43,398 - INFO - Epoch 2, Batch 6: Training loss = 0.9500
2025-04-03 18:26:43,500 - INFO - Epoch 2, Batch 7: Training loss = 0.9400
2025-04-03 18:26:43,601 - INFO - Epoch 2, Batch 8: Training loss = 0.9300
2025-04-03 18:26:43,701 - INFO - Epoch 2, Batch 9: Training loss = 0.9200
2025-04-03 18:26:43,803 - INFO - Epoch 2, Batch 10: Training loss = 0.9100
2025-04-03 18:26:43,903 - INFO - Epoch 2, Batch 11: Training loss = 0.9000
2025-04-03 18:26:44,004 - INFO - Epoch 2, Batch 12: Training loss = 0.8900
2025-04-03 18:26:44,105 - INFO - Epoch 2, Batch 13: Training loss = 0.8800
2025-04-03 18:26:44,206 - INFO - Epoch 2, Batch 14: Training loss = 0.8700
2025-04-03 18:26:44,306 - INFO - Epoch 2, Batch 15: Training loss = 0.8600
2025-04-03 18:26:44,407 - INFO - Epoch 2, Batch 16: Training loss = 0.8500
2025-04-03 18:26:44,508 - INFO - Epoch 2, Batch 17: Training loss = 0.8400
2025-04-03 18:26:44,609 - INFO - Epoch 2, Batch 18: Training loss = 0.8300
2025-04-03 18:26:44,710 - INFO - Epoch 2, Batch 19: Training loss = 0.8200
2025-04-03 18:26:44,811 - INFO - Epoch 2, Batch 20: Training loss = 0.8100
2025-04-03 18:26:44,911 - INFO - Epoch 2, Batch 21: Training loss = 0.8000
2025-04-03 18:26:45,012 - INFO - Epoch 2, Batch 22: Training loss = 0.7900
2025-04-03 18:26:45,113 - INFO - Epoch 2, Batch 23: Training loss = 0.7800
2025-04-03 18:26:45,214 - INFO - Epoch 2, Batch 24: Training loss = 0.7700
2025-04-03 18:26:45,314 - INFO - Epoch 2, Batch 25: Training loss = 0.7600
2025-04-03 18:26:45,415 - INFO - Epoch 2, Batch 26: Training loss = 0.7500
2025-04-03 18:26:45,516 - INFO - Epoch 2, Batch 27: Training loss = 0.7400
2025-04-03 18:26:45,616 - INFO - Epoch 2, Batch 28: Training loss = 0.7300
2025-04-03 18:26:45,717 - INFO - Epoch 2, Batch 29: Training loss = 0.7200
2025-04-03 18:26:45,817 - INFO - Epoch 2, Batch 30: Training loss = 0.7100
2025-04-03 18:26:45,918 - INFO - Epoch 2, Batch 31: Training loss = 0.7000
2025-04-03 18:26:46,018 - INFO - Epoch 2, Batch 32: Training loss = 0.6900
2025-04-03 18:26:46,119 - INFO - Epoch 2, Batch 33: Training loss = 0.6800
2025-04-03 18:26:46,219 - INFO - Epoch 2, Batch 34: Training loss = 0.6700
2025-04-03 18:26:46,320 - INFO - Epoch 2, Batch 35: Training loss = 0.6600
2025-04-03 18:26:46,422 - INFO - Epoch 2, Batch 36: Training loss = 0.6500
2025-04-03 18:26:46,523 - INFO - Epoch 2, Batch 37: Training loss = 0.6400
2025-04-03 18:26:46,627 - INFO - Epoch 2, Batch 38: Training loss = 0.6300
2025-04-03 18:26:46,728 - INFO - Epoch 2, Batch 39: Training loss = 0.6200
2025-04-03 18:26:46,828 - INFO - Epoch 2, Batch 40: Training loss = 0.6100
2025-04-03 18:26:46,929 - INFO - Epoch 2, Batch 41: Training loss = 0.6000
2025-04-03 18:26:47,029 - INFO - Epoch 2, Batch 42: Training loss = 0.5900
2025-04-03 18:26:47,130 - INFO - Epoch 2, Batch 43: Training loss = 0.5800
2025-04-03 18:26:47,230 - INFO - Epoch 2, Batch 44: Training loss = 0.5700
2025-04-03 18:26:47,331 - INFO - Epoch 2, Batch 45: Training loss = 0.5600
2025-04-03 18:26:47,431 - INFO - Epoch 2, Batch 46: Training loss = 0.5500
2025-04-03 18:26:47,532 - INFO - Epoch 2, Batch 47: Training loss = 0.5400
2025-04-03 18:26:47,632 - INFO - Epoch 2, Batch 48: Training loss = 0.5300
2025-04-03 18:26:47,733 - INFO - Epoch 2, Batch 49: Training loss = 0.5200
2025-04-03 18:26:47,833 - INFO - Epoch 2, Batch 50: Training loss = 0.5100
2025-04-03 18:26:47,934 - INFO - Epoch 2, Batch 51: Training loss = 0.5000
2025-04-03 18:26:48,035 - INFO - Epoch 2, Batch 52: Training loss = 0.4900
2025-04-03 18:26:48,135 - INFO - Epoch 2, Batch 53: Training loss = 0.4800
2025-04-03 18:26:48,236 - INFO - Epoch 2, Batch 54: Training loss = 0.4700
2025-04-03 18:26:48,336 - INFO - Epoch 2, Batch 55: Training loss = 0.4600
2025-04-03 18:26:48,436 - INFO - Epoch 2, Batch 56: Training loss = 0.4500
2025-04-03 18:26:48,537 - INFO - Epoch 2, Batch 57: Training loss = 0.4400
2025-04-03 18:26:48,638 - INFO - Epoch 2, Batch 58: Training loss = 0.4300
2025-04-03 18:26:48,738 - INFO - Epoch 2, Batch 59: Training loss = 0.4200
2025-04-03 18:26:48,839 - INFO - Epoch 2, Batch 60: Training loss = 0.4100
2025-04-03 18:26:48,939 - INFO - Epoch 2, Batch 61: Training loss = 0.4000
2025-04-03 18:26:49,040 - INFO - Epoch 2, Batch 62: Training loss = 0.3900
2025-04-03 18:26:49,140 - INFO - Epoch 2, Batch 63: Training loss = 0.3800
2025-04-03 18:26:49,241 - INFO - Epoch 2, Batch 64: Training loss = 0.3700
2025-04-03 18:26:49,342 - INFO - Epoch 2, Batch 65: Training loss = 0.3600
2025-04-03 18:26:49,442 - INFO - Epoch 2, Batch 66: Training loss = 0.3500
2025-04-03 18:26:49,543 - INFO - Epoch 2, Batch 67: Training loss = 0.3400
2025-04-03 18:26:49,643 - INFO - Epoch 2, Batch 68: Training loss = 0.3300
2025-04-03 18:26:49,744 - INFO - Epoch 2, Batch 69: Training loss = 0.3200
2025-04-03 18:26:49,845 - INFO - Epoch 2, Batch 70: Training loss = 0.3100
2025-04-03 18:26:49,945 - INFO - Epoch 2, Batch 71: Training loss = 0.3000
2025-04-03 18:26:50,046 - INFO - Epoch 2, Batch 72: Training loss = 0.2900
2025-04-03 18:26:50,146 - INFO - Epoch 2, Batch 73: Training loss = 0.2800
2025-04-03 18:26:50,247 - INFO - Epoch 2, Batch 74: Training loss = 0.2700
2025-04-03 18:26:50,347 - INFO - Epoch 2, Batch 75: Training loss = 0.2600
2025-04-03 18:26:50,448 - INFO - Epoch 2, Batch 76: Training loss = 0.2500
2025-04-03 18:26:50,549 - INFO - Epoch 2, Batch 77: Training loss = 0.2400
2025-04-03 18:26:50,649 - INFO - Epoch 2, Batch 78: Training loss = 0.2300
2025-04-03 18:26:50,750 - INFO - Epoch 2, Batch 79: Training loss = 0.2200
2025-04-03 18:26:50,851 - INFO - Epoch 2, Batch 80: Training loss = 0.2100
2025-04-03 18:26:50,952 - INFO - Epoch 2, Batch 81: Training loss = 0.2000
2025-04-03 18:26:51,052 - INFO - Epoch 2, Batch 82: Training loss = 0.1900
2025-04-03 18:26:51,153 - INFO - Epoch 2, Batch 83: Training loss = 0.1800
2025-04-03 18:26:51,254 - INFO - Epoch 2, Batch 84: Training loss = 0.1700
2025-04-03 18:26:51,354 - INFO - Epoch 2, Batch 85: Training loss = 0.1600
2025-04-03 18:26:51,454 - INFO - Epoch 2, Batch 86: Training loss = 0.1500
2025-04-03 18:26:51,555 - INFO - Epoch 2, Batch 87: Training loss = 0.1400
2025-04-03 18:26:51,655 - INFO - Epoch 2, Batch 88: Training loss = 0.1300
2025-04-03 18:26:51,756 - INFO - Epoch 2, Batch 89: Training loss = 0.1200
2025-04-03 18:26:51,856 - INFO - Epoch 2, Batch 90: Training loss = 0.1100
2025-04-03 18:26:51,957 - INFO - Epoch 2, Batch 91: Training loss = 0.1000
2025-04-03 18:26:52,057 - INFO - Epoch 2, Batch 92: Training loss = 0.0900
2025-04-03 18:26:52,158 - INFO - Epoch 2, Batch 93: Training loss = 0.0800
2025-04-03 18:26:52,258 - INFO - Epoch 2, Batch 94: Training loss = 0.0700
2025-04-03 18:26:52,359 - INFO - Epoch 2, Batch 95: Training loss = 0.0600
2025-04-03 18:26:52,459 - INFO - Epoch 2, Batch 96: Training loss = 0.0500
2025-04-03 18:26:52,560 - INFO - Epoch 2, Batch 97: Training loss = 0.0400
2025-04-03 18:26:52,661 - INFO - Epoch 2, Batch 98: Training loss = 0.0300
2025-04-03 18:26:52,761 - INFO - Epoch 2, Batch 99: Training loss = 0.0200
2025-04-03 18:26:52,862 - INFO - Epoch 2, Batch 100: Training loss = 0.0100
2025-04-03 18:26:52,962 - INFO - Epoch 2: Validation loss = 0.0900, Validation accuracy = 0.8200
2025-04-03 18:26:52,962 - INFO - Epoch 2: MSE = 0.0900, RMSE = 0.3000, MAE = 0.0720, RÂ² = 0.9100
2025-04-03 18:26:52,963 - INFO - Epoch 3 started.
2025-04-03 18:26:52,963 - INFO - Epoch 3, Batch 1: Training loss = 1.0000
2025-04-03 18:26:53,063 - INFO - Epoch 3, Batch 2: Training loss = 0.9900
2025-04-03 18:26:53,164 - INFO - Epoch 3, Batch 3: Training loss = 0.9800
2025-04-03 18:26:53,264 - INFO - Epoch 3, Batch 4: Training loss = 0.9700
2025-04-03 18:26:53,365 - INFO - Epoch 3, Batch 5: Training loss = 0.9600
2025-04-03 18:26:53,465 - INFO - Epoch 3, Batch 6: Training loss = 0.9500
2025-04-03 18:26:53,566 - INFO - Epoch 3, Batch 7: Training loss = 0.9400
2025-04-03 18:26:53,667 - INFO - Epoch 3, Batch 8: Training loss = 0.9300
2025-04-03 18:26:53,768 - INFO - Epoch 3, Batch 9: Training loss = 0.9200
2025-04-03 18:26:53,869 - INFO - Epoch 3, Batch 10: Training loss = 0.9100
2025-04-03 18:26:53,970 - INFO - Epoch 3, Batch 11: Training loss = 0.9000
2025-04-03 18:26:54,070 - INFO - Epoch 3, Batch 12: Training loss = 0.8900
2025-04-03 18:26:54,172 - INFO - Epoch 3, Batch 13: Training loss = 0.8800
2025-04-03 18:26:54,272 - INFO - Epoch 3, Batch 14: Training loss = 0.8700
2025-04-03 18:26:54,373 - INFO - Epoch 3, Batch 15: Training loss = 0.8600
2025-04-03 18:26:54,473 - INFO - Epoch 3, Batch 16: Training loss = 0.8500
2025-04-03 18:26:54,574 - INFO - Epoch 3, Batch 17: Training loss = 0.8400
2025-04-03 18:26:54,674 - INFO - Epoch 3, Batch 18: Training loss = 0.8300
2025-04-03 18:26:54,775 - INFO - Epoch 3, Batch 19: Training loss = 0.8200
2025-04-03 18:26:54,875 - INFO - Epoch 3, Batch 20: Training loss = 0.8100
2025-04-03 18:26:54,976 - INFO - Epoch 3, Batch 21: Training loss = 0.8000
2025-04-03 18:26:55,077 - INFO - Epoch 3, Batch 22: Training loss = 0.7900
2025-04-03 18:26:55,178 - INFO - Epoch 3, Batch 23: Training loss = 0.7800
2025-04-03 18:26:55,278 - INFO - Epoch 3, Batch 24: Training loss = 0.7700
2025-04-03 18:26:55,379 - INFO - Epoch 3, Batch 25: Training loss = 0.7600
2025-04-03 18:26:55,480 - INFO - Epoch 3, Batch 26: Training loss = 0.7500
2025-04-03 18:26:55,581 - INFO - Epoch 3, Batch 27: Training loss = 0.7400
2025-04-03 18:26:55,682 - INFO - Epoch 3, Batch 28: Training loss = 0.7300
2025-04-03 18:26:55,783 - INFO - Epoch 3, Batch 29: Training loss = 0.7200
2025-04-03 18:26:55,884 - INFO - Epoch 3, Batch 30: Training loss = 0.7100
2025-04-03 18:26:55,984 - INFO - Epoch 3, Batch 31: Training loss = 0.7000
2025-04-03 18:26:56,085 - INFO - Epoch 3, Batch 32: Training loss = 0.6900
2025-04-03 18:26:56,186 - INFO - Epoch 3, Batch 33: Training loss = 0.6800
2025-04-03 18:26:56,286 - INFO - Epoch 3, Batch 34: Training loss = 0.6700
2025-04-03 18:26:56,388 - INFO - Epoch 3, Batch 35: Training loss = 0.6600
2025-04-03 18:26:56,489 - INFO - Epoch 3, Batch 36: Training loss = 0.6500
2025-04-03 18:26:56,589 - INFO - Epoch 3, Batch 37: Training loss = 0.6400
2025-04-03 18:26:56,690 - INFO - Epoch 3, Batch 38: Training loss = 0.6300
2025-04-03 18:26:56,791 - INFO - Epoch 3, Batch 39: Training loss = 0.6200
2025-04-03 18:26:56,892 - INFO - Epoch 3, Batch 40: Training loss = 0.6100
2025-04-03 18:26:56,992 - INFO - Epoch 3, Batch 41: Training loss = 0.6000
2025-04-03 18:26:57,094 - INFO - Epoch 3, Batch 42: Training loss = 0.5900
2025-04-03 18:26:57,195 - INFO - Epoch 3, Batch 43: Training loss = 0.5800
2025-04-03 18:26:57,296 - INFO - Epoch 3, Batch 44: Training loss = 0.5700
2025-04-03 18:26:57,397 - INFO - Epoch 3, Batch 45: Training loss = 0.5600
2025-04-03 18:26:57,498 - INFO - Epoch 3, Batch 46: Training loss = 0.5500
2025-04-03 18:26:57,599 - INFO - Epoch 3, Batch 47: Training loss = 0.5400
2025-04-03 18:26:57,700 - INFO - Epoch 3, Batch 48: Training loss = 0.5300
2025-04-03 18:26:57,801 - INFO - Epoch 3, Batch 49: Training loss = 0.5200
2025-04-03 18:26:57,902 - INFO - Epoch 3, Batch 50: Training loss = 0.5100
2025-04-03 18:26:58,003 - INFO - Epoch 3, Batch 51: Training loss = 0.5000
2025-04-03 18:26:58,104 - INFO - Epoch 3, Batch 52: Training loss = 0.4900
2025-04-03 18:26:58,204 - INFO - Epoch 3, Batch 53: Training loss = 0.4800
2025-04-03 18:26:58,305 - INFO - Epoch 3, Batch 54: Training loss = 0.4700
2025-04-03 18:26:58,406 - INFO - Epoch 3, Batch 55: Training loss = 0.4600
2025-04-03 18:26:58,508 - INFO - Epoch 3, Batch 56: Training loss = 0.4500
2025-04-03 18:26:58,609 - INFO - Epoch 3, Batch 57: Training loss = 0.4400
2025-04-03 18:26:58,709 - INFO - Epoch 3, Batch 58: Training loss = 0.4300
2025-04-03 18:26:58,810 - INFO - Epoch 3, Batch 59: Training loss = 0.4200
2025-04-03 18:26:58,911 - INFO - Epoch 3, Batch 60: Training loss = 0.4100
2025-04-03 18:26:59,012 - INFO - Epoch 3, Batch 61: Training loss = 0.4000
2025-04-03 18:26:59,113 - INFO - Epoch 3, Batch 62: Training loss = 0.3900
2025-04-03 18:26:59,213 - INFO - Epoch 3, Batch 63: Training loss = 0.3800
2025-04-03 18:26:59,314 - INFO - Epoch 3, Batch 64: Training loss = 0.3700
2025-04-03 18:26:59,414 - INFO - Epoch 3, Batch 65: Training loss = 0.3600
2025-04-03 18:26:59,515 - INFO - Epoch 3, Batch 66: Training loss = 0.3500
2025-04-03 18:26:59,616 - INFO - Epoch 3, Batch 67: Training loss = 0.3400
2025-04-03 18:26:59,717 - INFO - Epoch 3, Batch 68: Training loss = 0.3300
2025-04-03 18:26:59,818 - INFO - Epoch 3, Batch 69: Training loss = 0.3200
2025-04-03 18:26:59,919 - INFO - Epoch 3, Batch 70: Training loss = 0.3100
2025-04-03 18:27:00,020 - INFO - Epoch 3, Batch 71: Training loss = 0.3000
2025-04-03 18:27:00,121 - INFO - Epoch 3, Batch 72: Training loss = 0.2900
2025-04-03 18:27:00,221 - INFO - Epoch 3, Batch 73: Training loss = 0.2800
2025-04-03 18:27:00,322 - INFO - Epoch 3, Batch 74: Training loss = 0.2700
2025-04-03 18:27:00,422 - INFO - Epoch 3, Batch 75: Training loss = 0.2600
2025-04-03 18:27:00,523 - INFO - Epoch 3, Batch 76: Training loss = 0.2500
2025-04-03 18:27:00,623 - INFO - Epoch 3, Batch 77: Training loss = 0.2400
2025-04-03 18:27:00,724 - INFO - Epoch 3, Batch 78: Training loss = 0.2300
2025-04-03 18:27:00,825 - INFO - Epoch 3, Batch 79: Training loss = 0.2200
2025-04-03 18:27:00,925 - INFO - Epoch 3, Batch 80: Training loss = 0.2100
2025-04-03 18:27:01,025 - INFO - Epoch 3, Batch 81: Training loss = 0.2000
2025-04-03 18:27:01,126 - INFO - Epoch 3, Batch 82: Training loss = 0.1900
2025-04-03 18:27:01,227 - INFO - Epoch 3, Batch 83: Training loss = 0.1800
2025-04-03 18:27:01,327 - INFO - Epoch 3, Batch 84: Training loss = 0.1700
2025-04-03 18:27:01,428 - INFO - Epoch 3, Batch 85: Training loss = 0.1600
2025-04-03 18:27:01,528 - INFO - Epoch 3, Batch 86: Training loss = 0.1500
2025-04-03 18:27:01,629 - INFO - Epoch 3, Batch 87: Training loss = 0.1400
2025-04-03 18:27:01,730 - INFO - Epoch 3, Batch 88: Training loss = 0.1300
2025-04-03 18:27:01,830 - INFO - Epoch 3, Batch 89: Training loss = 0.1200
2025-04-03 18:27:01,931 - INFO - Epoch 3, Batch 90: Training loss = 0.1100
2025-04-03 18:27:02,032 - INFO - Epoch 3, Batch 91: Training loss = 0.1000
2025-04-03 18:27:02,132 - INFO - Epoch 3, Batch 92: Training loss = 0.0900
2025-04-03 18:27:02,233 - INFO - Epoch 3, Batch 93: Training loss = 0.0800
2025-04-03 18:27:02,334 - INFO - Epoch 3, Batch 94: Training loss = 0.0700
2025-04-03 18:27:02,435 - INFO - Epoch 3, Batch 95: Training loss = 0.0600
2025-04-03 18:27:02,535 - INFO - Epoch 3, Batch 96: Training loss = 0.0500
2025-04-03 18:27:02,636 - INFO - Epoch 3, Batch 97: Training loss = 0.0400
2025-04-03 18:27:02,737 - INFO - Epoch 3, Batch 98: Training loss = 0.0300
2025-04-03 18:27:02,837 - INFO - Epoch 3, Batch 99: Training loss = 0.0200
2025-04-03 18:27:02,938 - INFO - Epoch 3, Batch 100: Training loss = 0.0100
2025-04-03 18:27:03,038 - INFO - Epoch 3: Validation loss = 0.0800, Validation accuracy = 0.8400
2025-04-03 18:27:03,038 - INFO - Epoch 3: MSE = 0.0800, RMSE = 0.2828, MAE = 0.0640, RÂ² = 0.9200
2025-04-03 18:27:03,038 - INFO - Epoch 4 started.
2025-04-03 18:27:03,039 - INFO - Epoch 4, Batch 1: Training loss = 1.0000
2025-04-03 18:27:03,139 - INFO - Epoch 4, Batch 2: Training loss = 0.9900
2025-04-03 18:27:03,239 - INFO - Epoch 4, Batch 3: Training loss = 0.9800
2025-04-03 18:27:03,340 - INFO - Epoch 4, Batch 4: Training loss = 0.9700
2025-04-03 18:27:03,440 - INFO - Epoch 4, Batch 5: Training loss = 0.9600
2025-04-03 18:27:03,541 - INFO - Epoch 4, Batch 6: Training loss = 0.9500
2025-04-03 18:27:03,641 - INFO - Epoch 4, Batch 7: Training loss = 0.9400
2025-04-03 18:27:03,742 - INFO - Epoch 4, Batch 8: Training loss = 0.9300
2025-04-03 18:27:03,843 - INFO - Epoch 4, Batch 9: Training loss = 0.9200
2025-04-03 18:27:03,943 - INFO - Epoch 4, Batch 10: Training loss = 0.9100
2025-04-03 18:27:04,044 - INFO - Epoch 4, Batch 11: Training loss = 0.9000
2025-04-03 18:27:04,145 - INFO - Epoch 4, Batch 12: Training loss = 0.8900
2025-04-03 18:27:04,246 - INFO - Epoch 4, Batch 13: Training loss = 0.8800
2025-04-03 18:27:04,347 - INFO - Epoch 4, Batch 14: Training loss = 0.8700
2025-04-03 18:27:04,447 - INFO - Epoch 4, Batch 15: Training loss = 0.8600
2025-04-03 18:27:04,548 - INFO - Epoch 4, Batch 16: Training loss = 0.8500
2025-04-03 18:27:04,649 - INFO - Epoch 4, Batch 17: Training loss = 0.8400
2025-04-03 18:27:04,750 - INFO - Epoch 4, Batch 18: Training loss = 0.8300
2025-04-03 18:27:04,850 - INFO - Epoch 4, Batch 19: Training loss = 0.8200
2025-04-03 18:27:04,951 - INFO - Epoch 4, Batch 20: Training loss = 0.8100
2025-04-03 18:27:05,052 - INFO - Epoch 4, Batch 21: Training loss = 0.8000
2025-04-03 18:27:05,152 - INFO - Epoch 4, Batch 22: Training loss = 0.7900
2025-04-03 18:27:05,253 - INFO - Epoch 4, Batch 23: Training loss = 0.7800
2025-04-03 18:27:05,353 - INFO - Epoch 4, Batch 24: Training loss = 0.7700
2025-04-03 18:27:05,454 - INFO - Epoch 4, Batch 25: Training loss = 0.7600
2025-04-03 18:27:05,554 - INFO - Epoch 4, Batch 26: Training loss = 0.7500
2025-04-03 18:27:05,654 - INFO - Epoch 4, Batch 27: Training loss = 0.7400
2025-04-03 18:27:05,755 - INFO - Epoch 4, Batch 28: Training loss = 0.7300
2025-04-03 18:27:05,856 - INFO - Epoch 4, Batch 29: Training loss = 0.7200
2025-04-03 18:27:05,956 - INFO - Epoch 4, Batch 30: Training loss = 0.7100
2025-04-03 18:27:06,057 - INFO - Epoch 4, Batch 31: Training loss = 0.7000
2025-04-03 18:27:06,157 - INFO - Epoch 4, Batch 32: Training loss = 0.6900
2025-04-03 18:27:06,258 - INFO - Epoch 4, Batch 33: Training loss = 0.6800
2025-04-03 18:27:06,359 - INFO - Epoch 4, Batch 34: Training loss = 0.6700
2025-04-03 18:27:06,460 - INFO - Epoch 4, Batch 35: Training loss = 0.6600
2025-04-03 18:27:06,561 - INFO - Epoch 4, Batch 36: Training loss = 0.6500
2025-04-03 18:27:06,661 - INFO - Epoch 4, Batch 37: Training loss = 0.6400
2025-04-03 18:27:06,762 - INFO - Epoch 4, Batch 38: Training loss = 0.6300
2025-04-03 18:27:06,862 - INFO - Epoch 4, Batch 39: Training loss = 0.6200
2025-04-03 18:27:06,963 - INFO - Epoch 4, Batch 40: Training loss = 0.6100
2025-04-03 18:27:07,063 - INFO - Epoch 4, Batch 41: Training loss = 0.6000
2025-04-03 18:27:07,164 - INFO - Epoch 4, Batch 42: Training loss = 0.5900
2025-04-03 18:27:07,265 - INFO - Epoch 4, Batch 43: Training loss = 0.5800
2025-04-03 18:27:07,365 - INFO - Epoch 4, Batch 44: Training loss = 0.5700
2025-04-03 18:27:07,466 - INFO - Epoch 4, Batch 45: Training loss = 0.5600
2025-04-03 18:27:07,565 - INFO - Epoch 4, Batch 46: Training loss = 0.5500
2025-04-03 18:27:07,665 - INFO - Epoch 4, Batch 47: Training loss = 0.5400
2025-04-03 18:27:07,766 - INFO - Epoch 4, Batch 48: Training loss = 0.5300
2025-04-03 18:27:07,868 - INFO - Epoch 4, Batch 49: Training loss = 0.5200
2025-04-03 18:27:07,968 - INFO - Epoch 4, Batch 50: Training loss = 0.5100
2025-04-03 18:27:08,069 - INFO - Epoch 4, Batch 51: Training loss = 0.5000
2025-04-03 18:27:08,169 - INFO - Epoch 4, Batch 52: Training loss = 0.4900
2025-04-03 18:27:08,269 - INFO - Epoch 4, Batch 53: Training loss = 0.4800
2025-04-03 18:27:08,370 - INFO - Epoch 4, Batch 54: Training loss = 0.4700
2025-04-03 18:27:08,470 - INFO - Epoch 4, Batch 55: Training loss = 0.4600
2025-04-03 18:27:08,571 - INFO - Epoch 4, Batch 56: Training loss = 0.4500
2025-04-03 18:27:08,672 - INFO - Epoch 4, Batch 57: Training loss = 0.4400
2025-04-03 18:27:08,772 - INFO - Epoch 4, Batch 58: Training loss = 0.4300
2025-04-03 18:27:08,873 - INFO - Epoch 4, Batch 59: Training loss = 0.4200
2025-04-03 18:27:08,973 - INFO - Epoch 4, Batch 60: Training loss = 0.4100
2025-04-03 18:27:09,074 - INFO - Epoch 4, Batch 61: Training loss = 0.4000
2025-04-03 18:27:09,174 - INFO - Epoch 4, Batch 62: Training loss = 0.3900
2025-04-03 18:27:09,275 - INFO - Epoch 4, Batch 63: Training loss = 0.3800
2025-04-03 18:27:09,376 - INFO - Epoch 4, Batch 64: Training loss = 0.3700
2025-04-03 18:27:09,476 - INFO - Epoch 4, Batch 65: Training loss = 0.3600
2025-04-03 18:27:09,577 - INFO - Epoch 4, Batch 66: Training loss = 0.3500
2025-04-03 18:27:09,678 - INFO - Epoch 4, Batch 67: Training loss = 0.3400
2025-04-03 18:27:09,779 - INFO - Epoch 4, Batch 68: Training loss = 0.3300
2025-04-03 18:27:09,879 - INFO - Epoch 4, Batch 69: Training loss = 0.3200
2025-04-03 18:27:09,980 - INFO - Epoch 4, Batch 70: Training loss = 0.3100
2025-04-03 18:27:10,081 - INFO - Epoch 4, Batch 71: Training loss = 0.3000
2025-04-03 18:27:10,182 - INFO - Epoch 4, Batch 72: Training loss = 0.2900
2025-04-03 18:27:10,282 - INFO - Epoch 4, Batch 73: Training loss = 0.2800
2025-04-03 18:27:10,383 - INFO - Epoch 4, Batch 74: Training loss = 0.2700
2025-04-03 18:27:10,483 - INFO - Epoch 4, Batch 75: Training loss = 0.2600
2025-04-03 18:27:10,584 - INFO - Epoch 4, Batch 76: Training loss = 0.2500
2025-04-03 18:27:10,684 - INFO - Epoch 4, Batch 77: Training loss = 0.2400
2025-04-03 18:27:10,785 - INFO - Epoch 4, Batch 78: Training loss = 0.2300
2025-04-03 18:27:10,886 - INFO - Epoch 4, Batch 79: Training loss = 0.2200
2025-04-03 18:27:10,986 - INFO - Epoch 4, Batch 80: Training loss = 0.2100
2025-04-03 18:27:11,086 - INFO - Epoch 4, Batch 81: Training loss = 0.2000
2025-04-03 18:27:11,187 - INFO - Epoch 4, Batch 82: Training loss = 0.1900
2025-04-03 18:27:11,288 - INFO - Epoch 4, Batch 83: Training loss = 0.1800
2025-04-03 18:27:11,388 - INFO - Epoch 4, Batch 84: Training loss = 0.1700
2025-04-03 18:27:11,489 - INFO - Epoch 4, Batch 85: Training loss = 0.1600
2025-04-03 18:27:11,589 - INFO - Epoch 4, Batch 86: Training loss = 0.1500
2025-04-03 18:27:11,690 - INFO - Epoch 4, Batch 87: Training loss = 0.1400
2025-04-03 18:27:11,791 - INFO - Epoch 4, Batch 88: Training loss = 0.1300
2025-04-03 18:27:11,891 - INFO - Epoch 4, Batch 89: Training loss = 0.1200
2025-04-03 18:27:11,992 - INFO - Epoch 4, Batch 90: Training loss = 0.1100
2025-04-03 18:27:12,092 - INFO - Epoch 4, Batch 91: Training loss = 0.1000
2025-04-03 18:27:12,193 - INFO - Epoch 4, Batch 92: Training loss = 0.0900
2025-04-03 18:27:12,294 - INFO - Epoch 4, Batch 93: Training loss = 0.0800
2025-04-03 18:27:12,394 - INFO - Epoch 4, Batch 94: Training loss = 0.0700
2025-04-03 18:27:12,495 - INFO - Epoch 4, Batch 95: Training loss = 0.0600
2025-04-03 18:27:12,596 - INFO - Epoch 4, Batch 96: Training loss = 0.0500
2025-04-03 18:27:12,697 - INFO - Epoch 4, Batch 97: Training loss = 0.0400
2025-04-03 18:27:12,799 - INFO - Epoch 4, Batch 98: Training loss = 0.0300
2025-04-03 18:27:12,900 - INFO - Epoch 4, Batch 99: Training loss = 0.0200
2025-04-03 18:27:13,000 - INFO - Epoch 4, Batch 100: Training loss = 0.0100
2025-04-03 18:27:13,101 - INFO - Epoch 4: Validation loss = 0.0700, Validation accuracy = 0.8600
2025-04-03 18:27:13,102 - INFO - Epoch 4: MSE = 0.0700, RMSE = 0.2646, MAE = 0.0560, RÂ² = 0.9300
2025-04-03 18:27:13,102 - INFO - Epoch 5 started.
2025-04-03 18:27:13,103 - INFO - Epoch 5, Batch 1: Training loss = 1.0000
2025-04-03 18:27:13,203 - INFO - Epoch 5, Batch 2: Training loss = 0.9900
2025-04-03 18:27:13,304 - INFO - Epoch 5, Batch 3: Training loss = 0.9800
2025-04-03 18:27:13,405 - INFO - Epoch 5, Batch 4: Training loss = 0.9700
2025-04-03 18:27:13,506 - INFO - Epoch 5, Batch 5: Training loss = 0.9600
2025-04-03 18:27:13,607 - INFO - Epoch 5, Batch 6: Training loss = 0.9500
2025-04-03 18:27:13,708 - INFO - Epoch 5, Batch 7: Training loss = 0.9400
2025-04-03 18:27:13,809 - INFO - Epoch 5, Batch 8: Training loss = 0.9300
2025-04-03 18:27:13,910 - INFO - Epoch 5, Batch 9: Training loss = 0.9200
2025-04-03 18:27:14,011 - INFO - Epoch 5, Batch 10: Training loss = 0.9100
2025-04-03 18:27:14,111 - INFO - Epoch 5, Batch 11: Training loss = 0.9000
2025-04-03 18:27:14,212 - INFO - Epoch 5, Batch 12: Training loss = 0.8900
2025-04-03 18:27:14,313 - INFO - Epoch 5, Batch 13: Training loss = 0.8800
2025-04-03 18:27:14,414 - INFO - Epoch 5, Batch 14: Training loss = 0.8700
2025-04-03 18:27:14,515 - INFO - Epoch 5, Batch 15: Training loss = 0.8600
2025-04-03 18:27:14,616 - INFO - Epoch 5, Batch 16: Training loss = 0.8500
2025-04-03 18:27:14,717 - INFO - Epoch 5, Batch 17: Training loss = 0.8400
2025-04-03 18:27:14,818 - INFO - Epoch 5, Batch 18: Training loss = 0.8300
2025-04-03 18:27:14,919 - INFO - Epoch 5, Batch 19: Training loss = 0.8200
2025-04-03 18:27:15,019 - INFO - Epoch 5, Batch 20: Training loss = 0.8100
2025-04-03 18:27:15,120 - INFO - Epoch 5, Batch 21: Training loss = 0.8000
2025-04-03 18:27:15,220 - INFO - Epoch 5, Batch 22: Training loss = 0.7900
2025-04-03 18:27:15,321 - INFO - Epoch 5, Batch 23: Training loss = 0.7800
2025-04-03 18:27:15,422 - INFO - Epoch 5, Batch 24: Training loss = 0.7700
2025-04-03 18:27:15,523 - INFO - Epoch 5, Batch 25: Training loss = 0.7600
2025-04-03 18:27:15,624 - INFO - Epoch 5, Batch 26: Training loss = 0.7500
2025-04-03 18:27:15,725 - INFO - Epoch 5, Batch 27: Training loss = 0.7400
2025-04-03 18:27:15,826 - INFO - Epoch 5, Batch 28: Training loss = 0.7300
2025-04-03 18:27:15,927 - INFO - Epoch 5, Batch 29: Training loss = 0.7200
2025-04-03 18:27:16,027 - INFO - Epoch 5, Batch 30: Training loss = 0.7100
2025-04-03 18:27:16,128 - INFO - Epoch 5, Batch 31: Training loss = 0.7000
2025-04-03 18:27:16,229 - INFO - Epoch 5, Batch 32: Training loss = 0.6900
2025-04-03 18:27:16,329 - INFO - Epoch 5, Batch 33: Training loss = 0.6800
2025-04-03 18:27:16,430 - INFO - Epoch 5, Batch 34: Training loss = 0.6700
2025-04-03 18:27:16,531 - INFO - Epoch 5, Batch 35: Training loss = 0.6600
2025-04-03 18:27:16,631 - INFO - Epoch 5, Batch 36: Training loss = 0.6500
2025-04-03 18:27:16,733 - INFO - Epoch 5, Batch 37: Training loss = 0.6400
2025-04-03 18:27:16,834 - INFO - Epoch 5, Batch 38: Training loss = 0.6300
2025-04-03 18:27:16,935 - INFO - Epoch 5, Batch 39: Training loss = 0.6200
2025-04-03 18:27:17,036 - INFO - Epoch 5, Batch 40: Training loss = 0.6100
2025-04-03 18:27:17,136 - INFO - Epoch 5, Batch 41: Training loss = 0.6000
2025-04-03 18:27:17,237 - INFO - Epoch 5, Batch 42: Training loss = 0.5900
2025-04-03 18:27:17,337 - INFO - Epoch 5, Batch 43: Training loss = 0.5800
2025-04-03 18:27:17,438 - INFO - Epoch 5, Batch 44: Training loss = 0.5700
2025-04-03 18:27:17,538 - INFO - Epoch 5, Batch 45: Training loss = 0.5600
2025-04-03 18:27:17,639 - INFO - Epoch 5, Batch 46: Training loss = 0.5500
2025-04-03 18:27:17,739 - INFO - Epoch 5, Batch 47: Training loss = 0.5400
2025-04-03 18:27:17,840 - INFO - Epoch 5, Batch 48: Training loss = 0.5300
2025-04-03 18:27:17,940 - INFO - Epoch 5, Batch 49: Training loss = 0.5200
2025-04-03 18:27:18,041 - INFO - Epoch 5, Batch 50: Training loss = 0.5100
2025-04-03 18:27:18,141 - INFO - Epoch 5, Batch 51: Training loss = 0.5000
2025-04-03 18:27:18,242 - INFO - Epoch 5, Batch 52: Training loss = 0.4900
2025-04-03 18:27:18,342 - INFO - Epoch 5, Batch 53: Training loss = 0.4800
2025-04-03 18:27:18,443 - INFO - Epoch 5, Batch 54: Training loss = 0.4700
2025-04-03 18:27:18,543 - INFO - Epoch 5, Batch 55: Training loss = 0.4600
2025-04-03 18:27:18,644 - INFO - Epoch 5, Batch 56: Training loss = 0.4500
2025-04-03 18:27:18,745 - INFO - Epoch 5, Batch 57: Training loss = 0.4400
2025-04-03 18:27:18,846 - INFO - Epoch 5, Batch 58: Training loss = 0.4300
2025-04-03 18:27:18,947 - INFO - Epoch 5, Batch 59: Training loss = 0.4200
2025-04-03 18:27:19,048 - INFO - Epoch 5, Batch 60: Training loss = 0.4100
2025-04-03 18:27:19,149 - INFO - Epoch 5, Batch 61: Training loss = 0.4000
2025-04-03 18:27:19,249 - INFO - Epoch 5, Batch 62: Training loss = 0.3900
2025-04-03 18:27:19,350 - INFO - Epoch 5, Batch 63: Training loss = 0.3800
2025-04-03 18:27:19,450 - INFO - Epoch 5, Batch 64: Training loss = 0.3700
2025-04-03 18:27:19,551 - INFO - Epoch 5, Batch 65: Training loss = 0.3600
2025-04-03 18:27:19,652 - INFO - Epoch 5, Batch 66: Training loss = 0.3500
2025-04-03 18:27:19,753 - INFO - Epoch 5, Batch 67: Training loss = 0.3400
2025-04-03 18:27:19,854 - INFO - Epoch 5, Batch 68: Training loss = 0.3300
2025-04-03 18:27:19,955 - INFO - Epoch 5, Batch 69: Training loss = 0.3200
2025-04-03 18:27:20,055 - INFO - Epoch 5, Batch 70: Training loss = 0.3100
2025-04-03 18:27:20,156 - INFO - Epoch 5, Batch 71: Training loss = 0.3000
2025-04-03 18:27:20,258 - INFO - Epoch 5, Batch 72: Training loss = 0.2900
2025-04-03 18:27:20,359 - INFO - Epoch 5, Batch 73: Training loss = 0.2800
2025-04-03 18:27:20,459 - INFO - Epoch 5, Batch 74: Training loss = 0.2700
2025-04-03 18:27:20,560 - INFO - Epoch 5, Batch 75: Training loss = 0.2600
2025-04-03 18:27:20,661 - INFO - Epoch 5, Batch 76: Training loss = 0.2500
2025-04-03 18:27:20,761 - INFO - Epoch 5, Batch 77: Training loss = 0.2400
2025-04-03 18:27:20,862 - INFO - Epoch 5, Batch 78: Training loss = 0.2300
2025-04-03 18:27:20,963 - INFO - Epoch 5, Batch 79: Training loss = 0.2200
2025-04-03 18:27:21,064 - INFO - Epoch 5, Batch 80: Training loss = 0.2100
2025-04-03 18:27:21,164 - INFO - Epoch 5, Batch 81: Training loss = 0.2000
2025-04-03 18:27:21,265 - INFO - Epoch 5, Batch 82: Training loss = 0.1900
2025-04-03 18:27:21,365 - INFO - Epoch 5, Batch 83: Training loss = 0.1800
2025-04-03 18:27:21,466 - INFO - Epoch 5, Batch 84: Training loss = 0.1700
2025-04-03 18:27:21,566 - INFO - Epoch 5, Batch 85: Training loss = 0.1600
2025-04-03 18:27:21,667 - INFO - Epoch 5, Batch 86: Training loss = 0.1500
2025-04-03 18:27:21,768 - INFO - Epoch 5, Batch 87: Training loss = 0.1400
2025-04-03 18:27:21,868 - INFO - Epoch 5, Batch 88: Training loss = 0.1300
2025-04-03 18:27:21,969 - INFO - Epoch 5, Batch 89: Training loss = 0.1200
2025-04-03 18:27:22,070 - INFO - Epoch 5, Batch 90: Training loss = 0.1100
2025-04-03 18:27:22,171 - INFO - Epoch 5, Batch 91: Training loss = 0.1000
2025-04-03 18:27:22,271 - INFO - Epoch 5, Batch 92: Training loss = 0.0900
2025-04-03 18:27:22,372 - INFO - Epoch 5, Batch 93: Training loss = 0.0800
2025-04-03 18:27:22,473 - INFO - Epoch 5, Batch 94: Training loss = 0.0700
2025-04-03 18:27:22,574 - INFO - Epoch 5, Batch 95: Training loss = 0.0600
2025-04-03 18:27:22,675 - INFO - Epoch 5, Batch 96: Training loss = 0.0500
2025-04-03 18:27:22,776 - INFO - Epoch 5, Batch 97: Training loss = 0.0400
2025-04-03 18:27:22,876 - INFO - Epoch 5, Batch 98: Training loss = 0.0300
2025-04-03 18:27:22,977 - INFO - Epoch 5, Batch 99: Training loss = 0.0200
2025-04-03 18:27:23,078 - INFO - Epoch 5, Batch 100: Training loss = 0.0100
2025-04-03 18:27:23,179 - INFO - Epoch 5: Validation loss = 0.0600, Validation accuracy = 0.8800
2025-04-03 18:27:23,179 - INFO - Epoch 5: MSE = 0.0600, RMSE = 0.2449, MAE = 0.0480, RÂ² = 0.9400
2025-04-03 18:27:23,180 - INFO - Epoch 6 started.
2025-04-03 18:27:23,180 - INFO - Epoch 6, Batch 1: Training loss = 1.0000
2025-04-03 18:27:23,280 - INFO - Epoch 6, Batch 2: Training loss = 0.9900
2025-04-03 18:27:23,381 - INFO - Epoch 6, Batch 3: Training loss = 0.9800
2025-04-03 18:27:23,482 - INFO - Epoch 6, Batch 4: Training loss = 0.9700
2025-04-03 18:27:23,583 - INFO - Epoch 6, Batch 5: Training loss = 0.9600
2025-04-03 18:27:23,684 - INFO - Epoch 6, Batch 6: Training loss = 0.9500
2025-04-03 18:27:23,785 - INFO - Epoch 6, Batch 7: Training loss = 0.9400
2025-04-03 18:27:23,886 - INFO - Epoch 6, Batch 8: Training loss = 0.9300
2025-04-03 18:27:23,987 - INFO - Epoch 6, Batch 9: Training loss = 0.9200
2025-04-03 18:27:24,088 - INFO - Epoch 6, Batch 10: Training loss = 0.9100
2025-04-03 18:27:24,189 - INFO - Epoch 6, Batch 11: Training loss = 0.9000
2025-04-03 18:27:24,290 - INFO - Epoch 6, Batch 12: Training loss = 0.8900
2025-04-03 18:27:24,390 - INFO - Epoch 6, Batch 13: Training loss = 0.8800
2025-04-03 18:27:24,491 - INFO - Epoch 6, Batch 14: Training loss = 0.8700
2025-04-03 18:27:24,592 - INFO - Epoch 6, Batch 15: Training loss = 0.8600
2025-04-03 18:27:24,693 - INFO - Epoch 6, Batch 16: Training loss = 0.8500
2025-04-03 18:27:24,794 - INFO - Epoch 6, Batch 17: Training loss = 0.8400
2025-04-03 18:27:24,894 - INFO - Epoch 6, Batch 18: Training loss = 0.8300
2025-04-03 18:27:24,995 - INFO - Epoch 6, Batch 19: Training loss = 0.8200
2025-04-03 18:27:25,095 - INFO - Epoch 6, Batch 20: Training loss = 0.8100
2025-04-03 18:27:25,196 - INFO - Epoch 6, Batch 21: Training loss = 0.8000
2025-04-03 18:27:25,297 - INFO - Epoch 6, Batch 22: Training loss = 0.7900
2025-04-03 18:27:25,398 - INFO - Epoch 6, Batch 23: Training loss = 0.7800
2025-04-03 18:27:25,499 - INFO - Epoch 6, Batch 24: Training loss = 0.7700
2025-04-03 18:27:25,600 - INFO - Epoch 6, Batch 25: Training loss = 0.7600
2025-04-03 18:27:25,701 - INFO - Epoch 6, Batch 26: Training loss = 0.7500
2025-04-03 18:27:25,802 - INFO - Epoch 6, Batch 27: Training loss = 0.7400
2025-04-03 18:27:25,903 - INFO - Epoch 6, Batch 28: Training loss = 0.7300
2025-04-03 18:27:26,004 - INFO - Epoch 6, Batch 29: Training loss = 0.7200
2025-04-03 18:27:26,104 - INFO - Epoch 6, Batch 30: Training loss = 0.7100
2025-04-03 18:27:26,205 - INFO - Epoch 6, Batch 31: Training loss = 0.7000
2025-04-03 18:27:26,305 - INFO - Epoch 6, Batch 32: Training loss = 0.6900
2025-04-03 18:27:26,406 - INFO - Epoch 6, Batch 33: Training loss = 0.6800
2025-04-03 18:27:26,507 - INFO - Epoch 6, Batch 34: Training loss = 0.6700
2025-04-03 18:27:26,608 - INFO - Epoch 6, Batch 35: Training loss = 0.6600
2025-04-03 18:27:26,709 - INFO - Epoch 6, Batch 36: Training loss = 0.6500
2025-04-03 18:27:26,809 - INFO - Epoch 6, Batch 37: Training loss = 0.6400
2025-04-03 18:27:26,910 - INFO - Epoch 6, Batch 38: Training loss = 0.6300
2025-04-03 18:27:27,011 - INFO - Epoch 6, Batch 39: Training loss = 0.6200
2025-04-03 18:27:27,112 - INFO - Epoch 6, Batch 40: Training loss = 0.6100
2025-04-03 18:27:27,213 - INFO - Epoch 6, Batch 41: Training loss = 0.6000
2025-04-03 18:27:27,313 - INFO - Epoch 6, Batch 42: Training loss = 0.5900
2025-04-03 18:27:27,414 - INFO - Epoch 6, Batch 43: Training loss = 0.5800
2025-04-03 18:27:27,515 - INFO - Epoch 6, Batch 44: Training loss = 0.5700
2025-04-03 18:27:27,616 - INFO - Epoch 6, Batch 45: Training loss = 0.5600
2025-04-03 18:27:27,717 - INFO - Epoch 6, Batch 46: Training loss = 0.5500
2025-04-03 18:27:27,818 - INFO - Epoch 6, Batch 47: Training loss = 0.5400
2025-04-03 18:27:27,918 - INFO - Epoch 6, Batch 48: Training loss = 0.5300
2025-04-03 18:27:28,019 - INFO - Epoch 6, Batch 49: Training loss = 0.5200
2025-04-03 18:27:28,120 - INFO - Epoch 6, Batch 50: Training loss = 0.5100
2025-04-03 18:27:28,220 - INFO - Epoch 6, Batch 51: Training loss = 0.5000
2025-04-03 18:27:28,321 - INFO - Epoch 6, Batch 52: Training loss = 0.4900
2025-04-03 18:27:28,422 - INFO - Epoch 6, Batch 53: Training loss = 0.4800
2025-04-03 18:27:28,523 - INFO - Epoch 6, Batch 54: Training loss = 0.4700
2025-04-03 18:27:28,624 - INFO - Epoch 6, Batch 55: Training loss = 0.4600
2025-04-03 18:27:28,725 - INFO - Epoch 6, Batch 56: Training loss = 0.4500
2025-04-03 18:27:28,826 - INFO - Epoch 6, Batch 57: Training loss = 0.4400
2025-04-03 18:27:28,926 - INFO - Epoch 6, Batch 58: Training loss = 0.4300
2025-04-03 18:27:29,027 - INFO - Epoch 6, Batch 59: Training loss = 0.4200
2025-04-03 18:27:29,128 - INFO - Epoch 6, Batch 60: Training loss = 0.4100
2025-04-03 18:27:29,229 - INFO - Epoch 6, Batch 61: Training loss = 0.4000
2025-04-03 18:27:29,329 - INFO - Epoch 6, Batch 62: Training loss = 0.3900
2025-04-03 18:27:29,429 - INFO - Epoch 6, Batch 63: Training loss = 0.3800
2025-04-03 18:27:29,530 - INFO - Epoch 6, Batch 64: Training loss = 0.3700
2025-04-03 18:27:29,631 - INFO - Epoch 6, Batch 65: Training loss = 0.3600
2025-04-03 18:27:29,732 - INFO - Epoch 6, Batch 66: Training loss = 0.3500
2025-04-03 18:27:29,832 - INFO - Epoch 6, Batch 67: Training loss = 0.3400
2025-04-03 18:27:29,933 - INFO - Epoch 6, Batch 68: Training loss = 0.3300
2025-04-03 18:27:30,033 - INFO - Epoch 6, Batch 69: Training loss = 0.3200
2025-04-03 18:27:30,134 - INFO - Epoch 6, Batch 70: Training loss = 0.3100
2025-04-03 18:27:30,235 - INFO - Epoch 6, Batch 71: Training loss = 0.3000
2025-04-03 18:27:30,336 - INFO - Epoch 6, Batch 72: Training loss = 0.2900
2025-04-03 18:27:30,436 - INFO - Epoch 6, Batch 73: Training loss = 0.2800
2025-04-03 18:27:30,537 - INFO - Epoch 6, Batch 74: Training loss = 0.2700
2025-04-03 18:27:30,638 - INFO - Epoch 6, Batch 75: Training loss = 0.2600
2025-04-03 18:27:30,739 - INFO - Epoch 6, Batch 76: Training loss = 0.2500
2025-04-03 18:27:30,839 - INFO - Epoch 6, Batch 77: Training loss = 0.2400
2025-04-03 18:27:30,940 - INFO - Epoch 6, Batch 78: Training loss = 0.2300
2025-04-03 18:27:31,040 - INFO - Epoch 6, Batch 79: Training loss = 0.2200
2025-04-03 18:27:31,142 - INFO - Epoch 6, Batch 80: Training loss = 0.2100
2025-04-03 18:27:31,243 - INFO - Epoch 6, Batch 81: Training loss = 0.2000
2025-04-03 18:27:31,344 - INFO - Epoch 6, Batch 82: Training loss = 0.1900
2025-04-03 18:27:31,445 - INFO - Epoch 6, Batch 83: Training loss = 0.1800
2025-04-03 18:27:31,545 - INFO - Epoch 6, Batch 84: Training loss = 0.1700
2025-04-03 18:27:31,646 - INFO - Epoch 6, Batch 85: Training loss = 0.1600
2025-04-03 18:27:31,747 - INFO - Epoch 6, Batch 86: Training loss = 0.1500
2025-04-03 18:27:31,847 - INFO - Epoch 6, Batch 87: Training loss = 0.1400
2025-04-03 18:27:31,948 - INFO - Epoch 6, Batch 88: Training loss = 0.1300
2025-04-03 18:27:32,049 - INFO - Epoch 6, Batch 89: Training loss = 0.1200
2025-04-03 18:27:32,149 - INFO - Epoch 6, Batch 90: Training loss = 0.1100
2025-04-03 18:27:32,250 - INFO - Epoch 6, Batch 91: Training loss = 0.1000
2025-04-03 18:27:32,351 - INFO - Epoch 6, Batch 92: Training loss = 0.0900
2025-04-03 18:27:32,451 - INFO - Epoch 6, Batch 93: Training loss = 0.0800
2025-04-03 18:27:32,552 - INFO - Epoch 6, Batch 94: Training loss = 0.0700
2025-04-03 18:27:32,652 - INFO - Epoch 6, Batch 95: Training loss = 0.0600
2025-04-03 18:27:32,753 - INFO - Epoch 6, Batch 96: Training loss = 0.0500
2025-04-03 18:27:32,853 - INFO - Epoch 6, Batch 97: Training loss = 0.0400
2025-04-03 18:27:32,954 - INFO - Epoch 6, Batch 98: Training loss = 0.0300
2025-04-03 18:27:33,054 - INFO - Epoch 6, Batch 99: Training loss = 0.0200
2025-04-03 18:27:33,155 - INFO - Epoch 6, Batch 100: Training loss = 0.0100
2025-04-03 18:27:33,255 - INFO - Epoch 6: Validation loss = 0.0500, Validation accuracy = 0.9000
2025-04-03 18:27:33,255 - INFO - Epoch 6: MSE = 0.0500, RMSE = 0.2236, MAE = 0.0400, RÂ² = 0.9500
2025-04-03 18:27:33,255 - INFO - Epoch 7 started.
2025-04-03 18:27:33,255 - INFO - Epoch 7, Batch 1: Training loss = 1.0000
2025-04-03 18:27:33,356 - INFO - Epoch 7, Batch 2: Training loss = 0.9900
2025-04-03 18:27:33,456 - INFO - Epoch 7, Batch 3: Training loss = 0.9800
2025-04-03 18:27:33,557 - INFO - Epoch 7, Batch 4: Training loss = 0.9700
2025-04-03 18:27:33,657 - INFO - Epoch 7, Batch 5: Training loss = 0.9600
2025-04-03 18:27:33,758 - INFO - Epoch 7, Batch 6: Training loss = 0.9500
2025-04-03 18:27:33,858 - INFO - Epoch 7, Batch 7: Training loss = 0.9400
2025-04-03 18:27:33,959 - INFO - Epoch 7, Batch 8: Training loss = 0.9300
2025-04-03 18:27:34,059 - INFO - Epoch 7, Batch 9: Training loss = 0.9200
2025-04-03 18:27:34,160 - INFO - Epoch 7, Batch 10: Training loss = 0.9100
2025-04-03 18:27:34,260 - INFO - Epoch 7, Batch 11: Training loss = 0.9000
2025-04-03 18:27:34,361 - INFO - Epoch 7, Batch 12: Training loss = 0.8900
2025-04-03 18:27:34,462 - INFO - Epoch 7, Batch 13: Training loss = 0.8800
2025-04-03 18:27:34,562 - INFO - Epoch 7, Batch 14: Training loss = 0.8700
2025-04-03 18:27:34,663 - INFO - Epoch 7, Batch 15: Training loss = 0.8600
2025-04-03 18:27:34,764 - INFO - Epoch 7, Batch 16: Training loss = 0.8500
2025-04-03 18:27:34,864 - INFO - Epoch 7, Batch 17: Training loss = 0.8400
2025-04-03 18:27:34,965 - INFO - Epoch 7, Batch 18: Training loss = 0.8300
2025-04-03 18:27:35,066 - INFO - Epoch 7, Batch 19: Training loss = 0.8200
2025-04-03 18:27:35,166 - INFO - Epoch 7, Batch 20: Training loss = 0.8100
2025-04-03 18:27:35,267 - INFO - Epoch 7, Batch 21: Training loss = 0.8000
2025-04-03 18:27:35,367 - INFO - Epoch 7, Batch 22: Training loss = 0.7900
2025-04-03 18:27:35,467 - INFO - Epoch 7, Batch 23: Training loss = 0.7800
2025-04-03 18:27:35,568 - INFO - Epoch 7, Batch 24: Training loss = 0.7700
2025-04-03 18:27:35,668 - INFO - Epoch 7, Batch 25: Training loss = 0.7600
2025-04-03 18:27:35,769 - INFO - Epoch 7, Batch 26: Training loss = 0.7500
2025-04-03 18:27:35,869 - INFO - Epoch 7, Batch 27: Training loss = 0.7400
2025-04-03 18:27:35,970 - INFO - Epoch 7, Batch 28: Training loss = 0.7300
2025-04-03 18:27:36,071 - INFO - Epoch 7, Batch 29: Training loss = 0.7200
2025-04-03 18:27:36,171 - INFO - Epoch 7, Batch 30: Training loss = 0.7100
2025-04-03 18:27:36,272 - INFO - Epoch 7, Batch 31: Training loss = 0.7000
2025-04-03 18:27:36,372 - INFO - Epoch 7, Batch 32: Training loss = 0.6900
2025-04-03 18:27:36,473 - INFO - Epoch 7, Batch 33: Training loss = 0.6800
2025-04-03 18:27:36,573 - INFO - Epoch 7, Batch 34: Training loss = 0.6700
2025-04-03 18:27:36,674 - INFO - Epoch 7, Batch 35: Training loss = 0.6600
2025-04-03 18:27:36,774 - INFO - Epoch 7, Batch 36: Training loss = 0.6500
2025-04-03 18:27:36,875 - INFO - Epoch 7, Batch 37: Training loss = 0.6400
2025-04-03 18:27:36,975 - INFO - Epoch 7, Batch 38: Training loss = 0.6300
2025-04-03 18:27:37,076 - INFO - Epoch 7, Batch 39: Training loss = 0.6200
2025-04-03 18:27:37,176 - INFO - Epoch 7, Batch 40: Training loss = 0.6100
2025-04-03 18:27:37,277 - INFO - Epoch 7, Batch 41: Training loss = 0.6000
2025-04-03 18:27:37,377 - INFO - Epoch 7, Batch 42: Training loss = 0.5900
2025-04-03 18:27:37,478 - INFO - Epoch 7, Batch 43: Training loss = 0.5800
2025-04-03 18:27:37,577 - INFO - Epoch 7, Batch 44: Training loss = 0.5700
2025-04-03 18:27:37,678 - INFO - Epoch 7, Batch 45: Training loss = 0.5600
2025-04-03 18:27:37,778 - INFO - Epoch 7, Batch 46: Training loss = 0.5500
2025-04-03 18:27:37,878 - INFO - Epoch 7, Batch 47: Training loss = 0.5400
2025-04-03 18:27:37,979 - INFO - Epoch 7, Batch 48: Training loss = 0.5300
2025-04-03 18:27:38,079 - INFO - Epoch 7, Batch 49: Training loss = 0.5200
2025-04-03 18:27:38,180 - INFO - Epoch 7, Batch 50: Training loss = 0.5100
2025-04-03 18:27:38,280 - INFO - Epoch 7, Batch 51: Training loss = 0.5000
2025-04-03 18:27:38,381 - INFO - Epoch 7, Batch 52: Training loss = 0.4900
2025-04-03 18:27:38,482 - INFO - Epoch 7, Batch 53: Training loss = 0.4800
2025-04-03 18:27:38,583 - INFO - Epoch 7, Batch 54: Training loss = 0.4700
2025-04-03 18:27:38,684 - INFO - Epoch 7, Batch 55: Training loss = 0.4600
2025-04-03 18:27:38,784 - INFO - Epoch 7, Batch 56: Training loss = 0.4500
2025-04-03 18:27:38,885 - INFO - Epoch 7, Batch 57: Training loss = 0.4400
2025-04-03 18:27:38,986 - INFO - Epoch 7, Batch 58: Training loss = 0.4300
2025-04-03 18:27:39,086 - INFO - Epoch 7, Batch 59: Training loss = 0.4200
2025-04-03 18:27:39,187 - INFO - Epoch 7, Batch 60: Training loss = 0.4100
2025-04-03 18:27:39,287 - INFO - Epoch 7, Batch 61: Training loss = 0.4000
2025-04-03 18:27:39,388 - INFO - Epoch 7, Batch 62: Training loss = 0.3900
2025-04-03 18:27:39,488 - INFO - Epoch 7, Batch 63: Training loss = 0.3800
2025-04-03 18:27:39,589 - INFO - Epoch 7, Batch 64: Training loss = 0.3700
2025-04-03 18:27:39,690 - INFO - Epoch 7, Batch 65: Training loss = 0.3600
2025-04-03 18:27:39,791 - INFO - Epoch 7, Batch 66: Training loss = 0.3500
2025-04-03 18:27:39,892 - INFO - Epoch 7, Batch 67: Training loss = 0.3400
2025-04-03 18:27:39,992 - INFO - Epoch 7, Batch 68: Training loss = 0.3300
2025-04-03 18:27:40,093 - INFO - Epoch 7, Batch 69: Training loss = 0.3200
2025-04-03 18:27:40,193 - INFO - Epoch 7, Batch 70: Training loss = 0.3100
2025-04-03 18:27:40,294 - INFO - Epoch 7, Batch 71: Training loss = 0.3000
2025-04-03 18:27:40,395 - INFO - Epoch 7, Batch 72: Training loss = 0.2900
2025-04-03 18:27:40,495 - INFO - Epoch 7, Batch 73: Training loss = 0.2800
2025-04-03 18:27:40,596 - INFO - Epoch 7, Batch 74: Training loss = 0.2700
2025-04-03 18:27:40,696 - INFO - Epoch 7, Batch 75: Training loss = 0.2600
2025-04-03 18:27:40,797 - INFO - Epoch 7, Batch 76: Training loss = 0.2500
2025-04-03 18:27:40,897 - INFO - Epoch 7, Batch 77: Training loss = 0.2400
2025-04-03 18:27:40,997 - INFO - Epoch 7, Batch 78: Training loss = 0.2300
2025-04-03 18:27:41,098 - INFO - Epoch 7, Batch 79: Training loss = 0.2200
2025-04-03 18:27:41,199 - INFO - Epoch 7, Batch 80: Training loss = 0.2100
2025-04-03 18:27:41,300 - INFO - Epoch 7, Batch 81: Training loss = 0.2000
2025-04-03 18:27:41,400 - INFO - Epoch 7, Batch 82: Training loss = 0.1900
2025-04-03 18:27:41,501 - INFO - Epoch 7, Batch 83: Training loss = 0.1800
2025-04-03 18:27:41,601 - INFO - Epoch 7, Batch 84: Training loss = 0.1700
2025-04-03 18:27:41,702 - INFO - Epoch 7, Batch 85: Training loss = 0.1600
2025-04-03 18:27:41,803 - INFO - Epoch 7, Batch 86: Training loss = 0.1500
2025-04-03 18:27:41,904 - INFO - Epoch 7, Batch 87: Training loss = 0.1400
2025-04-03 18:27:42,004 - INFO - Epoch 7, Batch 88: Training loss = 0.1300
2025-04-03 18:27:42,105 - INFO - Epoch 7, Batch 89: Training loss = 0.1200
2025-04-03 18:27:42,206 - INFO - Epoch 7, Batch 90: Training loss = 0.1100
2025-04-03 18:27:42,306 - INFO - Epoch 7, Batch 91: Training loss = 0.1000
2025-04-03 18:27:42,407 - INFO - Epoch 7, Batch 92: Training loss = 0.0900
2025-04-03 18:27:42,508 - INFO - Epoch 7, Batch 93: Training loss = 0.0800
2025-04-03 18:27:42,609 - INFO - Epoch 7, Batch 94: Training loss = 0.0700
2025-04-03 18:27:42,709 - INFO - Epoch 7, Batch 95: Training loss = 0.0600
2025-04-03 18:27:42,811 - INFO - Epoch 7, Batch 96: Training loss = 0.0500
2025-04-03 18:27:42,912 - INFO - Epoch 7, Batch 97: Training loss = 0.0400
2025-04-03 18:27:43,012 - INFO - Epoch 7, Batch 98: Training loss = 0.0300
2025-04-03 18:27:43,113 - INFO - Epoch 7, Batch 99: Training loss = 0.0200
2025-04-03 18:27:43,214 - INFO - Epoch 7, Batch 100: Training loss = 0.0100
2025-04-03 18:27:43,315 - INFO - Epoch 7: Validation loss = 0.0400, Validation accuracy = 0.9200
2025-04-03 18:27:43,315 - INFO - Epoch 7: MSE = 0.0400, RMSE = 0.2000, MAE = 0.0320, RÂ² = 0.9600
2025-04-03 18:27:43,315 - INFO - Epoch 8 started.
2025-04-03 18:27:43,315 - INFO - Epoch 8, Batch 1: Training loss = 1.0000
2025-04-03 18:27:43,416 - INFO - Epoch 8, Batch 2: Training loss = 0.9900
2025-04-03 18:27:43,516 - INFO - Epoch 8, Batch 3: Training loss = 0.9800
2025-04-03 18:27:43,617 - INFO - Epoch 8, Batch 4: Training loss = 0.9700
2025-04-03 18:27:43,718 - INFO - Epoch 8, Batch 5: Training loss = 0.9600
2025-04-03 18:27:43,819 - INFO - Epoch 8, Batch 6: Training loss = 0.9500
2025-04-03 18:27:43,919 - INFO - Epoch 8, Batch 7: Training loss = 0.9400
2025-04-03 18:27:44,020 - INFO - Epoch 8, Batch 8: Training loss = 0.9300
2025-04-03 18:27:44,120 - INFO - Epoch 8, Batch 9: Training loss = 0.9200
2025-04-03 18:27:44,221 - INFO - Epoch 8, Batch 10: Training loss = 0.9100
2025-04-03 18:27:44,321 - INFO - Epoch 8, Batch 11: Training loss = 0.9000
2025-04-03 18:27:44,422 - INFO - Epoch 8, Batch 12: Training loss = 0.8900
2025-04-03 18:27:44,523 - INFO - Epoch 8, Batch 13: Training loss = 0.8800
2025-04-03 18:27:44,623 - INFO - Epoch 8, Batch 14: Training loss = 0.8700
2025-04-03 18:27:44,724 - INFO - Epoch 8, Batch 15: Training loss = 0.8600
2025-04-03 18:27:44,824 - INFO - Epoch 8, Batch 16: Training loss = 0.8500
2025-04-03 18:27:44,924 - INFO - Epoch 8, Batch 17: Training loss = 0.8400
2025-04-03 18:27:45,025 - INFO - Epoch 8, Batch 18: Training loss = 0.8300
2025-04-03 18:27:45,125 - INFO - Epoch 8, Batch 19: Training loss = 0.8200
2025-04-03 18:27:45,226 - INFO - Epoch 8, Batch 20: Training loss = 0.8100
2025-04-03 18:27:45,326 - INFO - Epoch 8, Batch 21: Training loss = 0.8000
2025-04-03 18:27:45,428 - INFO - Epoch 8, Batch 22: Training loss = 0.7900
2025-04-03 18:27:45,529 - INFO - Epoch 8, Batch 23: Training loss = 0.7800
2025-04-03 18:27:45,629 - INFO - Epoch 8, Batch 24: Training loss = 0.7700
2025-04-03 18:27:45,730 - INFO - Epoch 8, Batch 25: Training loss = 0.7600
2025-04-03 18:27:45,831 - INFO - Epoch 8, Batch 26: Training loss = 0.7500
2025-04-03 18:27:45,932 - INFO - Epoch 8, Batch 27: Training loss = 0.7400
2025-04-03 18:27:46,032 - INFO - Epoch 8, Batch 28: Training loss = 0.7300
2025-04-03 18:27:46,133 - INFO - Epoch 8, Batch 29: Training loss = 0.7200
2025-04-03 18:27:46,234 - INFO - Epoch 8, Batch 30: Training loss = 0.7100
2025-04-03 18:27:46,335 - INFO - Epoch 8, Batch 31: Training loss = 0.7000
2025-04-03 18:27:46,435 - INFO - Epoch 8, Batch 32: Training loss = 0.6900
2025-04-03 18:27:46,536 - INFO - Epoch 8, Batch 33: Training loss = 0.6800
2025-04-03 18:27:46,637 - INFO - Epoch 8, Batch 34: Training loss = 0.6700
2025-04-03 18:27:46,737 - INFO - Epoch 8, Batch 35: Training loss = 0.6600
2025-04-03 18:27:46,838 - INFO - Epoch 8, Batch 36: Training loss = 0.6500
2025-04-03 18:27:46,939 - INFO - Epoch 8, Batch 37: Training loss = 0.6400
2025-04-03 18:27:47,039 - INFO - Epoch 8, Batch 38: Training loss = 0.6300
2025-04-03 18:27:47,140 - INFO - Epoch 8, Batch 39: Training loss = 0.6200
2025-04-03 18:27:47,241 - INFO - Epoch 8, Batch 40: Training loss = 0.6100
2025-04-03 18:27:47,342 - INFO - Epoch 8, Batch 41: Training loss = 0.6000
2025-04-03 18:27:47,442 - INFO - Epoch 8, Batch 42: Training loss = 0.5900
2025-04-03 18:27:47,543 - INFO - Epoch 8, Batch 43: Training loss = 0.5800
2025-04-03 18:27:47,644 - INFO - Epoch 8, Batch 44: Training loss = 0.5700
2025-04-03 18:27:47,745 - INFO - Epoch 8, Batch 45: Training loss = 0.5600
2025-04-03 18:27:47,845 - INFO - Epoch 8, Batch 46: Training loss = 0.5500
2025-04-03 18:27:47,945 - INFO - Epoch 8, Batch 47: Training loss = 0.5400
2025-04-03 18:27:48,046 - INFO - Epoch 8, Batch 48: Training loss = 0.5300
2025-04-03 18:27:48,146 - INFO - Epoch 8, Batch 49: Training loss = 0.5200
2025-04-03 18:27:48,247 - INFO - Epoch 8, Batch 50: Training loss = 0.5100
2025-04-03 18:27:48,347 - INFO - Epoch 8, Batch 51: Training loss = 0.5000
2025-04-03 18:27:48,448 - INFO - Epoch 8, Batch 52: Training loss = 0.4900
2025-04-03 18:27:48,548 - INFO - Epoch 8, Batch 53: Training loss = 0.4800
2025-04-03 18:27:48,649 - INFO - Epoch 8, Batch 54: Training loss = 0.4700
2025-04-03 18:27:48,749 - INFO - Epoch 8, Batch 55: Training loss = 0.4600
2025-04-03 18:27:48,850 - INFO - Epoch 8, Batch 56: Training loss = 0.4500
2025-04-03 18:27:48,950 - INFO - Epoch 8, Batch 57: Training loss = 0.4400
2025-04-03 18:27:49,051 - INFO - Epoch 8, Batch 58: Training loss = 0.4300
2025-04-03 18:27:49,151 - INFO - Epoch 8, Batch 59: Training loss = 0.4200
2025-04-03 18:27:49,252 - INFO - Epoch 8, Batch 60: Training loss = 0.4100
2025-04-03 18:27:49,352 - INFO - Epoch 8, Batch 61: Training loss = 0.4000
2025-04-03 18:27:49,453 - INFO - Epoch 8, Batch 62: Training loss = 0.3900
2025-04-03 18:27:49,554 - INFO - Epoch 8, Batch 63: Training loss = 0.3800
2025-04-03 18:27:49,654 - INFO - Epoch 8, Batch 64: Training loss = 0.3700
2025-04-03 18:27:49,755 - INFO - Epoch 8, Batch 65: Training loss = 0.3600
2025-04-03 18:27:49,856 - INFO - Epoch 8, Batch 66: Training loss = 0.3500
2025-04-03 18:27:49,957 - INFO - Epoch 8, Batch 67: Training loss = 0.3400
2025-04-03 18:27:50,058 - INFO - Epoch 8, Batch 68: Training loss = 0.3300
2025-04-03 18:27:50,159 - INFO - Epoch 8, Batch 69: Training loss = 0.3200
2025-04-03 18:27:50,260 - INFO - Epoch 8, Batch 70: Training loss = 0.3100
2025-04-03 18:27:50,360 - INFO - Epoch 8, Batch 71: Training loss = 0.3000
2025-04-03 18:27:50,461 - INFO - Epoch 8, Batch 72: Training loss = 0.2900
2025-04-03 18:27:50,562 - INFO - Epoch 8, Batch 73: Training loss = 0.2800
2025-04-03 18:27:50,663 - INFO - Epoch 8, Batch 74: Training loss = 0.2700
2025-04-03 18:27:50,763 - INFO - Epoch 8, Batch 75: Training loss = 0.2600
2025-04-03 18:27:50,864 - INFO - Epoch 8, Batch 76: Training loss = 0.2500
2025-04-03 18:27:50,965 - INFO - Epoch 8, Batch 77: Training loss = 0.2400
2025-04-03 18:27:51,065 - INFO - Epoch 8, Batch 78: Training loss = 0.2300
2025-04-03 18:27:51,166 - INFO - Epoch 8, Batch 79: Training loss = 0.2200
2025-04-03 18:27:51,266 - INFO - Epoch 8, Batch 80: Training loss = 0.2100
2025-04-03 18:27:51,367 - INFO - Epoch 8, Batch 81: Training loss = 0.2000
2025-04-03 18:27:51,467 - INFO - Epoch 8, Batch 82: Training loss = 0.1900
2025-04-03 18:27:51,568 - INFO - Epoch 8, Batch 83: Training loss = 0.1800
2025-04-03 18:27:51,668 - INFO - Epoch 8, Batch 84: Training loss = 0.1700
2025-04-03 18:27:51,769 - INFO - Epoch 8, Batch 85: Training loss = 0.1600
2025-04-03 18:27:51,869 - INFO - Epoch 8, Batch 86: Training loss = 0.1500
2025-04-03 18:27:51,969 - INFO - Epoch 8, Batch 87: Training loss = 0.1400
2025-04-03 18:27:52,070 - INFO - Epoch 8, Batch 88: Training loss = 0.1300
2025-04-03 18:27:52,170 - INFO - Epoch 8, Batch 89: Training loss = 0.1200
2025-04-03 18:27:52,271 - INFO - Epoch 8, Batch 90: Training loss = 0.1100
2025-04-03 18:27:52,371 - INFO - Epoch 8, Batch 91: Training loss = 0.1000
2025-04-03 18:27:52,471 - INFO - Epoch 8, Batch 92: Training loss = 0.0900
2025-04-03 18:27:52,572 - INFO - Epoch 8, Batch 93: Training loss = 0.0800
2025-04-03 18:27:52,672 - INFO - Epoch 8, Batch 94: Training loss = 0.0700
2025-04-03 18:27:52,773 - INFO - Epoch 8, Batch 95: Training loss = 0.0600
2025-04-03 18:27:52,873 - INFO - Epoch 8, Batch 96: Training loss = 0.0500
2025-04-03 18:27:52,973 - INFO - Epoch 8, Batch 97: Training loss = 0.0400
2025-04-03 18:27:53,074 - INFO - Epoch 8, Batch 98: Training loss = 0.0300
2025-04-03 18:27:53,174 - INFO - Epoch 8, Batch 99: Training loss = 0.0200
2025-04-03 18:27:53,275 - INFO - Epoch 8, Batch 100: Training loss = 0.0100
2025-04-03 18:27:53,375 - INFO - Epoch 8: Validation loss = 0.0300, Validation accuracy = 0.9400
2025-04-03 18:27:53,375 - INFO - Epoch 8: MSE = 0.0300, RMSE = 0.1732, MAE = 0.0240, RÂ² = 0.9700
2025-04-03 18:27:53,376 - INFO - Epoch 9 started.
2025-04-03 18:27:53,376 - INFO - Epoch 9, Batch 1: Training loss = 1.0000
2025-04-03 18:27:53,476 - INFO - Epoch 9, Batch 2: Training loss = 0.9900
2025-04-03 18:27:53,576 - INFO - Epoch 9, Batch 3: Training loss = 0.9800
2025-04-03 18:27:53,677 - INFO - Epoch 9, Batch 4: Training loss = 0.9700
2025-04-03 18:27:53,777 - INFO - Epoch 9, Batch 5: Training loss = 0.9600
2025-04-03 18:27:53,878 - INFO - Epoch 9, Batch 6: Training loss = 0.9500
2025-04-03 18:27:53,978 - INFO - Epoch 9, Batch 7: Training loss = 0.9400
2025-04-03 18:27:54,078 - INFO - Epoch 9, Batch 8: Training loss = 0.9300
2025-04-03 18:27:54,179 - INFO - Epoch 9, Batch 9: Training loss = 0.9200
2025-04-03 18:27:54,279 - INFO - Epoch 9, Batch 10: Training loss = 0.9100
2025-04-03 18:27:54,380 - INFO - Epoch 9, Batch 11: Training loss = 0.9000
2025-04-03 18:27:54,480 - INFO - Epoch 9, Batch 12: Training loss = 0.8900
2025-04-03 18:27:54,580 - INFO - Epoch 9, Batch 13: Training loss = 0.8800
2025-04-03 18:27:54,681 - INFO - Epoch 9, Batch 14: Training loss = 0.8700
2025-04-03 18:27:54,781 - INFO - Epoch 9, Batch 15: Training loss = 0.8600
2025-04-03 18:27:54,882 - INFO - Epoch 9, Batch 16: Training loss = 0.8500
2025-04-03 18:27:54,982 - INFO - Epoch 9, Batch 17: Training loss = 0.8400
2025-04-03 18:27:55,082 - INFO - Epoch 9, Batch 18: Training loss = 0.8300
2025-04-03 18:27:55,183 - INFO - Epoch 9, Batch 19: Training loss = 0.8200
2025-04-03 18:27:55,283 - INFO - Epoch 9, Batch 20: Training loss = 0.8100
2025-04-03 18:27:55,384 - INFO - Epoch 9, Batch 21: Training loss = 0.8000
2025-04-03 18:27:55,484 - INFO - Epoch 9, Batch 22: Training loss = 0.7900
2025-04-03 18:27:55,585 - INFO - Epoch 9, Batch 23: Training loss = 0.7800
2025-04-03 18:27:55,685 - INFO - Epoch 9, Batch 24: Training loss = 0.7700
2025-04-03 18:27:55,785 - INFO - Epoch 9, Batch 25: Training loss = 0.7600
2025-04-03 18:27:55,886 - INFO - Epoch 9, Batch 26: Training loss = 0.7500
2025-04-03 18:27:55,986 - INFO - Epoch 9, Batch 27: Training loss = 0.7400
2025-04-03 18:27:56,087 - INFO - Epoch 9, Batch 28: Training loss = 0.7300
2025-04-03 18:27:56,187 - INFO - Epoch 9, Batch 29: Training loss = 0.7200
2025-04-03 18:27:56,288 - INFO - Epoch 9, Batch 30: Training loss = 0.7100
2025-04-03 18:27:56,388 - INFO - Epoch 9, Batch 31: Training loss = 0.7000
2025-04-03 18:27:56,488 - INFO - Epoch 9, Batch 32: Training loss = 0.6900
2025-04-03 18:27:56,589 - INFO - Epoch 9, Batch 33: Training loss = 0.6800
2025-04-03 18:27:56,689 - INFO - Epoch 9, Batch 34: Training loss = 0.6700
2025-04-03 18:27:56,790 - INFO - Epoch 9, Batch 35: Training loss = 0.6600
2025-04-03 18:27:56,890 - INFO - Epoch 9, Batch 36: Training loss = 0.6500
2025-04-03 18:27:56,991 - INFO - Epoch 9, Batch 37: Training loss = 0.6400
2025-04-03 18:27:57,091 - INFO - Epoch 9, Batch 38: Training loss = 0.6300
2025-04-03 18:27:57,191 - INFO - Epoch 9, Batch 39: Training loss = 0.6200
2025-04-03 18:27:57,292 - INFO - Epoch 9, Batch 40: Training loss = 0.6100
2025-04-03 18:27:57,392 - INFO - Epoch 9, Batch 41: Training loss = 0.6000
2025-04-03 18:27:57,493 - INFO - Epoch 9, Batch 42: Training loss = 0.5900
2025-04-03 18:27:57,593 - INFO - Epoch 9, Batch 43: Training loss = 0.5800
2025-04-03 18:27:57,694 - INFO - Epoch 9, Batch 44: Training loss = 0.5700
2025-04-03 18:27:57,794 - INFO - Epoch 9, Batch 45: Training loss = 0.5600
2025-04-03 18:27:57,895 - INFO - Epoch 9, Batch 46: Training loss = 0.5500
2025-04-03 18:27:57,995 - INFO - Epoch 9, Batch 47: Training loss = 0.5400
2025-04-03 18:27:58,095 - INFO - Epoch 9, Batch 48: Training loss = 0.5300
2025-04-03 18:27:58,196 - INFO - Epoch 9, Batch 49: Training loss = 0.5200
2025-04-03 18:27:58,296 - INFO - Epoch 9, Batch 50: Training loss = 0.5100
2025-04-03 18:27:58,397 - INFO - Epoch 9, Batch 51: Training loss = 0.5000
2025-04-03 18:27:58,497 - INFO - Epoch 9, Batch 52: Training loss = 0.4900
2025-04-03 18:27:58,598 - INFO - Epoch 9, Batch 53: Training loss = 0.4800
2025-04-03 18:27:58,698 - INFO - Epoch 9, Batch 54: Training loss = 0.4700
2025-04-03 18:27:58,799 - INFO - Epoch 9, Batch 55: Training loss = 0.4600
2025-04-03 18:27:58,899 - INFO - Epoch 9, Batch 56: Training loss = 0.4500
2025-04-03 18:27:59,000 - INFO - Epoch 9, Batch 57: Training loss = 0.4400
2025-04-03 18:27:59,100 - INFO - Epoch 9, Batch 58: Training loss = 0.4300
2025-04-03 18:27:59,200 - INFO - Epoch 9, Batch 59: Training loss = 0.4200
2025-04-03 18:27:59,301 - INFO - Epoch 9, Batch 60: Training loss = 0.4100
2025-04-03 18:27:59,401 - INFO - Epoch 9, Batch 61: Training loss = 0.4000
2025-04-03 18:27:59,502 - INFO - Epoch 9, Batch 62: Training loss = 0.3900
2025-04-03 18:27:59,602 - INFO - Epoch 9, Batch 63: Training loss = 0.3800
2025-04-03 18:27:59,703 - INFO - Epoch 9, Batch 64: Training loss = 0.3700
2025-04-03 18:27:59,803 - INFO - Epoch 9, Batch 65: Training loss = 0.3600
2025-04-03 18:27:59,903 - INFO - Epoch 9, Batch 66: Training loss = 0.3500
2025-04-03 18:28:00,004 - INFO - Epoch 9, Batch 67: Training loss = 0.3400
2025-04-03 18:28:00,104 - INFO - Epoch 9, Batch 68: Training loss = 0.3300
2025-04-03 18:28:00,205 - INFO - Epoch 9, Batch 69: Training loss = 0.3200
2025-04-03 18:28:00,305 - INFO - Epoch 9, Batch 70: Training loss = 0.3100
2025-04-03 18:28:00,406 - INFO - Epoch 9, Batch 71: Training loss = 0.3000
2025-04-03 18:28:00,506 - INFO - Epoch 9, Batch 72: Training loss = 0.2900
2025-04-03 18:28:00,606 - INFO - Epoch 9, Batch 73: Training loss = 0.2800
2025-04-03 18:28:00,707 - INFO - Epoch 9, Batch 74: Training loss = 0.2700
2025-04-03 18:28:00,807 - INFO - Epoch 9, Batch 75: Training loss = 0.2600
2025-04-03 18:28:00,907 - INFO - Epoch 9, Batch 76: Training loss = 0.2500
2025-04-03 18:28:01,008 - INFO - Epoch 9, Batch 77: Training loss = 0.2400
2025-04-03 18:28:01,108 - INFO - Epoch 9, Batch 78: Training loss = 0.2300
2025-04-03 18:28:01,209 - INFO - Epoch 9, Batch 79: Training loss = 0.2200
2025-04-03 18:28:01,309 - INFO - Epoch 9, Batch 80: Training loss = 0.2100
2025-04-03 18:28:01,410 - INFO - Epoch 9, Batch 81: Training loss = 0.2000
2025-04-03 18:28:01,510 - INFO - Epoch 9, Batch 82: Training loss = 0.1900
2025-04-03 18:28:01,610 - INFO - Epoch 9, Batch 83: Training loss = 0.1800
2025-04-03 18:28:01,711 - INFO - Epoch 9, Batch 84: Training loss = 0.1700
2025-04-03 18:28:01,811 - INFO - Epoch 9, Batch 85: Training loss = 0.1600
2025-04-03 18:28:01,912 - INFO - Epoch 9, Batch 86: Training loss = 0.1500
2025-04-03 18:28:02,012 - INFO - Epoch 9, Batch 87: Training loss = 0.1400
2025-04-03 18:28:02,112 - INFO - Epoch 9, Batch 88: Training loss = 0.1300
2025-04-03 18:28:02,213 - INFO - Epoch 9, Batch 89: Training loss = 0.1200
2025-04-03 18:28:02,313 - INFO - Epoch 9, Batch 90: Training loss = 0.1100
2025-04-03 18:28:02,414 - INFO - Epoch 9, Batch 91: Training loss = 0.1000
2025-04-03 18:28:02,514 - INFO - Epoch 9, Batch 92: Training loss = 0.0900
2025-04-03 18:28:02,615 - INFO - Epoch 9, Batch 93: Training loss = 0.0800
2025-04-03 18:28:02,715 - INFO - Epoch 9, Batch 94: Training loss = 0.0700
2025-04-03 18:28:02,816 - INFO - Epoch 9, Batch 95: Training loss = 0.0600
2025-04-03 18:28:02,916 - INFO - Epoch 9, Batch 96: Training loss = 0.0500
2025-04-03 18:28:03,017 - INFO - Epoch 9, Batch 97: Training loss = 0.0400
2025-04-03 18:28:03,117 - INFO - Epoch 9, Batch 98: Training loss = 0.0300
2025-04-03 18:28:03,217 - INFO - Epoch 9, Batch 99: Training loss = 0.0200
2025-04-03 18:28:03,318 - INFO - Epoch 9, Batch 100: Training loss = 0.0100
2025-04-03 18:28:03,418 - INFO - Epoch 9: Validation loss = 0.0200, Validation accuracy = 0.9600
2025-04-03 18:28:03,418 - INFO - Epoch 9: MSE = 0.0200, RMSE = 0.1414, MAE = 0.0160, RÂ² = 0.9800
2025-04-03 18:28:03,419 - INFO - Epoch 10 started.
2025-04-03 18:28:03,419 - INFO - Epoch 10, Batch 1: Training loss = 1.0000
2025-04-03 18:28:03,519 - INFO - Epoch 10, Batch 2: Training loss = 0.9900
2025-04-03 18:28:03,619 - INFO - Epoch 10, Batch 3: Training loss = 0.9800
2025-04-03 18:28:03,720 - INFO - Epoch 10, Batch 4: Training loss = 0.9700
2025-04-03 18:28:03,821 - INFO - Epoch 10, Batch 5: Training loss = 0.9600
2025-04-03 18:28:03,921 - INFO - Epoch 10, Batch 6: Training loss = 0.9500
2025-04-03 18:28:04,022 - INFO - Epoch 10, Batch 7: Training loss = 0.9400
2025-04-03 18:28:04,122 - INFO - Epoch 10, Batch 8: Training loss = 0.9300
2025-04-03 18:28:04,223 - INFO - Epoch 10, Batch 9: Training loss = 0.9200
2025-04-03 18:28:04,323 - INFO - Epoch 10, Batch 10: Training loss = 0.9100
2025-04-03 18:28:04,424 - INFO - Epoch 10, Batch 11: Training loss = 0.9000
2025-04-03 18:28:04,525 - INFO - Epoch 10, Batch 12: Training loss = 0.8900
2025-04-03 18:28:04,625 - INFO - Epoch 10, Batch 13: Training loss = 0.8800
2025-04-03 18:28:04,726 - INFO - Epoch 10, Batch 14: Training loss = 0.8700
2025-04-03 18:28:04,827 - INFO - Epoch 10, Batch 15: Training loss = 0.8600
2025-04-03 18:28:04,927 - INFO - Epoch 10, Batch 16: Training loss = 0.8500
2025-04-03 18:28:05,028 - INFO - Epoch 10, Batch 17: Training loss = 0.8400
2025-04-03 18:28:05,128 - INFO - Epoch 10, Batch 18: Training loss = 0.8300
2025-04-03 18:28:05,229 - INFO - Epoch 10, Batch 19: Training loss = 0.8200
2025-04-03 18:28:05,330 - INFO - Epoch 10, Batch 20: Training loss = 0.8100
2025-04-03 18:28:05,430 - INFO - Epoch 10, Batch 21: Training loss = 0.8000
2025-04-03 18:28:05,531 - INFO - Epoch 10, Batch 22: Training loss = 0.7900
2025-04-03 18:28:05,631 - INFO - Epoch 10, Batch 23: Training loss = 0.7800
2025-04-03 18:28:05,731 - INFO - Epoch 10, Batch 24: Training loss = 0.7700
2025-04-03 18:28:05,832 - INFO - Epoch 10, Batch 25: Training loss = 0.7600
2025-04-03 18:28:05,933 - INFO - Epoch 10, Batch 26: Training loss = 0.7500
2025-04-03 18:28:06,033 - INFO - Epoch 10, Batch 27: Training loss = 0.7400
2025-04-03 18:28:06,134 - INFO - Epoch 10, Batch 28: Training loss = 0.7300
2025-04-03 18:28:06,234 - INFO - Epoch 10, Batch 29: Training loss = 0.7200
2025-04-03 18:28:06,335 - INFO - Epoch 10, Batch 30: Training loss = 0.7100
2025-04-03 18:28:06,435 - INFO - Epoch 10, Batch 31: Training loss = 0.7000
2025-04-03 18:28:06,536 - INFO - Epoch 10, Batch 32: Training loss = 0.6900
2025-04-03 18:28:06,636 - INFO - Epoch 10, Batch 33: Training loss = 0.6800
2025-04-03 18:28:06,737 - INFO - Epoch 10, Batch 34: Training loss = 0.6700
2025-04-03 18:28:06,837 - INFO - Epoch 10, Batch 35: Training loss = 0.6600
2025-04-03 18:28:06,938 - INFO - Epoch 10, Batch 36: Training loss = 0.6500
2025-04-03 18:28:07,038 - INFO - Epoch 10, Batch 37: Training loss = 0.6400
2025-04-03 18:28:07,139 - INFO - Epoch 10, Batch 38: Training loss = 0.6300
2025-04-03 18:28:07,239 - INFO - Epoch 10, Batch 39: Training loss = 0.6200
2025-04-03 18:28:07,339 - INFO - Epoch 10, Batch 40: Training loss = 0.6100
2025-04-03 18:28:07,440 - INFO - Epoch 10, Batch 41: Training loss = 0.6000
2025-04-03 18:28:07,540 - INFO - Epoch 10, Batch 42: Training loss = 0.5900
2025-04-03 18:28:07,639 - INFO - Epoch 10, Batch 43: Training loss = 0.5800
2025-04-03 18:28:07,739 - INFO - Epoch 10, Batch 44: Training loss = 0.5700
2025-04-03 18:28:07,840 - INFO - Epoch 10, Batch 45: Training loss = 0.5600
2025-04-03 18:28:07,940 - INFO - Epoch 10, Batch 46: Training loss = 0.5500
2025-04-03 18:28:08,041 - INFO - Epoch 10, Batch 47: Training loss = 0.5400
2025-04-03 18:28:08,141 - INFO - Epoch 10, Batch 48: Training loss = 0.5300
2025-04-03 18:28:08,241 - INFO - Epoch 10, Batch 49: Training loss = 0.5200
2025-04-03 18:28:08,345 - INFO - Epoch 10, Batch 50: Training loss = 0.5100
2025-04-03 18:28:08,445 - INFO - Epoch 10, Batch 51: Training loss = 0.5000
2025-04-03 18:28:08,545 - INFO - Epoch 10, Batch 52: Training loss = 0.4900
2025-04-03 18:28:08,646 - INFO - Epoch 10, Batch 53: Training loss = 0.4800
2025-04-03 18:28:08,747 - INFO - Epoch 10, Batch 54: Training loss = 0.4700
2025-04-03 18:28:08,847 - INFO - Epoch 10, Batch 55: Training loss = 0.4600
2025-04-03 18:28:08,948 - INFO - Epoch 10, Batch 56: Training loss = 0.4500
2025-04-03 18:28:09,049 - INFO - Epoch 10, Batch 57: Training loss = 0.4400
2025-04-03 18:28:09,150 - INFO - Epoch 10, Batch 58: Training loss = 0.4300
2025-04-03 18:28:09,250 - INFO - Epoch 10, Batch 59: Training loss = 0.4200
2025-04-03 18:28:09,350 - INFO - Epoch 10, Batch 60: Training loss = 0.4100
2025-04-03 18:28:09,451 - INFO - Epoch 10, Batch 61: Training loss = 0.4000
2025-04-03 18:28:09,551 - INFO - Epoch 10, Batch 62: Training loss = 0.3900
2025-04-03 18:28:09,652 - INFO - Epoch 10, Batch 63: Training loss = 0.3800
2025-04-03 18:28:09,752 - INFO - Epoch 10, Batch 64: Training loss = 0.3700
2025-04-03 18:28:09,853 - INFO - Epoch 10, Batch 65: Training loss = 0.3600
2025-04-03 18:28:09,953 - INFO - Epoch 10, Batch 66: Training loss = 0.3500
2025-04-03 18:28:10,053 - INFO - Epoch 10, Batch 67: Training loss = 0.3400
2025-04-03 18:28:10,154 - INFO - Epoch 10, Batch 68: Training loss = 0.3300
2025-04-03 18:28:10,254 - INFO - Epoch 10, Batch 69: Training loss = 0.3200
2025-04-03 18:28:10,355 - INFO - Epoch 10, Batch 70: Training loss = 0.3100
2025-04-03 18:28:10,455 - INFO - Epoch 10, Batch 71: Training loss = 0.3000
2025-04-03 18:28:10,555 - INFO - Epoch 10, Batch 72: Training loss = 0.2900
2025-04-03 18:28:10,656 - INFO - Epoch 10, Batch 73: Training loss = 0.2800
2025-04-03 18:28:10,756 - INFO - Epoch 10, Batch 74: Training loss = 0.2700
2025-04-03 18:28:10,857 - INFO - Epoch 10, Batch 75: Training loss = 0.2600
2025-04-03 18:28:10,957 - INFO - Epoch 10, Batch 76: Training loss = 0.2500
2025-04-03 18:28:11,057 - INFO - Epoch 10, Batch 77: Training loss = 0.2400
2025-04-03 18:28:11,158 - INFO - Epoch 10, Batch 78: Training loss = 0.2300
2025-04-03 18:28:11,258 - INFO - Epoch 10, Batch 79: Training loss = 0.2200
2025-04-03 18:28:11,359 - INFO - Epoch 10, Batch 80: Training loss = 0.2100
2025-04-03 18:28:11,459 - INFO - Epoch 10, Batch 81: Training loss = 0.2000
2025-04-03 18:28:11,559 - INFO - Epoch 10, Batch 82: Training loss = 0.1900
2025-04-03 18:28:11,660 - INFO - Epoch 10, Batch 83: Training loss = 0.1800
2025-04-03 18:28:11,760 - INFO - Epoch 10, Batch 84: Training loss = 0.1700
2025-04-03 18:28:11,861 - INFO - Epoch 10, Batch 85: Training loss = 0.1600
2025-04-03 18:28:11,961 - INFO - Epoch 10, Batch 86: Training loss = 0.1500
2025-04-03 18:28:12,062 - INFO - Epoch 10, Batch 87: Training loss = 0.1400
2025-04-03 18:28:12,162 - INFO - Epoch 10, Batch 88: Training loss = 0.1300
2025-04-03 18:28:12,262 - INFO - Epoch 10, Batch 89: Training loss = 0.1200
2025-04-03 18:28:12,363 - INFO - Epoch 10, Batch 90: Training loss = 0.1100
2025-04-03 18:28:12,463 - INFO - Epoch 10, Batch 91: Training loss = 0.1000
2025-04-03 18:28:12,564 - INFO - Epoch 10, Batch 92: Training loss = 0.0900
2025-04-03 18:28:12,664 - INFO - Epoch 10, Batch 93: Training loss = 0.0800
2025-04-03 18:28:12,765 - INFO - Epoch 10, Batch 94: Training loss = 0.0700
2025-04-03 18:28:12,865 - INFO - Epoch 10, Batch 95: Training loss = 0.0600
2025-04-03 18:28:12,966 - INFO - Epoch 10, Batch 96: Training loss = 0.0500
2025-04-03 18:28:13,066 - INFO - Epoch 10, Batch 97: Training loss = 0.0400
2025-04-03 18:28:13,166 - INFO - Epoch 10, Batch 98: Training loss = 0.0300
2025-04-03 18:28:13,267 - INFO - Epoch 10, Batch 99: Training loss = 0.0200
2025-04-03 18:28:13,367 - INFO - Epoch 10, Batch 100: Training loss = 0.0100
2025-04-03 18:28:13,468 - INFO - Epoch 10: Validation loss = 0.0100, Validation accuracy = 0.9800
2025-04-03 18:28:13,468 - INFO - Epoch 10: MSE = 0.0100, RMSE = 0.1000, MAE = 0.0080, RÂ² = 0.9900
2025-04-03 18:28:13,468 - INFO - Regression training process completed.
