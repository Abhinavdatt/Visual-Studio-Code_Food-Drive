2025-04-03 18:07:33,245 - INFO - Starting regression training process...
2025-04-03 18:07:33,246 - INFO - Epoch 1 started.
2025-04-03 18:07:33,246 - INFO - Epoch 1, Batch 1: Training loss = 1.0000
2025-04-03 18:07:33,346 - INFO - Epoch 1, Batch 2: Training loss = 0.9900
2025-04-03 18:07:33,448 - INFO - Epoch 1, Batch 3: Training loss = 0.9800
2025-04-03 18:07:33,548 - INFO - Epoch 1, Batch 4: Training loss = 0.9700
2025-04-03 18:07:33,649 - INFO - Epoch 1, Batch 5: Training loss = 0.9600
2025-04-03 18:07:33,749 - INFO - Epoch 1, Batch 6: Training loss = 0.9500
2025-04-03 18:07:33,850 - INFO - Epoch 1, Batch 7: Training loss = 0.9400
2025-04-03 18:07:33,950 - INFO - Epoch 1, Batch 8: Training loss = 0.9300
2025-04-03 18:07:34,050 - INFO - Epoch 1, Batch 9: Training loss = 0.9200
2025-04-03 18:07:34,151 - INFO - Epoch 1, Batch 10: Training loss = 0.9100
2025-04-03 18:07:34,251 - INFO - Epoch 1, Batch 11: Training loss = 0.9000
2025-04-03 18:07:34,351 - INFO - Epoch 1, Batch 12: Training loss = 0.8900
2025-04-03 18:07:34,452 - INFO - Epoch 1, Batch 13: Training loss = 0.8800
2025-04-03 18:07:34,553 - INFO - Epoch 1, Batch 14: Training loss = 0.8700
2025-04-03 18:07:34,653 - INFO - Epoch 1, Batch 15: Training loss = 0.8600
2025-04-03 18:07:34,754 - INFO - Epoch 1, Batch 16: Training loss = 0.8500
2025-04-03 18:07:34,855 - INFO - Epoch 1, Batch 17: Training loss = 0.8400
2025-04-03 18:07:34,955 - INFO - Epoch 1, Batch 18: Training loss = 0.8300
2025-04-03 18:07:35,056 - INFO - Epoch 1, Batch 19: Training loss = 0.8200
2025-04-03 18:07:35,156 - INFO - Epoch 1, Batch 20: Training loss = 0.8100
2025-04-03 18:07:35,256 - INFO - Epoch 1, Batch 21: Training loss = 0.8000
2025-04-03 18:07:35,357 - INFO - Epoch 1, Batch 22: Training loss = 0.7900
2025-04-03 18:07:35,458 - INFO - Epoch 1, Batch 23: Training loss = 0.7800
2025-04-03 18:07:35,558 - INFO - Epoch 1, Batch 24: Training loss = 0.7700
2025-04-03 18:07:35,658 - INFO - Epoch 1, Batch 25: Training loss = 0.7600
2025-04-03 18:07:35,759 - INFO - Epoch 1, Batch 26: Training loss = 0.7500
2025-04-03 18:07:35,860 - INFO - Epoch 1, Batch 27: Training loss = 0.7400
2025-04-03 18:07:35,960 - INFO - Epoch 1, Batch 28: Training loss = 0.7300
2025-04-03 18:07:36,061 - INFO - Epoch 1, Batch 29: Training loss = 0.7200
2025-04-03 18:07:36,162 - INFO - Epoch 1, Batch 30: Training loss = 0.7100
2025-04-03 18:07:36,263 - INFO - Epoch 1, Batch 31: Training loss = 0.7000
2025-04-03 18:07:36,364 - INFO - Epoch 1, Batch 32: Training loss = 0.6900
2025-04-03 18:07:36,465 - INFO - Epoch 1, Batch 33: Training loss = 0.6800
2025-04-03 18:07:36,565 - INFO - Epoch 1, Batch 34: Training loss = 0.6700
2025-04-03 18:07:36,666 - INFO - Epoch 1, Batch 35: Training loss = 0.6600
2025-04-03 18:07:36,767 - INFO - Epoch 1, Batch 36: Training loss = 0.6500
2025-04-03 18:07:36,868 - INFO - Epoch 1, Batch 37: Training loss = 0.6400
2025-04-03 18:07:36,969 - INFO - Epoch 1, Batch 38: Training loss = 0.6300
2025-04-03 18:07:37,070 - INFO - Epoch 1, Batch 39: Training loss = 0.6200
2025-04-03 18:07:37,170 - INFO - Epoch 1, Batch 40: Training loss = 0.6100
2025-04-03 18:07:37,271 - INFO - Epoch 1, Batch 41: Training loss = 0.6000
2025-04-03 18:07:37,371 - INFO - Epoch 1, Batch 42: Training loss = 0.5900
2025-04-03 18:07:37,471 - INFO - Epoch 1, Batch 43: Training loss = 0.5800
2025-04-03 18:07:37,571 - INFO - Epoch 1, Batch 44: Training loss = 0.5700
2025-04-03 18:07:37,672 - INFO - Epoch 1, Batch 45: Training loss = 0.5600
2025-04-03 18:07:37,773 - INFO - Epoch 1, Batch 46: Training loss = 0.5500
2025-04-03 18:07:37,873 - INFO - Epoch 1, Batch 47: Training loss = 0.5400
2025-04-03 18:07:37,974 - INFO - Epoch 1, Batch 48: Training loss = 0.5300
2025-04-03 18:07:38,074 - INFO - Epoch 1, Batch 49: Training loss = 0.5200
2025-04-03 18:07:38,175 - INFO - Epoch 1, Batch 50: Training loss = 0.5100
2025-04-03 18:07:38,275 - INFO - Epoch 1, Batch 51: Training loss = 0.5000
2025-04-03 18:07:38,376 - INFO - Epoch 1, Batch 52: Training loss = 0.4900
2025-04-03 18:07:38,476 - INFO - Epoch 1, Batch 53: Training loss = 0.4800
2025-04-03 18:07:38,577 - INFO - Epoch 1, Batch 54: Training loss = 0.4700
2025-04-03 18:07:38,678 - INFO - Epoch 1, Batch 55: Training loss = 0.4600
2025-04-03 18:07:38,778 - INFO - Epoch 1, Batch 56: Training loss = 0.4500
2025-04-03 18:07:38,878 - INFO - Epoch 1, Batch 57: Training loss = 0.4400
2025-04-03 18:07:38,979 - INFO - Epoch 1, Batch 58: Training loss = 0.4300
2025-04-03 18:07:39,080 - INFO - Epoch 1, Batch 59: Training loss = 0.4200
2025-04-03 18:07:39,180 - INFO - Epoch 1, Batch 60: Training loss = 0.4100
2025-04-03 18:07:39,281 - INFO - Epoch 1, Batch 61: Training loss = 0.4000
2025-04-03 18:07:39,381 - INFO - Epoch 1, Batch 62: Training loss = 0.3900
2025-04-03 18:07:39,482 - INFO - Epoch 1, Batch 63: Training loss = 0.3800
2025-04-03 18:07:39,582 - INFO - Epoch 1, Batch 64: Training loss = 0.3700
2025-04-03 18:07:39,683 - INFO - Epoch 1, Batch 65: Training loss = 0.3600
2025-04-03 18:07:39,784 - INFO - Epoch 1, Batch 66: Training loss = 0.3500
2025-04-03 18:07:39,884 - INFO - Epoch 1, Batch 67: Training loss = 0.3400
2025-04-03 18:07:39,985 - INFO - Epoch 1, Batch 68: Training loss = 0.3300
2025-04-03 18:07:40,086 - INFO - Epoch 1, Batch 69: Training loss = 0.3200
2025-04-03 18:07:40,186 - INFO - Epoch 1, Batch 70: Training loss = 0.3100
2025-04-03 18:07:40,287 - INFO - Epoch 1, Batch 71: Training loss = 0.3000
2025-04-03 18:07:40,388 - INFO - Epoch 1, Batch 72: Training loss = 0.2900
2025-04-03 18:07:40,488 - INFO - Epoch 1, Batch 73: Training loss = 0.2800
2025-04-03 18:07:40,589 - INFO - Epoch 1, Batch 74: Training loss = 0.2700
2025-04-03 18:07:40,689 - INFO - Epoch 1, Batch 75: Training loss = 0.2600
2025-04-03 18:07:40,790 - INFO - Epoch 1, Batch 76: Training loss = 0.2500
2025-04-03 18:07:40,891 - INFO - Epoch 1, Batch 77: Training loss = 0.2400
2025-04-03 18:07:40,991 - INFO - Epoch 1, Batch 78: Training loss = 0.2300
2025-04-03 18:07:41,092 - INFO - Epoch 1, Batch 79: Training loss = 0.2200
2025-04-03 18:07:41,192 - INFO - Epoch 1, Batch 80: Training loss = 0.2100
2025-04-03 18:07:41,293 - INFO - Epoch 1, Batch 81: Training loss = 0.2000
2025-04-03 18:07:41,394 - INFO - Epoch 1, Batch 82: Training loss = 0.1900
2025-04-03 18:07:41,494 - INFO - Epoch 1, Batch 83: Training loss = 0.1800
2025-04-03 18:07:41,595 - INFO - Epoch 1, Batch 84: Training loss = 0.1700
2025-04-03 18:07:41,695 - INFO - Epoch 1, Batch 85: Training loss = 0.1600
2025-04-03 18:07:41,796 - INFO - Epoch 1, Batch 86: Training loss = 0.1500
2025-04-03 18:07:41,897 - INFO - Epoch 1, Batch 87: Training loss = 0.1400
2025-04-03 18:07:41,998 - INFO - Epoch 1, Batch 88: Training loss = 0.1300
2025-04-03 18:07:42,099 - INFO - Epoch 1, Batch 89: Training loss = 0.1200
2025-04-03 18:07:42,200 - INFO - Epoch 1, Batch 90: Training loss = 0.1100
2025-04-03 18:07:42,301 - INFO - Epoch 1, Batch 91: Training loss = 0.1000
2025-04-03 18:07:42,401 - INFO - Epoch 1, Batch 92: Training loss = 0.0900
2025-04-03 18:07:42,502 - INFO - Epoch 1, Batch 93: Training loss = 0.0800
2025-04-03 18:07:42,603 - INFO - Epoch 1, Batch 94: Training loss = 0.0700
2025-04-03 18:07:42,703 - INFO - Epoch 1, Batch 95: Training loss = 0.0600
2025-04-03 18:07:42,804 - INFO - Epoch 1, Batch 96: Training loss = 0.0500
2025-04-03 18:07:42,904 - INFO - Epoch 1, Batch 97: Training loss = 0.0400
2025-04-03 18:07:43,005 - INFO - Epoch 1, Batch 98: Training loss = 0.0300
2025-04-03 18:07:43,105 - INFO - Epoch 1, Batch 99: Training loss = 0.0200
2025-04-03 18:07:43,206 - INFO - Epoch 1, Batch 100: Training loss = 0.0100
2025-04-03 18:07:43,306 - INFO - Epoch 1: Validation loss = 0.1000, Validation accuracy = 0.8000
2025-04-03 18:07:43,306 - INFO - Epoch 1: MSE = 0.1000, RMSE = 0.3162, MAE = 0.0800, RÂ² = 0.9000
2025-04-03 18:07:43,307 - INFO - Epoch 2 started.
2025-04-03 18:07:43,307 - INFO - Epoch 2, Batch 1: Training loss = 1.0000
2025-04-03 18:07:43,407 - INFO - Epoch 2, Batch 2: Training loss = 0.9900
2025-04-03 18:07:43,508 - INFO - Epoch 2, Batch 3: Training loss = 0.9800
2025-04-03 18:07:43,609 - INFO - Epoch 2, Batch 4: Training loss = 0.9700
2025-04-03 18:07:43,709 - INFO - Epoch 2, Batch 5: Training loss = 0.9600
2025-04-03 18:07:43,810 - INFO - Epoch 2, Batch 6: Training loss = 0.9500
2025-04-03 18:07:43,911 - INFO - Epoch 2, Batch 7: Training loss = 0.9400
2025-04-03 18:07:44,012 - INFO - Epoch 2, Batch 8: Training loss = 0.9300
2025-04-03 18:07:44,112 - INFO - Epoch 2, Batch 9: Training loss = 0.9200
2025-04-03 18:07:44,213 - INFO - Epoch 2, Batch 10: Training loss = 0.9100
2025-04-03 18:07:44,313 - INFO - Epoch 2, Batch 11: Training loss = 0.9000
2025-04-03 18:07:44,414 - INFO - Epoch 2, Batch 12: Training loss = 0.8900
2025-04-03 18:07:44,514 - INFO - Epoch 2, Batch 13: Training loss = 0.8800
2025-04-03 18:07:44,615 - INFO - Epoch 2, Batch 14: Training loss = 0.8700
2025-04-03 18:07:44,715 - INFO - Epoch 2, Batch 15: Training loss = 0.8600
2025-04-03 18:07:44,816 - INFO - Epoch 2, Batch 16: Training loss = 0.8500
2025-04-03 18:07:44,916 - INFO - Epoch 2, Batch 17: Training loss = 0.8400
2025-04-03 18:07:45,017 - INFO - Epoch 2, Batch 18: Training loss = 0.8300
2025-04-03 18:07:45,118 - INFO - Epoch 2, Batch 19: Training loss = 0.8200
2025-04-03 18:07:45,218 - INFO - Epoch 2, Batch 20: Training loss = 0.8100
2025-04-03 18:07:45,319 - INFO - Epoch 2, Batch 21: Training loss = 0.8000
2025-04-03 18:07:45,419 - INFO - Epoch 2, Batch 22: Training loss = 0.7900
2025-04-03 18:07:45,520 - INFO - Epoch 2, Batch 23: Training loss = 0.7800
2025-04-03 18:07:45,621 - INFO - Epoch 2, Batch 24: Training loss = 0.7700
2025-04-03 18:07:45,722 - INFO - Epoch 2, Batch 25: Training loss = 0.7600
2025-04-03 18:07:45,823 - INFO - Epoch 2, Batch 26: Training loss = 0.7500
2025-04-03 18:07:45,924 - INFO - Epoch 2, Batch 27: Training loss = 0.7400
2025-04-03 18:07:46,024 - INFO - Epoch 2, Batch 28: Training loss = 0.7300
2025-04-03 18:07:46,125 - INFO - Epoch 2, Batch 29: Training loss = 0.7200
2025-04-03 18:07:46,226 - INFO - Epoch 2, Batch 30: Training loss = 0.7100
2025-04-03 18:07:46,326 - INFO - Epoch 2, Batch 31: Training loss = 0.7000
2025-04-03 18:07:46,427 - INFO - Epoch 2, Batch 32: Training loss = 0.6900
2025-04-03 18:07:46,527 - INFO - Epoch 2, Batch 33: Training loss = 0.6800
2025-04-03 18:07:46,627 - INFO - Epoch 2, Batch 34: Training loss = 0.6700
2025-04-03 18:07:46,728 - INFO - Epoch 2, Batch 35: Training loss = 0.6600
2025-04-03 18:07:46,828 - INFO - Epoch 2, Batch 36: Training loss = 0.6500
2025-04-03 18:07:46,929 - INFO - Epoch 2, Batch 37: Training loss = 0.6400
2025-04-03 18:07:47,030 - INFO - Epoch 2, Batch 38: Training loss = 0.6300
2025-04-03 18:07:47,130 - INFO - Epoch 2, Batch 39: Training loss = 0.6200
2025-04-03 18:07:47,231 - INFO - Epoch 2, Batch 40: Training loss = 0.6100
2025-04-03 18:07:47,331 - INFO - Epoch 2, Batch 41: Training loss = 0.6000
2025-04-03 18:07:47,431 - INFO - Epoch 2, Batch 42: Training loss = 0.5900
2025-04-03 18:07:47,532 - INFO - Epoch 2, Batch 43: Training loss = 0.5800
2025-04-03 18:07:47,633 - INFO - Epoch 2, Batch 44: Training loss = 0.5700
2025-04-03 18:07:47,734 - INFO - Epoch 2, Batch 45: Training loss = 0.5600
2025-04-03 18:07:47,834 - INFO - Epoch 2, Batch 46: Training loss = 0.5500
2025-04-03 18:07:47,935 - INFO - Epoch 2, Batch 47: Training loss = 0.5400
2025-04-03 18:07:48,036 - INFO - Epoch 2, Batch 48: Training loss = 0.5300
2025-04-03 18:07:48,136 - INFO - Epoch 2, Batch 49: Training loss = 0.5200
2025-04-03 18:07:48,236 - INFO - Epoch 2, Batch 50: Training loss = 0.5100
2025-04-03 18:07:48,337 - INFO - Epoch 2, Batch 51: Training loss = 0.5000
2025-04-03 18:07:48,438 - INFO - Epoch 2, Batch 52: Training loss = 0.4900
2025-04-03 18:07:48,539 - INFO - Epoch 2, Batch 53: Training loss = 0.4800
2025-04-03 18:07:48,639 - INFO - Epoch 2, Batch 54: Training loss = 0.4700
2025-04-03 18:07:48,740 - INFO - Epoch 2, Batch 55: Training loss = 0.4600
2025-04-03 18:07:48,840 - INFO - Epoch 2, Batch 56: Training loss = 0.4500
2025-04-03 18:07:48,941 - INFO - Epoch 2, Batch 57: Training loss = 0.4400
2025-04-03 18:07:49,041 - INFO - Epoch 2, Batch 58: Training loss = 0.4300
2025-04-03 18:07:49,141 - INFO - Epoch 2, Batch 59: Training loss = 0.4200
2025-04-03 18:07:49,242 - INFO - Epoch 2, Batch 60: Training loss = 0.4100
2025-04-03 18:07:49,343 - INFO - Epoch 2, Batch 61: Training loss = 0.4000
2025-04-03 18:07:49,443 - INFO - Epoch 2, Batch 62: Training loss = 0.3900
2025-04-03 18:07:49,544 - INFO - Epoch 2, Batch 63: Training loss = 0.3800
2025-04-03 18:07:49,644 - INFO - Epoch 2, Batch 64: Training loss = 0.3700
2025-04-03 18:07:49,745 - INFO - Epoch 2, Batch 65: Training loss = 0.3600
2025-04-03 18:07:49,846 - INFO - Epoch 2, Batch 66: Training loss = 0.3500
2025-04-03 18:07:49,946 - INFO - Epoch 2, Batch 67: Training loss = 0.3400
2025-04-03 18:07:50,047 - INFO - Epoch 2, Batch 68: Training loss = 0.3300
2025-04-03 18:07:50,147 - INFO - Epoch 2, Batch 69: Training loss = 0.3200
2025-04-03 18:07:50,248 - INFO - Epoch 2, Batch 70: Training loss = 0.3100
2025-04-03 18:07:50,348 - INFO - Epoch 2, Batch 71: Training loss = 0.3000
2025-04-03 18:07:50,449 - INFO - Epoch 2, Batch 72: Training loss = 0.2900
2025-04-03 18:07:50,549 - INFO - Epoch 2, Batch 73: Training loss = 0.2800
2025-04-03 18:07:50,650 - INFO - Epoch 2, Batch 74: Training loss = 0.2700
2025-04-03 18:07:50,751 - INFO - Epoch 2, Batch 75: Training loss = 0.2600
2025-04-03 18:07:50,851 - INFO - Epoch 2, Batch 76: Training loss = 0.2500
2025-04-03 18:07:50,952 - INFO - Epoch 2, Batch 77: Training loss = 0.2400
2025-04-03 18:07:51,052 - INFO - Epoch 2, Batch 78: Training loss = 0.2300
2025-04-03 18:07:51,153 - INFO - Epoch 2, Batch 79: Training loss = 0.2200
2025-04-03 18:07:51,253 - INFO - Epoch 2, Batch 80: Training loss = 0.2100
2025-04-03 18:07:51,354 - INFO - Epoch 2, Batch 81: Training loss = 0.2000
2025-04-03 18:07:51,454 - INFO - Epoch 2, Batch 82: Training loss = 0.1900
2025-04-03 18:07:51,555 - INFO - Epoch 2, Batch 83: Training loss = 0.1800
2025-04-03 18:07:51,656 - INFO - Epoch 2, Batch 84: Training loss = 0.1700
2025-04-03 18:07:51,757 - INFO - Epoch 2, Batch 85: Training loss = 0.1600
2025-04-03 18:07:51,858 - INFO - Epoch 2, Batch 86: Training loss = 0.1500
2025-04-03 18:07:51,958 - INFO - Epoch 2, Batch 87: Training loss = 0.1400
2025-04-03 18:07:52,058 - INFO - Epoch 2, Batch 88: Training loss = 0.1300
2025-04-03 18:07:52,159 - INFO - Epoch 2, Batch 89: Training loss = 0.1200
2025-04-03 18:07:52,259 - INFO - Epoch 2, Batch 90: Training loss = 0.1100
2025-04-03 18:07:52,360 - INFO - Epoch 2, Batch 91: Training loss = 0.1000
2025-04-03 18:07:52,461 - INFO - Epoch 2, Batch 92: Training loss = 0.0900
2025-04-03 18:07:52,561 - INFO - Epoch 2, Batch 93: Training loss = 0.0800
2025-04-03 18:07:52,662 - INFO - Epoch 2, Batch 94: Training loss = 0.0700
2025-04-03 18:07:52,763 - INFO - Epoch 2, Batch 95: Training loss = 0.0600
2025-04-03 18:07:52,863 - INFO - Epoch 2, Batch 96: Training loss = 0.0500
2025-04-03 18:07:52,964 - INFO - Epoch 2, Batch 97: Training loss = 0.0400
2025-04-03 18:07:53,065 - INFO - Epoch 2, Batch 98: Training loss = 0.0300
2025-04-03 18:07:53,165 - INFO - Epoch 2, Batch 99: Training loss = 0.0200
2025-04-03 18:07:53,266 - INFO - Epoch 2, Batch 100: Training loss = 0.0100
2025-04-03 18:07:53,366 - INFO - Epoch 2: Validation loss = 0.0900, Validation accuracy = 0.8200
2025-04-03 18:07:53,367 - INFO - Epoch 2: MSE = 0.0900, RMSE = 0.3000, MAE = 0.0720, RÂ² = 0.9100
2025-04-03 18:07:53,367 - INFO - Epoch 3 started.
2025-04-03 18:07:53,367 - INFO - Epoch 3, Batch 1: Training loss = 1.0000
2025-04-03 18:07:53,467 - INFO - Epoch 3, Batch 2: Training loss = 0.9900
2025-04-03 18:07:53,568 - INFO - Epoch 3, Batch 3: Training loss = 0.9800
2025-04-03 18:07:53,668 - INFO - Epoch 3, Batch 4: Training loss = 0.9700
2025-04-03 18:07:53,769 - INFO - Epoch 3, Batch 5: Training loss = 0.9600
2025-04-03 18:07:53,870 - INFO - Epoch 3, Batch 6: Training loss = 0.9500
2025-04-03 18:07:53,970 - INFO - Epoch 3, Batch 7: Training loss = 0.9400
2025-04-03 18:07:54,071 - INFO - Epoch 3, Batch 8: Training loss = 0.9300
2025-04-03 18:07:54,171 - INFO - Epoch 3, Batch 9: Training loss = 0.9200
2025-04-03 18:07:54,272 - INFO - Epoch 3, Batch 10: Training loss = 0.9100
2025-04-03 18:07:54,373 - INFO - Epoch 3, Batch 11: Training loss = 0.9000
2025-04-03 18:07:54,474 - INFO - Epoch 3, Batch 12: Training loss = 0.8900
2025-04-03 18:07:54,575 - INFO - Epoch 3, Batch 13: Training loss = 0.8800
2025-04-03 18:07:54,676 - INFO - Epoch 3, Batch 14: Training loss = 0.8700
2025-04-03 18:07:54,777 - INFO - Epoch 3, Batch 15: Training loss = 0.8600
2025-04-03 18:07:54,878 - INFO - Epoch 3, Batch 16: Training loss = 0.8500
2025-04-03 18:07:54,979 - INFO - Epoch 3, Batch 17: Training loss = 0.8400
2025-04-03 18:07:55,079 - INFO - Epoch 3, Batch 18: Training loss = 0.8300
2025-04-03 18:07:55,180 - INFO - Epoch 3, Batch 19: Training loss = 0.8200
2025-04-03 18:07:55,280 - INFO - Epoch 3, Batch 20: Training loss = 0.8100
2025-04-03 18:07:55,381 - INFO - Epoch 3, Batch 21: Training loss = 0.8000
2025-04-03 18:07:55,482 - INFO - Epoch 3, Batch 22: Training loss = 0.7900
2025-04-03 18:07:55,583 - INFO - Epoch 3, Batch 23: Training loss = 0.7800
2025-04-03 18:07:55,683 - INFO - Epoch 3, Batch 24: Training loss = 0.7700
2025-04-03 18:07:55,784 - INFO - Epoch 3, Batch 25: Training loss = 0.7600
2025-04-03 18:07:55,884 - INFO - Epoch 3, Batch 26: Training loss = 0.7500
2025-04-03 18:07:55,985 - INFO - Epoch 3, Batch 27: Training loss = 0.7400
2025-04-03 18:07:56,085 - INFO - Epoch 3, Batch 28: Training loss = 0.7300
2025-04-03 18:07:56,186 - INFO - Epoch 3, Batch 29: Training loss = 0.7200
2025-04-03 18:07:56,286 - INFO - Epoch 3, Batch 30: Training loss = 0.7100
2025-04-03 18:07:56,387 - INFO - Epoch 3, Batch 31: Training loss = 0.7000
2025-04-03 18:07:56,488 - INFO - Epoch 3, Batch 32: Training loss = 0.6900
2025-04-03 18:07:56,589 - INFO - Epoch 3, Batch 33: Training loss = 0.6800
2025-04-03 18:07:56,689 - INFO - Epoch 3, Batch 34: Training loss = 0.6700
2025-04-03 18:07:56,790 - INFO - Epoch 3, Batch 35: Training loss = 0.6600
2025-04-03 18:07:56,890 - INFO - Epoch 3, Batch 36: Training loss = 0.6500
2025-04-03 18:07:56,991 - INFO - Epoch 3, Batch 37: Training loss = 0.6400
2025-04-03 18:07:57,091 - INFO - Epoch 3, Batch 38: Training loss = 0.6300
2025-04-03 18:07:57,192 - INFO - Epoch 3, Batch 39: Training loss = 0.6200
2025-04-03 18:07:57,292 - INFO - Epoch 3, Batch 40: Training loss = 0.6100
2025-04-03 18:07:57,393 - INFO - Epoch 3, Batch 41: Training loss = 0.6000
2025-04-03 18:07:57,494 - INFO - Epoch 3, Batch 42: Training loss = 0.5900
2025-04-03 18:07:57,595 - INFO - Epoch 3, Batch 43: Training loss = 0.5800
2025-04-03 18:07:57,696 - INFO - Epoch 3, Batch 44: Training loss = 0.5700
2025-04-03 18:07:57,796 - INFO - Epoch 3, Batch 45: Training loss = 0.5600
2025-04-03 18:07:57,897 - INFO - Epoch 3, Batch 46: Training loss = 0.5500
2025-04-03 18:07:57,998 - INFO - Epoch 3, Batch 47: Training loss = 0.5400
2025-04-03 18:07:58,099 - INFO - Epoch 3, Batch 48: Training loss = 0.5300
2025-04-03 18:07:58,199 - INFO - Epoch 3, Batch 49: Training loss = 0.5200
2025-04-03 18:07:58,300 - INFO - Epoch 3, Batch 50: Training loss = 0.5100
2025-04-03 18:07:58,401 - INFO - Epoch 3, Batch 51: Training loss = 0.5000
2025-04-03 18:07:58,502 - INFO - Epoch 3, Batch 52: Training loss = 0.4900
2025-04-03 18:07:58,603 - INFO - Epoch 3, Batch 53: Training loss = 0.4800
2025-04-03 18:07:58,703 - INFO - Epoch 3, Batch 54: Training loss = 0.4700
2025-04-03 18:07:58,804 - INFO - Epoch 3, Batch 55: Training loss = 0.4600
2025-04-03 18:07:58,905 - INFO - Epoch 3, Batch 56: Training loss = 0.4500
2025-04-03 18:07:59,005 - INFO - Epoch 3, Batch 57: Training loss = 0.4400
2025-04-03 18:07:59,106 - INFO - Epoch 3, Batch 58: Training loss = 0.4300
2025-04-03 18:07:59,207 - INFO - Epoch 3, Batch 59: Training loss = 0.4200
2025-04-03 18:07:59,307 - INFO - Epoch 3, Batch 60: Training loss = 0.4100
2025-04-03 18:07:59,408 - INFO - Epoch 3, Batch 61: Training loss = 0.4000
2025-04-03 18:07:59,509 - INFO - Epoch 3, Batch 62: Training loss = 0.3900
2025-04-03 18:07:59,610 - INFO - Epoch 3, Batch 63: Training loss = 0.3800
2025-04-03 18:07:59,711 - INFO - Epoch 3, Batch 64: Training loss = 0.3700
2025-04-03 18:07:59,812 - INFO - Epoch 3, Batch 65: Training loss = 0.3600
2025-04-03 18:07:59,914 - INFO - Epoch 3, Batch 66: Training loss = 0.3500
2025-04-03 18:08:00,015 - INFO - Epoch 3, Batch 67: Training loss = 0.3400
2025-04-03 18:08:00,115 - INFO - Epoch 3, Batch 68: Training loss = 0.3300
2025-04-03 18:08:00,216 - INFO - Epoch 3, Batch 69: Training loss = 0.3200
2025-04-03 18:08:00,317 - INFO - Epoch 3, Batch 70: Training loss = 0.3100
2025-04-03 18:08:00,417 - INFO - Epoch 3, Batch 71: Training loss = 0.3000
2025-04-03 18:08:00,517 - INFO - Epoch 3, Batch 72: Training loss = 0.2900
2025-04-03 18:08:00,618 - INFO - Epoch 3, Batch 73: Training loss = 0.2800
2025-04-03 18:08:00,719 - INFO - Epoch 3, Batch 74: Training loss = 0.2700
2025-04-03 18:08:00,819 - INFO - Epoch 3, Batch 75: Training loss = 0.2600
2025-04-03 18:08:00,920 - INFO - Epoch 3, Batch 76: Training loss = 0.2500
2025-04-03 18:08:01,020 - INFO - Epoch 3, Batch 77: Training loss = 0.2400
2025-04-03 18:08:01,121 - INFO - Epoch 3, Batch 78: Training loss = 0.2300
2025-04-03 18:08:01,221 - INFO - Epoch 3, Batch 79: Training loss = 0.2200
2025-04-03 18:08:01,322 - INFO - Epoch 3, Batch 80: Training loss = 0.2100
2025-04-03 18:08:01,423 - INFO - Epoch 3, Batch 81: Training loss = 0.2000
2025-04-03 18:08:01,524 - INFO - Epoch 3, Batch 82: Training loss = 0.1900
2025-04-03 18:08:01,624 - INFO - Epoch 3, Batch 83: Training loss = 0.1800
2025-04-03 18:08:01,725 - INFO - Epoch 3, Batch 84: Training loss = 0.1700
2025-04-03 18:08:01,825 - INFO - Epoch 3, Batch 85: Training loss = 0.1600
2025-04-03 18:08:01,926 - INFO - Epoch 3, Batch 86: Training loss = 0.1500
2025-04-03 18:08:02,027 - INFO - Epoch 3, Batch 87: Training loss = 0.1400
2025-04-03 18:08:02,128 - INFO - Epoch 3, Batch 88: Training loss = 0.1300
2025-04-03 18:08:02,228 - INFO - Epoch 3, Batch 89: Training loss = 0.1200
2025-04-03 18:08:02,329 - INFO - Epoch 3, Batch 90: Training loss = 0.1100
2025-04-03 18:08:02,429 - INFO - Epoch 3, Batch 91: Training loss = 0.1000
2025-04-03 18:08:02,530 - INFO - Epoch 3, Batch 92: Training loss = 0.0900
2025-04-03 18:08:02,631 - INFO - Epoch 3, Batch 93: Training loss = 0.0800
2025-04-03 18:08:02,731 - INFO - Epoch 3, Batch 94: Training loss = 0.0700
2025-04-03 18:08:02,832 - INFO - Epoch 3, Batch 95: Training loss = 0.0600
2025-04-03 18:08:02,932 - INFO - Epoch 3, Batch 96: Training loss = 0.0500
2025-04-03 18:08:03,033 - INFO - Epoch 3, Batch 97: Training loss = 0.0400
2025-04-03 18:08:03,133 - INFO - Epoch 3, Batch 98: Training loss = 0.0300
2025-04-03 18:08:03,234 - INFO - Epoch 3, Batch 99: Training loss = 0.0200
2025-04-03 18:08:03,334 - INFO - Epoch 3, Batch 100: Training loss = 0.0100
2025-04-03 18:08:03,435 - INFO - Epoch 3: Validation loss = 0.0800, Validation accuracy = 0.8400
2025-04-03 18:08:03,435 - INFO - Epoch 3: MSE = 0.0800, RMSE = 0.2828, MAE = 0.0640, RÂ² = 0.9200
2025-04-03 18:08:03,435 - INFO - Epoch 4 started.
2025-04-03 18:08:03,435 - INFO - Epoch 4, Batch 1: Training loss = 1.0000
2025-04-03 18:08:03,536 - INFO - Epoch 4, Batch 2: Training loss = 0.9900
2025-04-03 18:08:03,636 - INFO - Epoch 4, Batch 3: Training loss = 0.9800
2025-04-03 18:08:03,737 - INFO - Epoch 4, Batch 4: Training loss = 0.9700
2025-04-03 18:08:03,838 - INFO - Epoch 4, Batch 5: Training loss = 0.9600
2025-04-03 18:08:03,939 - INFO - Epoch 4, Batch 6: Training loss = 0.9500
2025-04-03 18:08:04,040 - INFO - Epoch 4, Batch 7: Training loss = 0.9400
2025-04-03 18:08:04,140 - INFO - Epoch 4, Batch 8: Training loss = 0.9300
2025-04-03 18:08:04,240 - INFO - Epoch 4, Batch 9: Training loss = 0.9200
2025-04-03 18:08:04,341 - INFO - Epoch 4, Batch 10: Training loss = 0.9100
2025-04-03 18:08:04,441 - INFO - Epoch 4, Batch 11: Training loss = 0.9000
2025-04-03 18:08:04,542 - INFO - Epoch 4, Batch 12: Training loss = 0.8900
2025-04-03 18:08:04,643 - INFO - Epoch 4, Batch 13: Training loss = 0.8800
2025-04-03 18:08:04,743 - INFO - Epoch 4, Batch 14: Training loss = 0.8700
2025-04-03 18:08:04,843 - INFO - Epoch 4, Batch 15: Training loss = 0.8600
2025-04-03 18:08:04,944 - INFO - Epoch 4, Batch 16: Training loss = 0.8500
2025-04-03 18:08:05,045 - INFO - Epoch 4, Batch 17: Training loss = 0.8400
2025-04-03 18:08:05,146 - INFO - Epoch 4, Batch 18: Training loss = 0.8300
2025-04-03 18:08:05,247 - INFO - Epoch 4, Batch 19: Training loss = 0.8200
2025-04-03 18:08:05,347 - INFO - Epoch 4, Batch 20: Training loss = 0.8100
2025-04-03 18:08:05,448 - INFO - Epoch 4, Batch 21: Training loss = 0.8000
2025-04-03 18:08:05,548 - INFO - Epoch 4, Batch 22: Training loss = 0.7900
2025-04-03 18:08:05,649 - INFO - Epoch 4, Batch 23: Training loss = 0.7800
2025-04-03 18:08:05,750 - INFO - Epoch 4, Batch 24: Training loss = 0.7700
2025-04-03 18:08:05,851 - INFO - Epoch 4, Batch 25: Training loss = 0.7600
2025-04-03 18:08:05,951 - INFO - Epoch 4, Batch 26: Training loss = 0.7500
2025-04-03 18:08:06,052 - INFO - Epoch 4, Batch 27: Training loss = 0.7400
2025-04-03 18:08:06,153 - INFO - Epoch 4, Batch 28: Training loss = 0.7300
2025-04-03 18:08:06,254 - INFO - Epoch 4, Batch 29: Training loss = 0.7200
2025-04-03 18:08:06,355 - INFO - Epoch 4, Batch 30: Training loss = 0.7100
2025-04-03 18:08:06,456 - INFO - Epoch 4, Batch 31: Training loss = 0.7000
2025-04-03 18:08:06,556 - INFO - Epoch 4, Batch 32: Training loss = 0.6900
2025-04-03 18:08:06,657 - INFO - Epoch 4, Batch 33: Training loss = 0.6800
2025-04-03 18:08:06,758 - INFO - Epoch 4, Batch 34: Training loss = 0.6700
2025-04-03 18:08:06,858 - INFO - Epoch 4, Batch 35: Training loss = 0.6600
2025-04-03 18:08:06,959 - INFO - Epoch 4, Batch 36: Training loss = 0.6500
2025-04-03 18:08:07,060 - INFO - Epoch 4, Batch 37: Training loss = 0.6400
2025-04-03 18:08:07,161 - INFO - Epoch 4, Batch 38: Training loss = 0.6300
2025-04-03 18:08:07,262 - INFO - Epoch 4, Batch 39: Training loss = 0.6200
2025-04-03 18:08:07,363 - INFO - Epoch 4, Batch 40: Training loss = 0.6100
2025-04-03 18:08:07,463 - INFO - Epoch 4, Batch 41: Training loss = 0.6000
2025-04-03 18:08:07,563 - INFO - Epoch 4, Batch 42: Training loss = 0.5900
2025-04-03 18:08:07,664 - INFO - Epoch 4, Batch 43: Training loss = 0.5800
2025-04-03 18:08:07,765 - INFO - Epoch 4, Batch 44: Training loss = 0.5700
2025-04-03 18:08:07,866 - INFO - Epoch 4, Batch 45: Training loss = 0.5600
2025-04-03 18:08:07,967 - INFO - Epoch 4, Batch 46: Training loss = 0.5500
2025-04-03 18:08:08,067 - INFO - Epoch 4, Batch 47: Training loss = 0.5400
2025-04-03 18:08:08,167 - INFO - Epoch 4, Batch 48: Training loss = 0.5300
2025-04-03 18:08:08,268 - INFO - Epoch 4, Batch 49: Training loss = 0.5200
2025-04-03 18:08:08,368 - INFO - Epoch 4, Batch 50: Training loss = 0.5100
2025-04-03 18:08:08,469 - INFO - Epoch 4, Batch 51: Training loss = 0.5000
2025-04-03 18:08:08,569 - INFO - Epoch 4, Batch 52: Training loss = 0.4900
2025-04-03 18:08:08,670 - INFO - Epoch 4, Batch 53: Training loss = 0.4800
2025-04-03 18:08:08,771 - INFO - Epoch 4, Batch 54: Training loss = 0.4700
2025-04-03 18:08:08,872 - INFO - Epoch 4, Batch 55: Training loss = 0.4600
2025-04-03 18:08:08,973 - INFO - Epoch 4, Batch 56: Training loss = 0.4500
2025-04-03 18:08:09,073 - INFO - Epoch 4, Batch 57: Training loss = 0.4400
2025-04-03 18:08:09,174 - INFO - Epoch 4, Batch 58: Training loss = 0.4300
2025-04-03 18:08:09,274 - INFO - Epoch 4, Batch 59: Training loss = 0.4200
2025-04-03 18:08:09,375 - INFO - Epoch 4, Batch 60: Training loss = 0.4100
2025-04-03 18:08:09,476 - INFO - Epoch 4, Batch 61: Training loss = 0.4000
2025-04-03 18:08:09,577 - INFO - Epoch 4, Batch 62: Training loss = 0.3900
2025-04-03 18:08:09,677 - INFO - Epoch 4, Batch 63: Training loss = 0.3800
2025-04-03 18:08:09,778 - INFO - Epoch 4, Batch 64: Training loss = 0.3700
2025-04-03 18:08:09,879 - INFO - Epoch 4, Batch 65: Training loss = 0.3600
2025-04-03 18:08:09,980 - INFO - Epoch 4, Batch 66: Training loss = 0.3500
2025-04-03 18:08:10,080 - INFO - Epoch 4, Batch 67: Training loss = 0.3400
2025-04-03 18:08:10,181 - INFO - Epoch 4, Batch 68: Training loss = 0.3300
2025-04-03 18:08:10,282 - INFO - Epoch 4, Batch 69: Training loss = 0.3200
2025-04-03 18:08:10,382 - INFO - Epoch 4, Batch 70: Training loss = 0.3100
2025-04-03 18:08:10,483 - INFO - Epoch 4, Batch 71: Training loss = 0.3000
2025-04-03 18:08:10,583 - INFO - Epoch 4, Batch 72: Training loss = 0.2900
2025-04-03 18:08:10,684 - INFO - Epoch 4, Batch 73: Training loss = 0.2800
2025-04-03 18:08:10,785 - INFO - Epoch 4, Batch 74: Training loss = 0.2700
2025-04-03 18:08:10,886 - INFO - Epoch 4, Batch 75: Training loss = 0.2600
2025-04-03 18:08:10,986 - INFO - Epoch 4, Batch 76: Training loss = 0.2500
2025-04-03 18:08:11,087 - INFO - Epoch 4, Batch 77: Training loss = 0.2400
2025-04-03 18:08:11,188 - INFO - Epoch 4, Batch 78: Training loss = 0.2300
2025-04-03 18:08:11,288 - INFO - Epoch 4, Batch 79: Training loss = 0.2200
2025-04-03 18:08:11,389 - INFO - Epoch 4, Batch 80: Training loss = 0.2100
2025-04-03 18:08:11,489 - INFO - Epoch 4, Batch 81: Training loss = 0.2000
2025-04-03 18:08:11,590 - INFO - Epoch 4, Batch 82: Training loss = 0.1900
2025-04-03 18:08:11,691 - INFO - Epoch 4, Batch 83: Training loss = 0.1800
2025-04-03 18:08:11,792 - INFO - Epoch 4, Batch 84: Training loss = 0.1700
2025-04-03 18:08:11,893 - INFO - Epoch 4, Batch 85: Training loss = 0.1600
2025-04-03 18:08:11,994 - INFO - Epoch 4, Batch 86: Training loss = 0.1500
2025-04-03 18:08:12,094 - INFO - Epoch 4, Batch 87: Training loss = 0.1400
2025-04-03 18:08:12,195 - INFO - Epoch 4, Batch 88: Training loss = 0.1300
2025-04-03 18:08:12,295 - INFO - Epoch 4, Batch 89: Training loss = 0.1200
2025-04-03 18:08:12,396 - INFO - Epoch 4, Batch 90: Training loss = 0.1100
2025-04-03 18:08:12,497 - INFO - Epoch 4, Batch 91: Training loss = 0.1000
2025-04-03 18:08:12,598 - INFO - Epoch 4, Batch 92: Training loss = 0.0900
2025-04-03 18:08:12,699 - INFO - Epoch 4, Batch 93: Training loss = 0.0800
2025-04-03 18:08:12,799 - INFO - Epoch 4, Batch 94: Training loss = 0.0700
2025-04-03 18:08:12,900 - INFO - Epoch 4, Batch 95: Training loss = 0.0600
2025-04-03 18:08:13,000 - INFO - Epoch 4, Batch 96: Training loss = 0.0500
2025-04-03 18:08:13,101 - INFO - Epoch 4, Batch 97: Training loss = 0.0400
2025-04-03 18:08:13,201 - INFO - Epoch 4, Batch 98: Training loss = 0.0300
2025-04-03 18:08:13,302 - INFO - Epoch 4, Batch 99: Training loss = 0.0200
2025-04-03 18:08:13,403 - INFO - Epoch 4, Batch 100: Training loss = 0.0100
2025-04-03 18:08:13,503 - INFO - Epoch 4: Validation loss = 0.0700, Validation accuracy = 0.8600
2025-04-03 18:08:13,504 - INFO - Epoch 4: MSE = 0.0700, RMSE = 0.2646, MAE = 0.0560, RÂ² = 0.9300
2025-04-03 18:08:13,505 - INFO - Epoch 5 started.
2025-04-03 18:08:13,505 - INFO - Epoch 5, Batch 1: Training loss = 1.0000
2025-04-03 18:08:13,605 - INFO - Epoch 5, Batch 2: Training loss = 0.9900
2025-04-03 18:08:13,706 - INFO - Epoch 5, Batch 3: Training loss = 0.9800
2025-04-03 18:08:13,806 - INFO - Epoch 5, Batch 4: Training loss = 0.9700
2025-04-03 18:08:13,907 - INFO - Epoch 5, Batch 5: Training loss = 0.9600
2025-04-03 18:08:14,008 - INFO - Epoch 5, Batch 6: Training loss = 0.9500
2025-04-03 18:08:14,109 - INFO - Epoch 5, Batch 7: Training loss = 0.9400
2025-04-03 18:08:14,210 - INFO - Epoch 5, Batch 8: Training loss = 0.9300
2025-04-03 18:08:14,311 - INFO - Epoch 5, Batch 9: Training loss = 0.9200
2025-04-03 18:08:14,412 - INFO - Epoch 5, Batch 10: Training loss = 0.9100
2025-04-03 18:08:14,513 - INFO - Epoch 5, Batch 11: Training loss = 0.9000
2025-04-03 18:08:14,614 - INFO - Epoch 5, Batch 12: Training loss = 0.8900
2025-04-03 18:08:14,714 - INFO - Epoch 5, Batch 13: Training loss = 0.8800
2025-04-03 18:08:14,815 - INFO - Epoch 5, Batch 14: Training loss = 0.8700
2025-04-03 18:08:14,916 - INFO - Epoch 5, Batch 15: Training loss = 0.8600
2025-04-03 18:08:15,016 - INFO - Epoch 5, Batch 16: Training loss = 0.8500
2025-04-03 18:08:15,117 - INFO - Epoch 5, Batch 17: Training loss = 0.8400
2025-04-03 18:08:15,218 - INFO - Epoch 5, Batch 18: Training loss = 0.8300
2025-04-03 18:08:15,319 - INFO - Epoch 5, Batch 19: Training loss = 0.8200
2025-04-03 18:08:15,420 - INFO - Epoch 5, Batch 20: Training loss = 0.8100
2025-04-03 18:08:15,520 - INFO - Epoch 5, Batch 21: Training loss = 0.8000
2025-04-03 18:08:15,621 - INFO - Epoch 5, Batch 22: Training loss = 0.7900
2025-04-03 18:08:15,722 - INFO - Epoch 5, Batch 23: Training loss = 0.7800
2025-04-03 18:08:15,823 - INFO - Epoch 5, Batch 24: Training loss = 0.7700
2025-04-03 18:08:15,923 - INFO - Epoch 5, Batch 25: Training loss = 0.7600
2025-04-03 18:08:16,024 - INFO - Epoch 5, Batch 26: Training loss = 0.7500
2025-04-03 18:08:16,125 - INFO - Epoch 5, Batch 27: Training loss = 0.7400
2025-04-03 18:08:16,225 - INFO - Epoch 5, Batch 28: Training loss = 0.7300
2025-04-03 18:08:16,326 - INFO - Epoch 5, Batch 29: Training loss = 0.7200
2025-04-03 18:08:16,427 - INFO - Epoch 5, Batch 30: Training loss = 0.7100
2025-04-03 18:08:16,528 - INFO - Epoch 5, Batch 31: Training loss = 0.7000
2025-04-03 18:08:16,628 - INFO - Epoch 5, Batch 32: Training loss = 0.6900
2025-04-03 18:08:16,729 - INFO - Epoch 5, Batch 33: Training loss = 0.6800
2025-04-03 18:08:16,830 - INFO - Epoch 5, Batch 34: Training loss = 0.6700
2025-04-03 18:08:16,930 - INFO - Epoch 5, Batch 35: Training loss = 0.6600
2025-04-03 18:08:17,031 - INFO - Epoch 5, Batch 36: Training loss = 0.6500
2025-04-03 18:08:17,132 - INFO - Epoch 5, Batch 37: Training loss = 0.6400
2025-04-03 18:08:17,232 - INFO - Epoch 5, Batch 38: Training loss = 0.6300
2025-04-03 18:08:17,333 - INFO - Epoch 5, Batch 39: Training loss = 0.6200
2025-04-03 18:08:17,434 - INFO - Epoch 5, Batch 40: Training loss = 0.6100
2025-04-03 18:08:17,535 - INFO - Epoch 5, Batch 41: Training loss = 0.6000
2025-04-03 18:08:17,635 - INFO - Epoch 5, Batch 42: Training loss = 0.5900
2025-04-03 18:08:17,736 - INFO - Epoch 5, Batch 43: Training loss = 0.5800
2025-04-03 18:08:17,837 - INFO - Epoch 5, Batch 44: Training loss = 0.5700
2025-04-03 18:08:17,937 - INFO - Epoch 5, Batch 45: Training loss = 0.5600
2025-04-03 18:08:18,038 - INFO - Epoch 5, Batch 46: Training loss = 0.5500
2025-04-03 18:08:18,139 - INFO - Epoch 5, Batch 47: Training loss = 0.5400
2025-04-03 18:08:18,239 - INFO - Epoch 5, Batch 48: Training loss = 0.5300
2025-04-03 18:08:18,340 - INFO - Epoch 5, Batch 49: Training loss = 0.5200
2025-04-03 18:08:18,441 - INFO - Epoch 5, Batch 50: Training loss = 0.5100
2025-04-03 18:08:18,542 - INFO - Epoch 5, Batch 51: Training loss = 0.5000
2025-04-03 18:08:18,643 - INFO - Epoch 5, Batch 52: Training loss = 0.4900
2025-04-03 18:08:18,745 - INFO - Epoch 5, Batch 53: Training loss = 0.4800
2025-04-03 18:08:18,846 - INFO - Epoch 5, Batch 54: Training loss = 0.4700
2025-04-03 18:08:18,946 - INFO - Epoch 5, Batch 55: Training loss = 0.4600
2025-04-03 18:08:19,047 - INFO - Epoch 5, Batch 56: Training loss = 0.4500
2025-04-03 18:08:19,148 - INFO - Epoch 5, Batch 57: Training loss = 0.4400
2025-04-03 18:08:19,248 - INFO - Epoch 5, Batch 58: Training loss = 0.4300
2025-04-03 18:08:19,349 - INFO - Epoch 5, Batch 59: Training loss = 0.4200
2025-04-03 18:08:19,450 - INFO - Epoch 5, Batch 60: Training loss = 0.4100
2025-04-03 18:08:19,551 - INFO - Epoch 5, Batch 61: Training loss = 0.4000
2025-04-03 18:08:19,651 - INFO - Epoch 5, Batch 62: Training loss = 0.3900
2025-04-03 18:08:19,752 - INFO - Epoch 5, Batch 63: Training loss = 0.3800
2025-04-03 18:08:19,853 - INFO - Epoch 5, Batch 64: Training loss = 0.3700
2025-04-03 18:08:19,953 - INFO - Epoch 5, Batch 65: Training loss = 0.3600
2025-04-03 18:08:20,054 - INFO - Epoch 5, Batch 66: Training loss = 0.3500
2025-04-03 18:08:20,155 - INFO - Epoch 5, Batch 67: Training loss = 0.3400
2025-04-03 18:08:20,256 - INFO - Epoch 5, Batch 68: Training loss = 0.3300
2025-04-03 18:08:20,356 - INFO - Epoch 5, Batch 69: Training loss = 0.3200
2025-04-03 18:08:20,457 - INFO - Epoch 5, Batch 70: Training loss = 0.3100
2025-04-03 18:08:20,558 - INFO - Epoch 5, Batch 71: Training loss = 0.3000
2025-04-03 18:08:20,659 - INFO - Epoch 5, Batch 72: Training loss = 0.2900
2025-04-03 18:08:20,760 - INFO - Epoch 5, Batch 73: Training loss = 0.2800
2025-04-03 18:08:20,861 - INFO - Epoch 5, Batch 74: Training loss = 0.2700
2025-04-03 18:08:20,965 - INFO - Epoch 5, Batch 75: Training loss = 0.2600
2025-04-03 18:08:21,066 - INFO - Epoch 5, Batch 76: Training loss = 0.2500
2025-04-03 18:08:21,166 - INFO - Epoch 5, Batch 77: Training loss = 0.2400
2025-04-03 18:08:21,267 - INFO - Epoch 5, Batch 78: Training loss = 0.2300
2025-04-03 18:08:21,368 - INFO - Epoch 5, Batch 79: Training loss = 0.2200
2025-04-03 18:08:21,469 - INFO - Epoch 5, Batch 80: Training loss = 0.2100
2025-04-03 18:08:21,570 - INFO - Epoch 5, Batch 81: Training loss = 0.2000
2025-04-03 18:08:21,671 - INFO - Epoch 5, Batch 82: Training loss = 0.1900
2025-04-03 18:08:21,772 - INFO - Epoch 5, Batch 83: Training loss = 0.1800
2025-04-03 18:08:21,872 - INFO - Epoch 5, Batch 84: Training loss = 0.1700
2025-04-03 18:08:21,973 - INFO - Epoch 5, Batch 85: Training loss = 0.1600
2025-04-03 18:08:22,073 - INFO - Epoch 5, Batch 86: Training loss = 0.1500
2025-04-03 18:08:22,174 - INFO - Epoch 5, Batch 87: Training loss = 0.1400
2025-04-03 18:08:22,275 - INFO - Epoch 5, Batch 88: Training loss = 0.1300
2025-04-03 18:08:22,376 - INFO - Epoch 5, Batch 89: Training loss = 0.1200
2025-04-03 18:08:22,477 - INFO - Epoch 5, Batch 90: Training loss = 0.1100
2025-04-03 18:08:22,578 - INFO - Epoch 5, Batch 91: Training loss = 0.1000
2025-04-03 18:08:22,679 - INFO - Epoch 5, Batch 92: Training loss = 0.0900
2025-04-03 18:08:22,780 - INFO - Epoch 5, Batch 93: Training loss = 0.0800
2025-04-03 18:08:22,880 - INFO - Epoch 5, Batch 94: Training loss = 0.0700
2025-04-03 18:08:22,981 - INFO - Epoch 5, Batch 95: Training loss = 0.0600
2025-04-03 18:08:23,082 - INFO - Epoch 5, Batch 96: Training loss = 0.0500
2025-04-03 18:08:23,183 - INFO - Epoch 5, Batch 97: Training loss = 0.0400
2025-04-03 18:08:23,284 - INFO - Epoch 5, Batch 98: Training loss = 0.0300
2025-04-03 18:08:23,385 - INFO - Epoch 5, Batch 99: Training loss = 0.0200
2025-04-03 18:08:23,486 - INFO - Epoch 5, Batch 100: Training loss = 0.0100
2025-04-03 18:08:23,587 - INFO - Epoch 5: Validation loss = 0.0600, Validation accuracy = 0.8800
2025-04-03 18:08:23,587 - INFO - Epoch 5: MSE = 0.0600, RMSE = 0.2449, MAE = 0.0480, RÂ² = 0.9400
2025-04-03 18:08:23,588 - INFO - Epoch 6 started.
2025-04-03 18:08:23,588 - INFO - Epoch 6, Batch 1: Training loss = 1.0000
2025-04-03 18:08:23,689 - INFO - Epoch 6, Batch 2: Training loss = 0.9900
2025-04-03 18:08:23,790 - INFO - Epoch 6, Batch 3: Training loss = 0.9800
2025-04-03 18:08:23,891 - INFO - Epoch 6, Batch 4: Training loss = 0.9700
2025-04-03 18:08:23,991 - INFO - Epoch 6, Batch 5: Training loss = 0.9600
2025-04-03 18:08:24,092 - INFO - Epoch 6, Batch 6: Training loss = 0.9500
2025-04-03 18:08:24,193 - INFO - Epoch 6, Batch 7: Training loss = 0.9400
2025-04-03 18:08:24,294 - INFO - Epoch 6, Batch 8: Training loss = 0.9300
2025-04-03 18:08:24,395 - INFO - Epoch 6, Batch 9: Training loss = 0.9200
2025-04-03 18:08:24,495 - INFO - Epoch 6, Batch 10: Training loss = 0.9100
2025-04-03 18:08:24,596 - INFO - Epoch 6, Batch 11: Training loss = 0.9000
2025-04-03 18:08:24,697 - INFO - Epoch 6, Batch 12: Training loss = 0.8900
2025-04-03 18:08:24,798 - INFO - Epoch 6, Batch 13: Training loss = 0.8800
2025-04-03 18:08:24,898 - INFO - Epoch 6, Batch 14: Training loss = 0.8700
2025-04-03 18:08:24,999 - INFO - Epoch 6, Batch 15: Training loss = 0.8600
2025-04-03 18:08:25,100 - INFO - Epoch 6, Batch 16: Training loss = 0.8500
2025-04-03 18:08:25,201 - INFO - Epoch 6, Batch 17: Training loss = 0.8400
2025-04-03 18:08:25,302 - INFO - Epoch 6, Batch 18: Training loss = 0.8300
2025-04-03 18:08:25,403 - INFO - Epoch 6, Batch 19: Training loss = 0.8200
2025-04-03 18:08:25,505 - INFO - Epoch 6, Batch 20: Training loss = 0.8100
2025-04-03 18:08:25,605 - INFO - Epoch 6, Batch 21: Training loss = 0.8000
2025-04-03 18:08:25,706 - INFO - Epoch 6, Batch 22: Training loss = 0.7900
2025-04-03 18:08:25,807 - INFO - Epoch 6, Batch 23: Training loss = 0.7800
2025-04-03 18:08:25,907 - INFO - Epoch 6, Batch 24: Training loss = 0.7700
2025-04-03 18:08:26,008 - INFO - Epoch 6, Batch 25: Training loss = 0.7600
2025-04-03 18:08:26,108 - INFO - Epoch 6, Batch 26: Training loss = 0.7500
2025-04-03 18:08:26,209 - INFO - Epoch 6, Batch 27: Training loss = 0.7400
2025-04-03 18:08:26,309 - INFO - Epoch 6, Batch 28: Training loss = 0.7300
2025-04-03 18:08:26,409 - INFO - Epoch 6, Batch 29: Training loss = 0.7200
2025-04-03 18:08:26,510 - INFO - Epoch 6, Batch 30: Training loss = 0.7100
2025-04-03 18:08:26,611 - INFO - Epoch 6, Batch 31: Training loss = 0.7000
2025-04-03 18:08:26,711 - INFO - Epoch 6, Batch 32: Training loss = 0.6900
2025-04-03 18:08:26,811 - INFO - Epoch 6, Batch 33: Training loss = 0.6800
2025-04-03 18:08:26,912 - INFO - Epoch 6, Batch 34: Training loss = 0.6700
2025-04-03 18:08:27,012 - INFO - Epoch 6, Batch 35: Training loss = 0.6600
2025-04-03 18:08:27,113 - INFO - Epoch 6, Batch 36: Training loss = 0.6500
2025-04-03 18:08:27,214 - INFO - Epoch 6, Batch 37: Training loss = 0.6400
2025-04-03 18:08:27,314 - INFO - Epoch 6, Batch 38: Training loss = 0.6300
2025-04-03 18:08:27,415 - INFO - Epoch 6, Batch 39: Training loss = 0.6200
2025-04-03 18:08:27,516 - INFO - Epoch 6, Batch 40: Training loss = 0.6100
2025-04-03 18:08:27,616 - INFO - Epoch 6, Batch 41: Training loss = 0.6000
2025-04-03 18:08:27,717 - INFO - Epoch 6, Batch 42: Training loss = 0.5900
2025-04-03 18:08:27,818 - INFO - Epoch 6, Batch 43: Training loss = 0.5800
2025-04-03 18:08:27,918 - INFO - Epoch 6, Batch 44: Training loss = 0.5700
2025-04-03 18:08:28,019 - INFO - Epoch 6, Batch 45: Training loss = 0.5600
2025-04-03 18:08:28,120 - INFO - Epoch 6, Batch 46: Training loss = 0.5500
2025-04-03 18:08:28,220 - INFO - Epoch 6, Batch 47: Training loss = 0.5400
2025-04-03 18:08:28,321 - INFO - Epoch 6, Batch 48: Training loss = 0.5300
2025-04-03 18:08:28,422 - INFO - Epoch 6, Batch 49: Training loss = 0.5200
2025-04-03 18:08:28,523 - INFO - Epoch 6, Batch 50: Training loss = 0.5100
2025-04-03 18:08:28,624 - INFO - Epoch 6, Batch 51: Training loss = 0.5000
2025-04-03 18:08:28,725 - INFO - Epoch 6, Batch 52: Training loss = 0.4900
2025-04-03 18:08:28,826 - INFO - Epoch 6, Batch 53: Training loss = 0.4800
2025-04-03 18:08:28,926 - INFO - Epoch 6, Batch 54: Training loss = 0.4700
2025-04-03 18:08:29,027 - INFO - Epoch 6, Batch 55: Training loss = 0.4600
2025-04-03 18:08:29,127 - INFO - Epoch 6, Batch 56: Training loss = 0.4500
2025-04-03 18:08:29,228 - INFO - Epoch 6, Batch 57: Training loss = 0.4400
2025-04-03 18:08:29,328 - INFO - Epoch 6, Batch 58: Training loss = 0.4300
2025-04-03 18:08:29,429 - INFO - Epoch 6, Batch 59: Training loss = 0.4200
2025-04-03 18:08:29,530 - INFO - Epoch 6, Batch 60: Training loss = 0.4100
2025-04-03 18:08:29,631 - INFO - Epoch 6, Batch 61: Training loss = 0.4000
2025-04-03 18:08:29,732 - INFO - Epoch 6, Batch 62: Training loss = 0.3900
2025-04-03 18:08:29,832 - INFO - Epoch 6, Batch 63: Training loss = 0.3800
2025-04-03 18:08:29,933 - INFO - Epoch 6, Batch 64: Training loss = 0.3700
2025-04-03 18:08:30,033 - INFO - Epoch 6, Batch 65: Training loss = 0.3600
2025-04-03 18:08:30,134 - INFO - Epoch 6, Batch 66: Training loss = 0.3500
2025-04-03 18:08:30,234 - INFO - Epoch 6, Batch 67: Training loss = 0.3400
2025-04-03 18:08:30,335 - INFO - Epoch 6, Batch 68: Training loss = 0.3300
2025-04-03 18:08:30,436 - INFO - Epoch 6, Batch 69: Training loss = 0.3200
2025-04-03 18:08:30,537 - INFO - Epoch 6, Batch 70: Training loss = 0.3100
2025-04-03 18:08:30,638 - INFO - Epoch 6, Batch 71: Training loss = 0.3000
2025-04-03 18:08:30,739 - INFO - Epoch 6, Batch 72: Training loss = 0.2900
2025-04-03 18:08:30,839 - INFO - Epoch 6, Batch 73: Training loss = 0.2800
2025-04-03 18:08:30,940 - INFO - Epoch 6, Batch 74: Training loss = 0.2700
2025-04-03 18:08:31,041 - INFO - Epoch 6, Batch 75: Training loss = 0.2600
2025-04-03 18:08:31,142 - INFO - Epoch 6, Batch 76: Training loss = 0.2500
2025-04-03 18:08:31,242 - INFO - Epoch 6, Batch 77: Training loss = 0.2400
2025-04-03 18:08:31,343 - INFO - Epoch 6, Batch 78: Training loss = 0.2300
2025-04-03 18:08:31,444 - INFO - Epoch 6, Batch 79: Training loss = 0.2200
2025-04-03 18:08:31,545 - INFO - Epoch 6, Batch 80: Training loss = 0.2100
2025-04-03 18:08:31,646 - INFO - Epoch 6, Batch 81: Training loss = 0.2000
2025-04-03 18:08:31,747 - INFO - Epoch 6, Batch 82: Training loss = 0.1900
2025-04-03 18:08:31,848 - INFO - Epoch 6, Batch 83: Training loss = 0.1800
2025-04-03 18:08:31,948 - INFO - Epoch 6, Batch 84: Training loss = 0.1700
2025-04-03 18:08:32,049 - INFO - Epoch 6, Batch 85: Training loss = 0.1600
2025-04-03 18:08:32,149 - INFO - Epoch 6, Batch 86: Training loss = 0.1500
2025-04-03 18:08:32,250 - INFO - Epoch 6, Batch 87: Training loss = 0.1400
2025-04-03 18:08:32,350 - INFO - Epoch 6, Batch 88: Training loss = 0.1300
2025-04-03 18:08:32,451 - INFO - Epoch 6, Batch 89: Training loss = 0.1200
2025-04-03 18:08:32,552 - INFO - Epoch 6, Batch 90: Training loss = 0.1100
2025-04-03 18:08:32,653 - INFO - Epoch 6, Batch 91: Training loss = 0.1000
2025-04-03 18:08:32,754 - INFO - Epoch 6, Batch 92: Training loss = 0.0900
2025-04-03 18:08:32,854 - INFO - Epoch 6, Batch 93: Training loss = 0.0800
2025-04-03 18:08:32,955 - INFO - Epoch 6, Batch 94: Training loss = 0.0700
2025-04-03 18:08:33,056 - INFO - Epoch 6, Batch 95: Training loss = 0.0600
2025-04-03 18:08:33,156 - INFO - Epoch 6, Batch 96: Training loss = 0.0500
2025-04-03 18:08:33,257 - INFO - Epoch 6, Batch 97: Training loss = 0.0400
2025-04-03 18:08:33,358 - INFO - Epoch 6, Batch 98: Training loss = 0.0300
2025-04-03 18:08:33,459 - INFO - Epoch 6, Batch 99: Training loss = 0.0200
2025-04-03 18:08:33,560 - INFO - Epoch 6, Batch 100: Training loss = 0.0100
2025-04-03 18:08:33,661 - INFO - Epoch 6: Validation loss = 0.0500, Validation accuracy = 0.9000
2025-04-03 18:08:33,662 - INFO - Epoch 6: MSE = 0.0500, RMSE = 0.2236, MAE = 0.0400, RÂ² = 0.9500
2025-04-03 18:08:33,662 - INFO - Epoch 7 started.
2025-04-03 18:08:33,662 - INFO - Epoch 7, Batch 1: Training loss = 1.0000
2025-04-03 18:08:33,763 - INFO - Epoch 7, Batch 2: Training loss = 0.9900
2025-04-03 18:08:33,864 - INFO - Epoch 7, Batch 3: Training loss = 0.9800
2025-04-03 18:08:33,965 - INFO - Epoch 7, Batch 4: Training loss = 0.9700
2025-04-03 18:08:34,066 - INFO - Epoch 7, Batch 5: Training loss = 0.9600
2025-04-03 18:08:34,166 - INFO - Epoch 7, Batch 6: Training loss = 0.9500
2025-04-03 18:08:34,267 - INFO - Epoch 7, Batch 7: Training loss = 0.9400
2025-04-03 18:08:34,368 - INFO - Epoch 7, Batch 8: Training loss = 0.9300
2025-04-03 18:08:34,469 - INFO - Epoch 7, Batch 9: Training loss = 0.9200
2025-04-03 18:08:34,569 - INFO - Epoch 7, Batch 10: Training loss = 0.9100
2025-04-03 18:08:34,670 - INFO - Epoch 7, Batch 11: Training loss = 0.9000
2025-04-03 18:08:34,771 - INFO - Epoch 7, Batch 12: Training loss = 0.8900
2025-04-03 18:08:34,872 - INFO - Epoch 7, Batch 13: Training loss = 0.8800
2025-04-03 18:08:34,974 - INFO - Epoch 7, Batch 14: Training loss = 0.8700
2025-04-03 18:08:35,075 - INFO - Epoch 7, Batch 15: Training loss = 0.8600
2025-04-03 18:08:35,175 - INFO - Epoch 7, Batch 16: Training loss = 0.8500
2025-04-03 18:08:35,276 - INFO - Epoch 7, Batch 17: Training loss = 0.8400
2025-04-03 18:08:35,376 - INFO - Epoch 7, Batch 18: Training loss = 0.8300
2025-04-03 18:08:35,477 - INFO - Epoch 7, Batch 19: Training loss = 0.8200
2025-04-03 18:08:35,577 - INFO - Epoch 7, Batch 20: Training loss = 0.8100
2025-04-03 18:08:35,678 - INFO - Epoch 7, Batch 21: Training loss = 0.8000
2025-04-03 18:08:35,780 - INFO - Epoch 7, Batch 22: Training loss = 0.7900
2025-04-03 18:08:35,880 - INFO - Epoch 7, Batch 23: Training loss = 0.7800
2025-04-03 18:08:35,981 - INFO - Epoch 7, Batch 24: Training loss = 0.7700
2025-04-03 18:08:36,082 - INFO - Epoch 7, Batch 25: Training loss = 0.7600
2025-04-03 18:08:36,182 - INFO - Epoch 7, Batch 26: Training loss = 0.7500
2025-04-03 18:08:36,283 - INFO - Epoch 7, Batch 27: Training loss = 0.7400
2025-04-03 18:08:36,384 - INFO - Epoch 7, Batch 28: Training loss = 0.7300
2025-04-03 18:08:36,485 - INFO - Epoch 7, Batch 29: Training loss = 0.7200
2025-04-03 18:08:36,585 - INFO - Epoch 7, Batch 30: Training loss = 0.7100
2025-04-03 18:08:36,686 - INFO - Epoch 7, Batch 31: Training loss = 0.7000
2025-04-03 18:08:36,787 - INFO - Epoch 7, Batch 32: Training loss = 0.6900
2025-04-03 18:08:36,888 - INFO - Epoch 7, Batch 33: Training loss = 0.6800
2025-04-03 18:08:36,989 - INFO - Epoch 7, Batch 34: Training loss = 0.6700
2025-04-03 18:08:37,090 - INFO - Epoch 7, Batch 35: Training loss = 0.6600
2025-04-03 18:08:37,191 - INFO - Epoch 7, Batch 36: Training loss = 0.6500
2025-04-03 18:08:37,292 - INFO - Epoch 7, Batch 37: Training loss = 0.6400
2025-04-03 18:08:37,393 - INFO - Epoch 7, Batch 38: Training loss = 0.6300
2025-04-03 18:08:37,492 - INFO - Epoch 7, Batch 39: Training loss = 0.6200
2025-04-03 18:08:37,593 - INFO - Epoch 7, Batch 40: Training loss = 0.6100
2025-04-03 18:08:37,693 - INFO - Epoch 7, Batch 41: Training loss = 0.6000
2025-04-03 18:08:37,794 - INFO - Epoch 7, Batch 42: Training loss = 0.5900
2025-04-03 18:08:37,894 - INFO - Epoch 7, Batch 43: Training loss = 0.5800
2025-04-03 18:08:37,995 - INFO - Epoch 7, Batch 44: Training loss = 0.5700
2025-04-03 18:08:38,096 - INFO - Epoch 7, Batch 45: Training loss = 0.5600
2025-04-03 18:08:38,196 - INFO - Epoch 7, Batch 46: Training loss = 0.5500
2025-04-03 18:08:38,297 - INFO - Epoch 7, Batch 47: Training loss = 0.5400
2025-04-03 18:08:38,397 - INFO - Epoch 7, Batch 48: Training loss = 0.5300
2025-04-03 18:08:38,498 - INFO - Epoch 7, Batch 49: Training loss = 0.5200
2025-04-03 18:08:38,598 - INFO - Epoch 7, Batch 50: Training loss = 0.5100
2025-04-03 18:08:38,699 - INFO - Epoch 7, Batch 51: Training loss = 0.5000
2025-04-03 18:08:38,800 - INFO - Epoch 7, Batch 52: Training loss = 0.4900
2025-04-03 18:08:38,900 - INFO - Epoch 7, Batch 53: Training loss = 0.4800
2025-04-03 18:08:39,001 - INFO - Epoch 7, Batch 54: Training loss = 0.4700
2025-04-03 18:08:39,102 - INFO - Epoch 7, Batch 55: Training loss = 0.4600
2025-04-03 18:08:39,203 - INFO - Epoch 7, Batch 56: Training loss = 0.4500
2025-04-03 18:08:39,303 - INFO - Epoch 7, Batch 57: Training loss = 0.4400
2025-04-03 18:08:39,404 - INFO - Epoch 7, Batch 58: Training loss = 0.4300
2025-04-03 18:08:39,505 - INFO - Epoch 7, Batch 59: Training loss = 0.4200
2025-04-03 18:08:39,606 - INFO - Epoch 7, Batch 60: Training loss = 0.4100
2025-04-03 18:08:39,707 - INFO - Epoch 7, Batch 61: Training loss = 0.4000
2025-04-03 18:08:39,808 - INFO - Epoch 7, Batch 62: Training loss = 0.3900
2025-04-03 18:08:39,909 - INFO - Epoch 7, Batch 63: Training loss = 0.3800
2025-04-03 18:08:40,009 - INFO - Epoch 7, Batch 64: Training loss = 0.3700
2025-04-03 18:08:40,110 - INFO - Epoch 7, Batch 65: Training loss = 0.3600
2025-04-03 18:08:40,211 - INFO - Epoch 7, Batch 66: Training loss = 0.3500
2025-04-03 18:08:40,312 - INFO - Epoch 7, Batch 67: Training loss = 0.3400
2025-04-03 18:08:40,413 - INFO - Epoch 7, Batch 68: Training loss = 0.3300
2025-04-03 18:08:40,513 - INFO - Epoch 7, Batch 69: Training loss = 0.3200
2025-04-03 18:08:40,614 - INFO - Epoch 7, Batch 70: Training loss = 0.3100
2025-04-03 18:08:40,714 - INFO - Epoch 7, Batch 71: Training loss = 0.3000
2025-04-03 18:08:40,815 - INFO - Epoch 7, Batch 72: Training loss = 0.2900
2025-04-03 18:08:40,916 - INFO - Epoch 7, Batch 73: Training loss = 0.2800
2025-04-03 18:08:41,017 - INFO - Epoch 7, Batch 74: Training loss = 0.2700
2025-04-03 18:08:41,117 - INFO - Epoch 7, Batch 75: Training loss = 0.2600
2025-04-03 18:08:41,218 - INFO - Epoch 7, Batch 76: Training loss = 0.2500
2025-04-03 18:08:41,319 - INFO - Epoch 7, Batch 77: Training loss = 0.2400
2025-04-03 18:08:41,420 - INFO - Epoch 7, Batch 78: Training loss = 0.2300
2025-04-03 18:08:41,521 - INFO - Epoch 7, Batch 79: Training loss = 0.2200
2025-04-03 18:08:41,622 - INFO - Epoch 7, Batch 80: Training loss = 0.2100
2025-04-03 18:08:41,723 - INFO - Epoch 7, Batch 81: Training loss = 0.2000
2025-04-03 18:08:41,823 - INFO - Epoch 7, Batch 82: Training loss = 0.1900
2025-04-03 18:08:41,924 - INFO - Epoch 7, Batch 83: Training loss = 0.1800
2025-04-03 18:08:42,025 - INFO - Epoch 7, Batch 84: Training loss = 0.1700
2025-04-03 18:08:42,125 - INFO - Epoch 7, Batch 85: Training loss = 0.1600
2025-04-03 18:08:42,226 - INFO - Epoch 7, Batch 86: Training loss = 0.1500
2025-04-03 18:08:42,327 - INFO - Epoch 7, Batch 87: Training loss = 0.1400
2025-04-03 18:08:42,427 - INFO - Epoch 7, Batch 88: Training loss = 0.1300
2025-04-03 18:08:42,528 - INFO - Epoch 7, Batch 89: Training loss = 0.1200
2025-04-03 18:08:42,629 - INFO - Epoch 7, Batch 90: Training loss = 0.1100
2025-04-03 18:08:42,730 - INFO - Epoch 7, Batch 91: Training loss = 0.1000
2025-04-03 18:08:42,831 - INFO - Epoch 7, Batch 92: Training loss = 0.0900
2025-04-03 18:08:42,932 - INFO - Epoch 7, Batch 93: Training loss = 0.0800
2025-04-03 18:08:43,033 - INFO - Epoch 7, Batch 94: Training loss = 0.0700
2025-04-03 18:08:43,134 - INFO - Epoch 7, Batch 95: Training loss = 0.0600
2025-04-03 18:08:43,235 - INFO - Epoch 7, Batch 96: Training loss = 0.0500
2025-04-03 18:08:43,336 - INFO - Epoch 7, Batch 97: Training loss = 0.0400
2025-04-03 18:08:43,437 - INFO - Epoch 7, Batch 98: Training loss = 0.0300
2025-04-03 18:08:43,538 - INFO - Epoch 7, Batch 99: Training loss = 0.0200
2025-04-03 18:08:43,639 - INFO - Epoch 7, Batch 100: Training loss = 0.0100
2025-04-03 18:08:43,740 - INFO - Epoch 7: Validation loss = 0.0400, Validation accuracy = 0.9200
2025-04-03 18:08:43,741 - INFO - Epoch 7: MSE = 0.0400, RMSE = 0.2000, MAE = 0.0320, RÂ² = 0.9600
2025-04-03 18:08:43,741 - INFO - Epoch 8 started.
2025-04-03 18:08:43,742 - INFO - Epoch 8, Batch 1: Training loss = 1.0000
2025-04-03 18:08:43,842 - INFO - Epoch 8, Batch 2: Training loss = 0.9900
2025-04-03 18:08:43,943 - INFO - Epoch 8, Batch 3: Training loss = 0.9800
2025-04-03 18:08:44,044 - INFO - Epoch 8, Batch 4: Training loss = 0.9700
2025-04-03 18:08:44,144 - INFO - Epoch 8, Batch 5: Training loss = 0.9600
2025-04-03 18:08:44,245 - INFO - Epoch 8, Batch 6: Training loss = 0.9500
2025-04-03 18:08:44,346 - INFO - Epoch 8, Batch 7: Training loss = 0.9400
2025-04-03 18:08:44,446 - INFO - Epoch 8, Batch 8: Training loss = 0.9300
2025-04-03 18:08:44,547 - INFO - Epoch 8, Batch 9: Training loss = 0.9200
2025-04-03 18:08:44,648 - INFO - Epoch 8, Batch 10: Training loss = 0.9100
2025-04-03 18:08:44,748 - INFO - Epoch 8, Batch 11: Training loss = 0.9000
2025-04-03 18:08:44,850 - INFO - Epoch 8, Batch 12: Training loss = 0.8900
2025-04-03 18:08:44,951 - INFO - Epoch 8, Batch 13: Training loss = 0.8800
2025-04-03 18:08:45,052 - INFO - Epoch 8, Batch 14: Training loss = 0.8700
2025-04-03 18:08:45,152 - INFO - Epoch 8, Batch 15: Training loss = 0.8600
2025-04-03 18:08:45,253 - INFO - Epoch 8, Batch 16: Training loss = 0.8500
2025-04-03 18:08:45,353 - INFO - Epoch 8, Batch 17: Training loss = 0.8400
2025-04-03 18:08:45,454 - INFO - Epoch 8, Batch 18: Training loss = 0.8300
2025-04-03 18:08:45,555 - INFO - Epoch 8, Batch 19: Training loss = 0.8200
2025-04-03 18:08:45,655 - INFO - Epoch 8, Batch 20: Training loss = 0.8100
2025-04-03 18:08:45,756 - INFO - Epoch 8, Batch 21: Training loss = 0.8000
2025-04-03 18:08:45,856 - INFO - Epoch 8, Batch 22: Training loss = 0.7900
2025-04-03 18:08:45,957 - INFO - Epoch 8, Batch 23: Training loss = 0.7800
2025-04-03 18:08:46,057 - INFO - Epoch 8, Batch 24: Training loss = 0.7700
2025-04-03 18:08:46,158 - INFO - Epoch 8, Batch 25: Training loss = 0.7600
2025-04-03 18:08:46,258 - INFO - Epoch 8, Batch 26: Training loss = 0.7500
2025-04-03 18:08:46,359 - INFO - Epoch 8, Batch 27: Training loss = 0.7400
2025-04-03 18:08:46,460 - INFO - Epoch 8, Batch 28: Training loss = 0.7300
2025-04-03 18:08:46,561 - INFO - Epoch 8, Batch 29: Training loss = 0.7200
2025-04-03 18:08:46,662 - INFO - Epoch 8, Batch 30: Training loss = 0.7100
2025-04-03 18:08:46,763 - INFO - Epoch 8, Batch 31: Training loss = 0.7000
2025-04-03 18:08:46,864 - INFO - Epoch 8, Batch 32: Training loss = 0.6900
2025-04-03 18:08:46,965 - INFO - Epoch 8, Batch 33: Training loss = 0.6800
2025-04-03 18:08:47,066 - INFO - Epoch 8, Batch 34: Training loss = 0.6700
2025-04-03 18:08:47,166 - INFO - Epoch 8, Batch 35: Training loss = 0.6600
2025-04-03 18:08:47,267 - INFO - Epoch 8, Batch 36: Training loss = 0.6500
2025-04-03 18:08:47,368 - INFO - Epoch 8, Batch 37: Training loss = 0.6400
2025-04-03 18:08:47,469 - INFO - Epoch 8, Batch 38: Training loss = 0.6300
2025-04-03 18:08:47,570 - INFO - Epoch 8, Batch 39: Training loss = 0.6200
2025-04-03 18:08:47,671 - INFO - Epoch 8, Batch 40: Training loss = 0.6100
2025-04-03 18:08:47,772 - INFO - Epoch 8, Batch 41: Training loss = 0.6000
2025-04-03 18:08:47,873 - INFO - Epoch 8, Batch 42: Training loss = 0.5900
2025-04-03 18:08:47,973 - INFO - Epoch 8, Batch 43: Training loss = 0.5800
2025-04-03 18:08:48,074 - INFO - Epoch 8, Batch 44: Training loss = 0.5700
2025-04-03 18:08:48,174 - INFO - Epoch 8, Batch 45: Training loss = 0.5600
2025-04-03 18:08:48,275 - INFO - Epoch 8, Batch 46: Training loss = 0.5500
2025-04-03 18:08:48,376 - INFO - Epoch 8, Batch 47: Training loss = 0.5400
2025-04-03 18:08:48,477 - INFO - Epoch 8, Batch 48: Training loss = 0.5300
2025-04-03 18:08:48,577 - INFO - Epoch 8, Batch 49: Training loss = 0.5200
2025-04-03 18:08:48,678 - INFO - Epoch 8, Batch 50: Training loss = 0.5100
2025-04-03 18:08:48,779 - INFO - Epoch 8, Batch 51: Training loss = 0.5000
2025-04-03 18:08:48,880 - INFO - Epoch 8, Batch 52: Training loss = 0.4900
2025-04-03 18:08:48,980 - INFO - Epoch 8, Batch 53: Training loss = 0.4800
2025-04-03 18:08:49,081 - INFO - Epoch 8, Batch 54: Training loss = 0.4700
2025-04-03 18:08:49,182 - INFO - Epoch 8, Batch 55: Training loss = 0.4600
2025-04-03 18:08:49,283 - INFO - Epoch 8, Batch 56: Training loss = 0.4500
2025-04-03 18:08:49,384 - INFO - Epoch 8, Batch 57: Training loss = 0.4400
2025-04-03 18:08:49,486 - INFO - Epoch 8, Batch 58: Training loss = 0.4300
2025-04-03 18:08:49,587 - INFO - Epoch 8, Batch 59: Training loss = 0.4200
2025-04-03 18:08:49,688 - INFO - Epoch 8, Batch 60: Training loss = 0.4100
2025-04-03 18:08:49,789 - INFO - Epoch 8, Batch 61: Training loss = 0.4000
2025-04-03 18:08:49,890 - INFO - Epoch 8, Batch 62: Training loss = 0.3900
2025-04-03 18:08:49,990 - INFO - Epoch 8, Batch 63: Training loss = 0.3800
2025-04-03 18:08:50,091 - INFO - Epoch 8, Batch 64: Training loss = 0.3700
2025-04-03 18:08:50,192 - INFO - Epoch 8, Batch 65: Training loss = 0.3600
2025-04-03 18:08:50,293 - INFO - Epoch 8, Batch 66: Training loss = 0.3500
2025-04-03 18:08:50,394 - INFO - Epoch 8, Batch 67: Training loss = 0.3400
2025-04-03 18:08:50,494 - INFO - Epoch 8, Batch 68: Training loss = 0.3300
2025-04-03 18:08:50,595 - INFO - Epoch 8, Batch 69: Training loss = 0.3200
2025-04-03 18:08:50,695 - INFO - Epoch 8, Batch 70: Training loss = 0.3100
2025-04-03 18:08:50,796 - INFO - Epoch 8, Batch 71: Training loss = 0.3000
2025-04-03 18:08:50,897 - INFO - Epoch 8, Batch 72: Training loss = 0.2900
2025-04-03 18:08:50,997 - INFO - Epoch 8, Batch 73: Training loss = 0.2800
2025-04-03 18:08:51,098 - INFO - Epoch 8, Batch 74: Training loss = 0.2700
2025-04-03 18:08:51,198 - INFO - Epoch 8, Batch 75: Training loss = 0.2600
2025-04-03 18:08:51,299 - INFO - Epoch 8, Batch 76: Training loss = 0.2500
2025-04-03 18:08:51,400 - INFO - Epoch 8, Batch 77: Training loss = 0.2400
2025-04-03 18:08:51,501 - INFO - Epoch 8, Batch 78: Training loss = 0.2300
2025-04-03 18:08:51,602 - INFO - Epoch 8, Batch 79: Training loss = 0.2200
2025-04-03 18:08:51,703 - INFO - Epoch 8, Batch 80: Training loss = 0.2100
2025-04-03 18:08:51,804 - INFO - Epoch 8, Batch 81: Training loss = 0.2000
2025-04-03 18:08:51,904 - INFO - Epoch 8, Batch 82: Training loss = 0.1900
2025-04-03 18:08:52,005 - INFO - Epoch 8, Batch 83: Training loss = 0.1800
2025-04-03 18:08:52,106 - INFO - Epoch 8, Batch 84: Training loss = 0.1700
2025-04-03 18:08:52,207 - INFO - Epoch 8, Batch 85: Training loss = 0.1600
2025-04-03 18:08:52,308 - INFO - Epoch 8, Batch 86: Training loss = 0.1500
2025-04-03 18:08:52,409 - INFO - Epoch 8, Batch 87: Training loss = 0.1400
2025-04-03 18:08:52,510 - INFO - Epoch 8, Batch 88: Training loss = 0.1300
2025-04-03 18:08:52,611 - INFO - Epoch 8, Batch 89: Training loss = 0.1200
2025-04-03 18:08:52,712 - INFO - Epoch 8, Batch 90: Training loss = 0.1100
2025-04-03 18:08:52,813 - INFO - Epoch 8, Batch 91: Training loss = 0.1000
2025-04-03 18:08:52,914 - INFO - Epoch 8, Batch 92: Training loss = 0.0900
2025-04-03 18:08:53,015 - INFO - Epoch 8, Batch 93: Training loss = 0.0800
2025-04-03 18:08:53,115 - INFO - Epoch 8, Batch 94: Training loss = 0.0700
2025-04-03 18:08:53,216 - INFO - Epoch 8, Batch 95: Training loss = 0.0600
2025-04-03 18:08:53,316 - INFO - Epoch 8, Batch 96: Training loss = 0.0500
2025-04-03 18:08:53,418 - INFO - Epoch 8, Batch 97: Training loss = 0.0400
2025-04-03 18:08:53,519 - INFO - Epoch 8, Batch 98: Training loss = 0.0300
2025-04-03 18:08:53,620 - INFO - Epoch 8, Batch 99: Training loss = 0.0200
2025-04-03 18:08:53,721 - INFO - Epoch 8, Batch 100: Training loss = 0.0100
2025-04-03 18:08:53,821 - INFO - Epoch 8: Validation loss = 0.0300, Validation accuracy = 0.9400
2025-04-03 18:08:53,822 - INFO - Epoch 8: MSE = 0.0300, RMSE = 0.1732, MAE = 0.0240, RÂ² = 0.9700
2025-04-03 18:08:53,822 - INFO - Epoch 9 started.
2025-04-03 18:08:53,822 - INFO - Epoch 9, Batch 1: Training loss = 1.0000
2025-04-03 18:08:53,923 - INFO - Epoch 9, Batch 2: Training loss = 0.9900
2025-04-03 18:08:54,024 - INFO - Epoch 9, Batch 3: Training loss = 0.9800
2025-04-03 18:08:54,125 - INFO - Epoch 9, Batch 4: Training loss = 0.9700
2025-04-03 18:08:54,225 - INFO - Epoch 9, Batch 5: Training loss = 0.9600
2025-04-03 18:08:54,326 - INFO - Epoch 9, Batch 6: Training loss = 0.9500
2025-04-03 18:08:54,427 - INFO - Epoch 9, Batch 7: Training loss = 0.9400
2025-04-03 18:08:54,528 - INFO - Epoch 9, Batch 8: Training loss = 0.9300
2025-04-03 18:08:54,629 - INFO - Epoch 9, Batch 9: Training loss = 0.9200
2025-04-03 18:08:54,729 - INFO - Epoch 9, Batch 10: Training loss = 0.9100
2025-04-03 18:08:54,830 - INFO - Epoch 9, Batch 11: Training loss = 0.9000
2025-04-03 18:08:54,931 - INFO - Epoch 9, Batch 12: Training loss = 0.8900
2025-04-03 18:08:55,032 - INFO - Epoch 9, Batch 13: Training loss = 0.8800
2025-04-03 18:08:55,132 - INFO - Epoch 9, Batch 14: Training loss = 0.8700
2025-04-03 18:08:55,233 - INFO - Epoch 9, Batch 15: Training loss = 0.8600
2025-04-03 18:08:55,334 - INFO - Epoch 9, Batch 16: Training loss = 0.8500
2025-04-03 18:08:55,435 - INFO - Epoch 9, Batch 17: Training loss = 0.8400
2025-04-03 18:08:55,536 - INFO - Epoch 9, Batch 18: Training loss = 0.8300
2025-04-03 18:08:55,636 - INFO - Epoch 9, Batch 19: Training loss = 0.8200
2025-04-03 18:08:55,738 - INFO - Epoch 9, Batch 20: Training loss = 0.8100
2025-04-03 18:08:55,838 - INFO - Epoch 9, Batch 21: Training loss = 0.8000
2025-04-03 18:08:55,939 - INFO - Epoch 9, Batch 22: Training loss = 0.7900
2025-04-03 18:08:56,040 - INFO - Epoch 9, Batch 23: Training loss = 0.7800
2025-04-03 18:08:56,141 - INFO - Epoch 9, Batch 24: Training loss = 0.7700
2025-04-03 18:08:56,242 - INFO - Epoch 9, Batch 25: Training loss = 0.7600
2025-04-03 18:08:56,343 - INFO - Epoch 9, Batch 26: Training loss = 0.7500
2025-04-03 18:08:56,444 - INFO - Epoch 9, Batch 27: Training loss = 0.7400
2025-04-03 18:08:56,545 - INFO - Epoch 9, Batch 28: Training loss = 0.7300
2025-04-03 18:08:56,646 - INFO - Epoch 9, Batch 29: Training loss = 0.7200
2025-04-03 18:08:56,746 - INFO - Epoch 9, Batch 30: Training loss = 0.7100
2025-04-03 18:08:56,847 - INFO - Epoch 9, Batch 31: Training loss = 0.7000
2025-04-03 18:08:56,948 - INFO - Epoch 9, Batch 32: Training loss = 0.6900
2025-04-03 18:08:57,048 - INFO - Epoch 9, Batch 33: Training loss = 0.6800
2025-04-03 18:08:57,149 - INFO - Epoch 9, Batch 34: Training loss = 0.6700
2025-04-03 18:08:57,250 - INFO - Epoch 9, Batch 35: Training loss = 0.6600
2025-04-03 18:08:57,351 - INFO - Epoch 9, Batch 36: Training loss = 0.6500
2025-04-03 18:08:57,452 - INFO - Epoch 9, Batch 37: Training loss = 0.6400
2025-04-03 18:08:57,553 - INFO - Epoch 9, Batch 38: Training loss = 0.6300
2025-04-03 18:08:57,654 - INFO - Epoch 9, Batch 39: Training loss = 0.6200
2025-04-03 18:08:57,754 - INFO - Epoch 9, Batch 40: Training loss = 0.6100
2025-04-03 18:08:57,855 - INFO - Epoch 9, Batch 41: Training loss = 0.6000
2025-04-03 18:08:57,956 - INFO - Epoch 9, Batch 42: Training loss = 0.5900
2025-04-03 18:08:58,057 - INFO - Epoch 9, Batch 43: Training loss = 0.5800
2025-04-03 18:08:58,158 - INFO - Epoch 9, Batch 44: Training loss = 0.5700
2025-04-03 18:08:58,258 - INFO - Epoch 9, Batch 45: Training loss = 0.5600
2025-04-03 18:08:58,359 - INFO - Epoch 9, Batch 46: Training loss = 0.5500
2025-04-03 18:08:58,460 - INFO - Epoch 9, Batch 47: Training loss = 0.5400
2025-04-03 18:08:58,561 - INFO - Epoch 9, Batch 48: Training loss = 0.5300
2025-04-03 18:08:58,662 - INFO - Epoch 9, Batch 49: Training loss = 0.5200
2025-04-03 18:08:58,763 - INFO - Epoch 9, Batch 50: Training loss = 0.5100
2025-04-03 18:08:58,863 - INFO - Epoch 9, Batch 51: Training loss = 0.5000
2025-04-03 18:08:58,964 - INFO - Epoch 9, Batch 52: Training loss = 0.4900
2025-04-03 18:08:59,065 - INFO - Epoch 9, Batch 53: Training loss = 0.4800
2025-04-03 18:08:59,166 - INFO - Epoch 9, Batch 54: Training loss = 0.4700
2025-04-03 18:08:59,267 - INFO - Epoch 9, Batch 55: Training loss = 0.4600
2025-04-03 18:08:59,367 - INFO - Epoch 9, Batch 56: Training loss = 0.4500
2025-04-03 18:08:59,468 - INFO - Epoch 9, Batch 57: Training loss = 0.4400
2025-04-03 18:08:59,570 - INFO - Epoch 9, Batch 58: Training loss = 0.4300
2025-04-03 18:08:59,670 - INFO - Epoch 9, Batch 59: Training loss = 0.4200
2025-04-03 18:08:59,771 - INFO - Epoch 9, Batch 60: Training loss = 0.4100
2025-04-03 18:08:59,872 - INFO - Epoch 9, Batch 61: Training loss = 0.4000
2025-04-03 18:08:59,973 - INFO - Epoch 9, Batch 62: Training loss = 0.3900
2025-04-03 18:09:00,074 - INFO - Epoch 9, Batch 63: Training loss = 0.3800
2025-04-03 18:09:00,174 - INFO - Epoch 9, Batch 64: Training loss = 0.3700
2025-04-03 18:09:00,275 - INFO - Epoch 9, Batch 65: Training loss = 0.3600
2025-04-03 18:09:00,376 - INFO - Epoch 9, Batch 66: Training loss = 0.3500
2025-04-03 18:09:00,477 - INFO - Epoch 9, Batch 67: Training loss = 0.3400
2025-04-03 18:09:00,578 - INFO - Epoch 9, Batch 68: Training loss = 0.3300
2025-04-03 18:09:00,678 - INFO - Epoch 9, Batch 69: Training loss = 0.3200
2025-04-03 18:09:00,779 - INFO - Epoch 9, Batch 70: Training loss = 0.3100
2025-04-03 18:09:00,880 - INFO - Epoch 9, Batch 71: Training loss = 0.3000
2025-04-03 18:09:00,981 - INFO - Epoch 9, Batch 72: Training loss = 0.2900
2025-04-03 18:09:01,081 - INFO - Epoch 9, Batch 73: Training loss = 0.2800
2025-04-03 18:09:01,182 - INFO - Epoch 9, Batch 74: Training loss = 0.2700
2025-04-03 18:09:01,283 - INFO - Epoch 9, Batch 75: Training loss = 0.2600
2025-04-03 18:09:01,383 - INFO - Epoch 9, Batch 76: Training loss = 0.2500
2025-04-03 18:09:01,484 - INFO - Epoch 9, Batch 77: Training loss = 0.2400
2025-04-03 18:09:01,584 - INFO - Epoch 9, Batch 78: Training loss = 0.2300
2025-04-03 18:09:01,686 - INFO - Epoch 9, Batch 79: Training loss = 0.2200
2025-04-03 18:09:01,787 - INFO - Epoch 9, Batch 80: Training loss = 0.2100
2025-04-03 18:09:01,887 - INFO - Epoch 9, Batch 81: Training loss = 0.2000
2025-04-03 18:09:01,988 - INFO - Epoch 9, Batch 82: Training loss = 0.1900
2025-04-03 18:09:02,089 - INFO - Epoch 9, Batch 83: Training loss = 0.1800
2025-04-03 18:09:02,189 - INFO - Epoch 9, Batch 84: Training loss = 0.1700
2025-04-03 18:09:02,290 - INFO - Epoch 9, Batch 85: Training loss = 0.1600
2025-04-03 18:09:02,391 - INFO - Epoch 9, Batch 86: Training loss = 0.1500
2025-04-03 18:09:02,492 - INFO - Epoch 9, Batch 87: Training loss = 0.1400
2025-04-03 18:09:02,593 - INFO - Epoch 9, Batch 88: Training loss = 0.1300
2025-04-03 18:09:02,693 - INFO - Epoch 9, Batch 89: Training loss = 0.1200
2025-04-03 18:09:02,794 - INFO - Epoch 9, Batch 90: Training loss = 0.1100
2025-04-03 18:09:02,895 - INFO - Epoch 9, Batch 91: Training loss = 0.1000
2025-04-03 18:09:02,996 - INFO - Epoch 9, Batch 92: Training loss = 0.0900
2025-04-03 18:09:03,097 - INFO - Epoch 9, Batch 93: Training loss = 0.0800
2025-04-03 18:09:03,198 - INFO - Epoch 9, Batch 94: Training loss = 0.0700
2025-04-03 18:09:03,299 - INFO - Epoch 9, Batch 95: Training loss = 0.0600
2025-04-03 18:09:03,400 - INFO - Epoch 9, Batch 96: Training loss = 0.0500
2025-04-03 18:09:03,500 - INFO - Epoch 9, Batch 97: Training loss = 0.0400
2025-04-03 18:09:03,601 - INFO - Epoch 9, Batch 98: Training loss = 0.0300
2025-04-03 18:09:03,702 - INFO - Epoch 9, Batch 99: Training loss = 0.0200
2025-04-03 18:09:03,804 - INFO - Epoch 9, Batch 100: Training loss = 0.0100
2025-04-03 18:09:03,905 - INFO - Epoch 9: Validation loss = 0.0200, Validation accuracy = 0.9600
2025-04-03 18:09:03,905 - INFO - Epoch 9: MSE = 0.0200, RMSE = 0.1414, MAE = 0.0160, RÂ² = 0.9800
2025-04-03 18:09:03,906 - INFO - Epoch 10 started.
2025-04-03 18:09:03,906 - INFO - Epoch 10, Batch 1: Training loss = 1.0000
2025-04-03 18:09:04,007 - INFO - Epoch 10, Batch 2: Training loss = 0.9900
2025-04-03 18:09:04,108 - INFO - Epoch 10, Batch 3: Training loss = 0.9800
2025-04-03 18:09:04,208 - INFO - Epoch 10, Batch 4: Training loss = 0.9700
2025-04-03 18:09:04,310 - INFO - Epoch 10, Batch 5: Training loss = 0.9600
2025-04-03 18:09:04,410 - INFO - Epoch 10, Batch 6: Training loss = 0.9500
2025-04-03 18:09:04,511 - INFO - Epoch 10, Batch 7: Training loss = 0.9400
2025-04-03 18:09:04,612 - INFO - Epoch 10, Batch 8: Training loss = 0.9300
2025-04-03 18:09:04,713 - INFO - Epoch 10, Batch 9: Training loss = 0.9200
2025-04-03 18:09:04,814 - INFO - Epoch 10, Batch 10: Training loss = 0.9100
2025-04-03 18:09:04,915 - INFO - Epoch 10, Batch 11: Training loss = 0.9000
2025-04-03 18:09:05,015 - INFO - Epoch 10, Batch 12: Training loss = 0.8900
2025-04-03 18:09:05,116 - INFO - Epoch 10, Batch 13: Training loss = 0.8800
2025-04-03 18:09:05,217 - INFO - Epoch 10, Batch 14: Training loss = 0.8700
2025-04-03 18:09:05,317 - INFO - Epoch 10, Batch 15: Training loss = 0.8600
2025-04-03 18:09:05,418 - INFO - Epoch 10, Batch 16: Training loss = 0.8500
2025-04-03 18:09:05,519 - INFO - Epoch 10, Batch 17: Training loss = 0.8400
2025-04-03 18:09:05,620 - INFO - Epoch 10, Batch 18: Training loss = 0.8300
2025-04-03 18:09:05,721 - INFO - Epoch 10, Batch 19: Training loss = 0.8200
2025-04-03 18:09:05,822 - INFO - Epoch 10, Batch 20: Training loss = 0.8100
2025-04-03 18:09:05,923 - INFO - Epoch 10, Batch 21: Training loss = 0.8000
2025-04-03 18:09:06,023 - INFO - Epoch 10, Batch 22: Training loss = 0.7900
2025-04-03 18:09:06,124 - INFO - Epoch 10, Batch 23: Training loss = 0.7800
2025-04-03 18:09:06,225 - INFO - Epoch 10, Batch 24: Training loss = 0.7700
2025-04-03 18:09:06,326 - INFO - Epoch 10, Batch 25: Training loss = 0.7600
2025-04-03 18:09:06,427 - INFO - Epoch 10, Batch 26: Training loss = 0.7500
2025-04-03 18:09:06,528 - INFO - Epoch 10, Batch 27: Training loss = 0.7400
2025-04-03 18:09:06,629 - INFO - Epoch 10, Batch 28: Training loss = 0.7300
2025-04-03 18:09:06,730 - INFO - Epoch 10, Batch 29: Training loss = 0.7200
2025-04-03 18:09:06,831 - INFO - Epoch 10, Batch 30: Training loss = 0.7100
2025-04-03 18:09:06,931 - INFO - Epoch 10, Batch 31: Training loss = 0.7000
2025-04-03 18:09:07,032 - INFO - Epoch 10, Batch 32: Training loss = 0.6900
2025-04-03 18:09:07,133 - INFO - Epoch 10, Batch 33: Training loss = 0.6800
2025-04-03 18:09:07,233 - INFO - Epoch 10, Batch 34: Training loss = 0.6700
2025-04-03 18:09:07,334 - INFO - Epoch 10, Batch 35: Training loss = 0.6600
2025-04-03 18:09:07,433 - INFO - Epoch 10, Batch 36: Training loss = 0.6500
2025-04-03 18:09:07,534 - INFO - Epoch 10, Batch 37: Training loss = 0.6400
2025-04-03 18:09:07,634 - INFO - Epoch 10, Batch 38: Training loss = 0.6300
2025-04-03 18:09:07,735 - INFO - Epoch 10, Batch 39: Training loss = 0.6200
2025-04-03 18:09:07,836 - INFO - Epoch 10, Batch 40: Training loss = 0.6100
2025-04-03 18:09:07,937 - INFO - Epoch 10, Batch 41: Training loss = 0.6000
2025-04-03 18:09:08,037 - INFO - Epoch 10, Batch 42: Training loss = 0.5900
2025-04-03 18:09:08,138 - INFO - Epoch 10, Batch 43: Training loss = 0.5800
2025-04-03 18:09:08,239 - INFO - Epoch 10, Batch 44: Training loss = 0.5700
2025-04-03 18:09:08,339 - INFO - Epoch 10, Batch 45: Training loss = 0.5600
2025-04-03 18:09:08,440 - INFO - Epoch 10, Batch 46: Training loss = 0.5500
2025-04-03 18:09:08,541 - INFO - Epoch 10, Batch 47: Training loss = 0.5400
2025-04-03 18:09:08,641 - INFO - Epoch 10, Batch 48: Training loss = 0.5300
2025-04-03 18:09:08,742 - INFO - Epoch 10, Batch 49: Training loss = 0.5200
2025-04-03 18:09:08,843 - INFO - Epoch 10, Batch 50: Training loss = 0.5100
2025-04-03 18:09:08,944 - INFO - Epoch 10, Batch 51: Training loss = 0.5000
2025-04-03 18:09:09,045 - INFO - Epoch 10, Batch 52: Training loss = 0.4900
2025-04-03 18:09:09,145 - INFO - Epoch 10, Batch 53: Training loss = 0.4800
2025-04-03 18:09:09,246 - INFO - Epoch 10, Batch 54: Training loss = 0.4700
2025-04-03 18:09:09,347 - INFO - Epoch 10, Batch 55: Training loss = 0.4600
2025-04-03 18:09:09,448 - INFO - Epoch 10, Batch 56: Training loss = 0.4500
2025-04-03 18:09:09,548 - INFO - Epoch 10, Batch 57: Training loss = 0.4400
2025-04-03 18:09:09,649 - INFO - Epoch 10, Batch 58: Training loss = 0.4300
2025-04-03 18:09:09,750 - INFO - Epoch 10, Batch 59: Training loss = 0.4200
2025-04-03 18:09:09,850 - INFO - Epoch 10, Batch 60: Training loss = 0.4100
2025-04-03 18:09:09,951 - INFO - Epoch 10, Batch 61: Training loss = 0.4000
2025-04-03 18:09:10,052 - INFO - Epoch 10, Batch 62: Training loss = 0.3900
2025-04-03 18:09:10,153 - INFO - Epoch 10, Batch 63: Training loss = 0.3800
2025-04-03 18:09:10,253 - INFO - Epoch 10, Batch 64: Training loss = 0.3700
2025-04-03 18:09:10,354 - INFO - Epoch 10, Batch 65: Training loss = 0.3600
2025-04-03 18:09:10,455 - INFO - Epoch 10, Batch 66: Training loss = 0.3500
2025-04-03 18:09:10,556 - INFO - Epoch 10, Batch 67: Training loss = 0.3400
2025-04-03 18:09:10,657 - INFO - Epoch 10, Batch 68: Training loss = 0.3300
2025-04-03 18:09:10,758 - INFO - Epoch 10, Batch 69: Training loss = 0.3200
2025-04-03 18:09:10,859 - INFO - Epoch 10, Batch 70: Training loss = 0.3100
2025-04-03 18:09:10,959 - INFO - Epoch 10, Batch 71: Training loss = 0.3000
2025-04-03 18:09:11,060 - INFO - Epoch 10, Batch 72: Training loss = 0.2900
2025-04-03 18:09:11,161 - INFO - Epoch 10, Batch 73: Training loss = 0.2800
2025-04-03 18:09:11,261 - INFO - Epoch 10, Batch 74: Training loss = 0.2700
2025-04-03 18:09:11,362 - INFO - Epoch 10, Batch 75: Training loss = 0.2600
2025-04-03 18:09:11,463 - INFO - Epoch 10, Batch 76: Training loss = 0.2500
2025-04-03 18:09:11,565 - INFO - Epoch 10, Batch 77: Training loss = 0.2400
2025-04-03 18:09:11,666 - INFO - Epoch 10, Batch 78: Training loss = 0.2300
2025-04-03 18:09:11,767 - INFO - Epoch 10, Batch 79: Training loss = 0.2200
2025-04-03 18:09:11,867 - INFO - Epoch 10, Batch 80: Training loss = 0.2100
2025-04-03 18:09:11,968 - INFO - Epoch 10, Batch 81: Training loss = 0.2000
2025-04-03 18:09:12,069 - INFO - Epoch 10, Batch 82: Training loss = 0.1900
2025-04-03 18:09:12,170 - INFO - Epoch 10, Batch 83: Training loss = 0.1800
2025-04-03 18:09:12,270 - INFO - Epoch 10, Batch 84: Training loss = 0.1700
2025-04-03 18:09:12,372 - INFO - Epoch 10, Batch 85: Training loss = 0.1600
2025-04-03 18:09:12,472 - INFO - Epoch 10, Batch 86: Training loss = 0.1500
2025-04-03 18:09:12,573 - INFO - Epoch 10, Batch 87: Training loss = 0.1400
2025-04-03 18:09:12,674 - INFO - Epoch 10, Batch 88: Training loss = 0.1300
2025-04-03 18:09:12,775 - INFO - Epoch 10, Batch 89: Training loss = 0.1200
2025-04-03 18:09:12,876 - INFO - Epoch 10, Batch 90: Training loss = 0.1100
2025-04-03 18:09:12,976 - INFO - Epoch 10, Batch 91: Training loss = 0.1000
2025-04-03 18:09:13,076 - INFO - Epoch 10, Batch 92: Training loss = 0.0900
2025-04-03 18:09:13,177 - INFO - Epoch 10, Batch 93: Training loss = 0.0800
2025-04-03 18:09:13,278 - INFO - Epoch 10, Batch 94: Training loss = 0.0700
2025-04-03 18:09:13,379 - INFO - Epoch 10, Batch 95: Training loss = 0.0600
2025-04-03 18:09:13,480 - INFO - Epoch 10, Batch 96: Training loss = 0.0500
2025-04-03 18:09:13,581 - INFO - Epoch 10, Batch 97: Training loss = 0.0400
2025-04-03 18:09:13,682 - INFO - Epoch 10, Batch 98: Training loss = 0.0300
2025-04-03 18:09:13,783 - INFO - Epoch 10, Batch 99: Training loss = 0.0200
2025-04-03 18:09:13,884 - INFO - Epoch 10, Batch 100: Training loss = 0.0100
2025-04-03 18:09:13,985 - INFO - Epoch 10: Validation loss = 0.0100, Validation accuracy = 0.9800
2025-04-03 18:09:13,986 - INFO - Epoch 10: MSE = 0.0100, RMSE = 0.1000, MAE = 0.0080, RÂ² = 0.9900
2025-04-03 18:09:13,986 - INFO - Regression training process completed.
2025-04-03 18:20:01,546 - INFO - Starting regression training process...
2025-04-03 18:20:01,546 - INFO - Epoch 1 started.
2025-04-03 18:20:01,546 - INFO - Epoch 1, Batch 1: Training loss = 1.0000
2025-04-03 18:20:01,647 - INFO - Epoch 1, Batch 2: Training loss = 0.9900
2025-04-03 18:20:01,747 - INFO - Epoch 1, Batch 3: Training loss = 0.9800
2025-04-03 18:20:01,848 - INFO - Epoch 1, Batch 4: Training loss = 0.9700
2025-04-03 18:20:01,949 - INFO - Epoch 1, Batch 5: Training loss = 0.9600
2025-04-03 18:20:02,049 - INFO - Epoch 1, Batch 6: Training loss = 0.9500
2025-04-03 18:20:02,150 - INFO - Epoch 1, Batch 7: Training loss = 0.9400
2025-04-03 18:20:02,250 - INFO - Epoch 1, Batch 8: Training loss = 0.9300
2025-04-03 18:20:02,351 - INFO - Epoch 1, Batch 9: Training loss = 0.9200
2025-04-03 18:20:02,452 - INFO - Epoch 1, Batch 10: Training loss = 0.9100
2025-04-03 18:20:02,553 - INFO - Epoch 1, Batch 11: Training loss = 0.9000
2025-04-03 18:20:02,654 - INFO - Epoch 1, Batch 12: Training loss = 0.8900
2025-04-03 18:20:02,754 - INFO - Epoch 1, Batch 13: Training loss = 0.8800
2025-04-03 18:20:02,855 - INFO - Epoch 1, Batch 14: Training loss = 0.8700
2025-04-03 18:20:02,956 - INFO - Epoch 1, Batch 15: Training loss = 0.8600
2025-04-03 18:20:03,057 - INFO - Epoch 1, Batch 16: Training loss = 0.8500
2025-04-03 18:20:03,157 - INFO - Epoch 1, Batch 17: Training loss = 0.8400
2025-04-03 18:20:03,258 - INFO - Epoch 1, Batch 18: Training loss = 0.8300
2025-04-03 18:20:03,358 - INFO - Epoch 1, Batch 19: Training loss = 0.8200
2025-04-03 18:20:03,459 - INFO - Epoch 1, Batch 20: Training loss = 0.8100
2025-04-03 18:20:03,559 - INFO - Epoch 1, Batch 21: Training loss = 0.8000
2025-04-03 18:20:03,660 - INFO - Epoch 1, Batch 22: Training loss = 0.7900
2025-04-03 18:20:03,761 - INFO - Epoch 1, Batch 23: Training loss = 0.7800
2025-04-03 18:20:03,861 - INFO - Epoch 1, Batch 24: Training loss = 0.7700
2025-04-03 18:20:03,962 - INFO - Epoch 1, Batch 25: Training loss = 0.7600
2025-04-03 18:20:04,063 - INFO - Epoch 1, Batch 26: Training loss = 0.7500
2025-04-03 18:20:04,164 - INFO - Epoch 1, Batch 27: Training loss = 0.7400
2025-04-03 18:20:04,265 - INFO - Epoch 1, Batch 28: Training loss = 0.7300
2025-04-03 18:20:04,365 - INFO - Epoch 1, Batch 29: Training loss = 0.7200
2025-04-03 18:20:04,466 - INFO - Epoch 1, Batch 30: Training loss = 0.7100
2025-04-03 18:20:04,567 - INFO - Epoch 1, Batch 31: Training loss = 0.7000
2025-04-03 18:20:04,668 - INFO - Epoch 1, Batch 32: Training loss = 0.6900
2025-04-03 18:20:04,769 - INFO - Epoch 1, Batch 33: Training loss = 0.6800
2025-04-03 18:20:04,869 - INFO - Epoch 1, Batch 34: Training loss = 0.6700
2025-04-03 18:20:04,970 - INFO - Epoch 1, Batch 35: Training loss = 0.6600
2025-04-03 18:20:05,070 - INFO - Epoch 1, Batch 36: Training loss = 0.6500
2025-04-03 18:20:05,171 - INFO - Epoch 1, Batch 37: Training loss = 0.6400
2025-04-03 18:20:05,272 - INFO - Epoch 1, Batch 38: Training loss = 0.6300
2025-04-03 18:20:05,372 - INFO - Epoch 1, Batch 39: Training loss = 0.6200
2025-04-03 18:20:05,473 - INFO - Epoch 1, Batch 40: Training loss = 0.6100
2025-04-03 18:20:05,574 - INFO - Epoch 1, Batch 41: Training loss = 0.6000
2025-04-03 18:20:05,675 - INFO - Epoch 1, Batch 42: Training loss = 0.5900
2025-04-03 18:20:05,775 - INFO - Epoch 1, Batch 43: Training loss = 0.5800
2025-04-03 18:20:05,876 - INFO - Epoch 1, Batch 44: Training loss = 0.5700
2025-04-03 18:20:05,977 - INFO - Epoch 1, Batch 45: Training loss = 0.5600
2025-04-03 18:20:06,077 - INFO - Epoch 1, Batch 46: Training loss = 0.5500
2025-04-03 18:20:06,178 - INFO - Epoch 1, Batch 47: Training loss = 0.5400
2025-04-03 18:20:06,279 - INFO - Epoch 1, Batch 48: Training loss = 0.5300
2025-04-03 18:20:06,379 - INFO - Epoch 1, Batch 49: Training loss = 0.5200
2025-04-03 18:20:06,480 - INFO - Epoch 1, Batch 50: Training loss = 0.5100
2025-04-03 18:20:06,581 - INFO - Epoch 1, Batch 51: Training loss = 0.5000
2025-04-03 18:20:06,681 - INFO - Epoch 1, Batch 52: Training loss = 0.4900
2025-04-03 18:20:06,782 - INFO - Epoch 1, Batch 53: Training loss = 0.4800
2025-04-03 18:20:06,882 - INFO - Epoch 1, Batch 54: Training loss = 0.4700
2025-04-03 18:20:06,983 - INFO - Epoch 1, Batch 55: Training loss = 0.4600
2025-04-03 18:20:07,084 - INFO - Epoch 1, Batch 56: Training loss = 0.4500
2025-04-03 18:20:07,184 - INFO - Epoch 1, Batch 57: Training loss = 0.4400
2025-04-03 18:20:07,285 - INFO - Epoch 1, Batch 58: Training loss = 0.4300
2025-04-03 18:20:07,386 - INFO - Epoch 1, Batch 59: Training loss = 0.4200
2025-04-03 18:20:07,486 - INFO - Epoch 1, Batch 60: Training loss = 0.4100
2025-04-03 18:20:07,585 - INFO - Epoch 1, Batch 61: Training loss = 0.4000
2025-04-03 18:20:07,686 - INFO - Epoch 1, Batch 62: Training loss = 0.3900
2025-04-03 18:20:07,787 - INFO - Epoch 1, Batch 63: Training loss = 0.3800
2025-04-03 18:20:07,887 - INFO - Epoch 1, Batch 64: Training loss = 0.3700
2025-04-03 18:20:07,988 - INFO - Epoch 1, Batch 65: Training loss = 0.3600
2025-04-03 18:20:08,089 - INFO - Epoch 1, Batch 66: Training loss = 0.3500
2025-04-03 18:20:08,190 - INFO - Epoch 1, Batch 67: Training loss = 0.3400
2025-04-03 18:20:08,290 - INFO - Epoch 1, Batch 68: Training loss = 0.3300
2025-04-03 18:20:08,391 - INFO - Epoch 1, Batch 69: Training loss = 0.3200
2025-04-03 18:20:08,492 - INFO - Epoch 1, Batch 70: Training loss = 0.3100
2025-04-03 18:20:08,593 - INFO - Epoch 1, Batch 71: Training loss = 0.3000
2025-04-03 18:20:08,693 - INFO - Epoch 1, Batch 72: Training loss = 0.2900
2025-04-03 18:20:08,794 - INFO - Epoch 1, Batch 73: Training loss = 0.2800
2025-04-03 18:20:08,894 - INFO - Epoch 1, Batch 74: Training loss = 0.2700
2025-04-03 18:20:08,995 - INFO - Epoch 1, Batch 75: Training loss = 0.2600
2025-04-03 18:20:09,095 - INFO - Epoch 1, Batch 76: Training loss = 0.2500
2025-04-03 18:20:09,196 - INFO - Epoch 1, Batch 77: Training loss = 0.2400
2025-04-03 18:20:09,297 - INFO - Epoch 1, Batch 78: Training loss = 0.2300
2025-04-03 18:20:09,398 - INFO - Epoch 1, Batch 79: Training loss = 0.2200
2025-04-03 18:20:09,498 - INFO - Epoch 1, Batch 80: Training loss = 0.2100
2025-04-03 18:20:09,599 - INFO - Epoch 1, Batch 81: Training loss = 0.2000
2025-04-03 18:20:09,699 - INFO - Epoch 1, Batch 82: Training loss = 0.1900
2025-04-03 18:20:09,800 - INFO - Epoch 1, Batch 83: Training loss = 0.1800
2025-04-03 18:20:09,900 - INFO - Epoch 1, Batch 84: Training loss = 0.1700
2025-04-03 18:20:10,000 - INFO - Epoch 1, Batch 85: Training loss = 0.1600
2025-04-03 18:20:10,101 - INFO - Epoch 1, Batch 86: Training loss = 0.1500
2025-04-03 18:20:10,202 - INFO - Epoch 1, Batch 87: Training loss = 0.1400
2025-04-03 18:20:10,302 - INFO - Epoch 1, Batch 88: Training loss = 0.1300
2025-04-03 18:20:10,403 - INFO - Epoch 1, Batch 89: Training loss = 0.1200
2025-04-03 18:20:10,504 - INFO - Epoch 1, Batch 90: Training loss = 0.1100
2025-04-03 18:20:10,604 - INFO - Epoch 1, Batch 91: Training loss = 0.1000
2025-04-03 18:20:10,705 - INFO - Epoch 1, Batch 92: Training loss = 0.0900
2025-04-03 18:20:10,806 - INFO - Epoch 1, Batch 93: Training loss = 0.0800
2025-04-03 18:20:10,907 - INFO - Epoch 1, Batch 94: Training loss = 0.0700
2025-04-03 18:20:11,007 - INFO - Epoch 1, Batch 95: Training loss = 0.0600
2025-04-03 18:20:11,108 - INFO - Epoch 1, Batch 96: Training loss = 0.0500
2025-04-03 18:20:11,208 - INFO - Epoch 1, Batch 97: Training loss = 0.0400
2025-04-03 18:20:11,310 - INFO - Epoch 1, Batch 98: Training loss = 0.0300
2025-04-03 18:20:11,410 - INFO - Epoch 1, Batch 99: Training loss = 0.0200
2025-04-03 18:20:11,511 - INFO - Epoch 1, Batch 100: Training loss = 0.0100
2025-04-03 18:20:11,612 - INFO - Epoch 1: Validation loss = 0.1000, Validation accuracy = 0.8000
2025-04-03 18:20:11,612 - INFO - Epoch 1: MSE = 0.1000, RMSE = 0.3162, MAE = 0.0800, RÂ² = 0.9000
2025-04-03 18:20:11,612 - INFO - Epoch 2 started.
2025-04-03 18:20:11,612 - INFO - Epoch 2, Batch 1: Training loss = 1.0000
2025-04-03 18:20:11,713 - INFO - Epoch 2, Batch 2: Training loss = 0.9900
2025-04-03 18:20:11,813 - INFO - Epoch 2, Batch 3: Training loss = 0.9800
2025-04-03 18:20:11,914 - INFO - Epoch 2, Batch 4: Training loss = 0.9700
2025-04-03 18:20:12,015 - INFO - Epoch 2, Batch 5: Training loss = 0.9600
2025-04-03 18:20:12,116 - INFO - Epoch 2, Batch 6: Training loss = 0.9500
2025-04-03 18:20:12,217 - INFO - Epoch 2, Batch 7: Training loss = 0.9400
2025-04-03 18:20:12,318 - INFO - Epoch 2, Batch 8: Training loss = 0.9300
2025-04-03 18:20:12,418 - INFO - Epoch 2, Batch 9: Training loss = 0.9200
2025-04-03 18:20:12,519 - INFO - Epoch 2, Batch 10: Training loss = 0.9100
2025-04-03 18:20:12,620 - INFO - Epoch 2, Batch 11: Training loss = 0.9000
2025-04-03 18:20:12,720 - INFO - Epoch 2, Batch 12: Training loss = 0.8900
2025-04-03 18:20:12,821 - INFO - Epoch 2, Batch 13: Training loss = 0.8800
2025-04-03 18:20:12,922 - INFO - Epoch 2, Batch 14: Training loss = 0.8700
2025-04-03 18:20:13,023 - INFO - Epoch 2, Batch 15: Training loss = 0.8600
2025-04-03 18:20:13,123 - INFO - Epoch 2, Batch 16: Training loss = 0.8500
2025-04-03 18:20:13,224 - INFO - Epoch 2, Batch 17: Training loss = 0.8400
2025-04-03 18:20:13,325 - INFO - Epoch 2, Batch 18: Training loss = 0.8300
2025-04-03 18:20:13,425 - INFO - Epoch 2, Batch 19: Training loss = 0.8200
2025-04-03 18:20:13,526 - INFO - Epoch 2, Batch 20: Training loss = 0.8100
2025-04-03 18:20:13,626 - INFO - Epoch 2, Batch 21: Training loss = 0.8000
2025-04-03 18:20:13,727 - INFO - Epoch 2, Batch 22: Training loss = 0.7900
2025-04-03 18:20:13,828 - INFO - Epoch 2, Batch 23: Training loss = 0.7800
2025-04-03 18:20:13,928 - INFO - Epoch 2, Batch 24: Training loss = 0.7700
2025-04-03 18:20:14,028 - INFO - Epoch 2, Batch 25: Training loss = 0.7600
2025-04-03 18:20:14,129 - INFO - Epoch 2, Batch 26: Training loss = 0.7500
2025-04-03 18:20:14,229 - INFO - Epoch 2, Batch 27: Training loss = 0.7400
2025-04-03 18:20:14,330 - INFO - Epoch 2, Batch 28: Training loss = 0.7300
2025-04-03 18:20:14,431 - INFO - Epoch 2, Batch 29: Training loss = 0.7200
2025-04-03 18:20:14,531 - INFO - Epoch 2, Batch 30: Training loss = 0.7100
2025-04-03 18:20:14,632 - INFO - Epoch 2, Batch 31: Training loss = 0.7000
2025-04-03 18:20:14,733 - INFO - Epoch 2, Batch 32: Training loss = 0.6900
2025-04-03 18:20:14,834 - INFO - Epoch 2, Batch 33: Training loss = 0.6800
2025-04-03 18:20:14,934 - INFO - Epoch 2, Batch 34: Training loss = 0.6700
2025-04-03 18:20:15,035 - INFO - Epoch 2, Batch 35: Training loss = 0.6600
2025-04-03 18:20:15,135 - INFO - Epoch 2, Batch 36: Training loss = 0.6500
2025-04-03 18:20:15,236 - INFO - Epoch 2, Batch 37: Training loss = 0.6400
2025-04-03 18:20:15,336 - INFO - Epoch 2, Batch 38: Training loss = 0.6300
2025-04-03 18:20:15,437 - INFO - Epoch 2, Batch 39: Training loss = 0.6200
2025-04-03 18:20:15,537 - INFO - Epoch 2, Batch 40: Training loss = 0.6100
2025-04-03 18:20:15,638 - INFO - Epoch 2, Batch 41: Training loss = 0.6000
2025-04-03 18:20:15,738 - INFO - Epoch 2, Batch 42: Training loss = 0.5900
2025-04-03 18:20:15,839 - INFO - Epoch 2, Batch 43: Training loss = 0.5800
2025-04-03 18:20:15,939 - INFO - Epoch 2, Batch 44: Training loss = 0.5700
2025-04-03 18:20:16,040 - INFO - Epoch 2, Batch 45: Training loss = 0.5600
2025-04-03 18:20:16,140 - INFO - Epoch 2, Batch 46: Training loss = 0.5500
2025-04-03 18:20:16,241 - INFO - Epoch 2, Batch 47: Training loss = 0.5400
2025-04-03 18:20:16,341 - INFO - Epoch 2, Batch 48: Training loss = 0.5300
2025-04-03 18:20:16,442 - INFO - Epoch 2, Batch 49: Training loss = 0.5200
2025-04-03 18:20:16,542 - INFO - Epoch 2, Batch 50: Training loss = 0.5100
2025-04-03 18:20:16,643 - INFO - Epoch 2, Batch 51: Training loss = 0.5000
2025-04-03 18:20:16,743 - INFO - Epoch 2, Batch 52: Training loss = 0.4900
2025-04-03 18:20:16,844 - INFO - Epoch 2, Batch 53: Training loss = 0.4800
2025-04-03 18:20:16,944 - INFO - Epoch 2, Batch 54: Training loss = 0.4700
2025-04-03 18:20:17,045 - INFO - Epoch 2, Batch 55: Training loss = 0.4600
2025-04-03 18:20:17,145 - INFO - Epoch 2, Batch 56: Training loss = 0.4500
2025-04-03 18:20:17,246 - INFO - Epoch 2, Batch 57: Training loss = 0.4400
2025-04-03 18:20:17,346 - INFO - Epoch 2, Batch 58: Training loss = 0.4300
2025-04-03 18:20:17,447 - INFO - Epoch 2, Batch 59: Training loss = 0.4200
2025-04-03 18:20:17,547 - INFO - Epoch 2, Batch 60: Training loss = 0.4100
2025-04-03 18:20:17,648 - INFO - Epoch 2, Batch 61: Training loss = 0.4000
2025-04-03 18:20:17,748 - INFO - Epoch 2, Batch 62: Training loss = 0.3900
2025-04-03 18:20:17,849 - INFO - Epoch 2, Batch 63: Training loss = 0.3800
2025-04-03 18:20:17,949 - INFO - Epoch 2, Batch 64: Training loss = 0.3700
2025-04-03 18:20:18,050 - INFO - Epoch 2, Batch 65: Training loss = 0.3600
2025-04-03 18:20:18,151 - INFO - Epoch 2, Batch 66: Training loss = 0.3500
2025-04-03 18:20:18,251 - INFO - Epoch 2, Batch 67: Training loss = 0.3400
2025-04-03 18:20:18,352 - INFO - Epoch 2, Batch 68: Training loss = 0.3300
2025-04-03 18:20:18,453 - INFO - Epoch 2, Batch 69: Training loss = 0.3200
2025-04-03 18:20:18,553 - INFO - Epoch 2, Batch 70: Training loss = 0.3100
2025-04-03 18:20:18,654 - INFO - Epoch 2, Batch 71: Training loss = 0.3000
2025-04-03 18:20:18,754 - INFO - Epoch 2, Batch 72: Training loss = 0.2900
2025-04-03 18:20:18,855 - INFO - Epoch 2, Batch 73: Training loss = 0.2800
2025-04-03 18:20:18,956 - INFO - Epoch 2, Batch 74: Training loss = 0.2700
2025-04-03 18:20:19,057 - INFO - Epoch 2, Batch 75: Training loss = 0.2600
2025-04-03 18:20:19,158 - INFO - Epoch 2, Batch 76: Training loss = 0.2500
2025-04-03 18:20:19,258 - INFO - Epoch 2, Batch 77: Training loss = 0.2400
2025-04-03 18:20:19,360 - INFO - Epoch 2, Batch 78: Training loss = 0.2300
2025-04-03 18:20:19,461 - INFO - Epoch 2, Batch 79: Training loss = 0.2200
2025-04-03 18:20:19,562 - INFO - Epoch 2, Batch 80: Training loss = 0.2100
2025-04-03 18:20:19,662 - INFO - Epoch 2, Batch 81: Training loss = 0.2000
2025-04-03 18:20:19,763 - INFO - Epoch 2, Batch 82: Training loss = 0.1900
2025-04-03 18:20:19,864 - INFO - Epoch 2, Batch 83: Training loss = 0.1800
2025-04-03 18:20:19,964 - INFO - Epoch 2, Batch 84: Training loss = 0.1700
2025-04-03 18:20:20,065 - INFO - Epoch 2, Batch 85: Training loss = 0.1600
2025-04-03 18:20:20,166 - INFO - Epoch 2, Batch 86: Training loss = 0.1500
2025-04-03 18:20:20,267 - INFO - Epoch 2, Batch 87: Training loss = 0.1400
2025-04-03 18:20:20,368 - INFO - Epoch 2, Batch 88: Training loss = 0.1300
2025-04-03 18:20:20,468 - INFO - Epoch 2, Batch 89: Training loss = 0.1200
2025-04-03 18:20:20,569 - INFO - Epoch 2, Batch 90: Training loss = 0.1100
2025-04-03 18:20:20,670 - INFO - Epoch 2, Batch 91: Training loss = 0.1000
2025-04-03 18:20:20,771 - INFO - Epoch 2, Batch 92: Training loss = 0.0900
2025-04-03 18:20:20,872 - INFO - Epoch 2, Batch 93: Training loss = 0.0800
2025-04-03 18:20:20,973 - INFO - Epoch 2, Batch 94: Training loss = 0.0700
2025-04-03 18:20:21,073 - INFO - Epoch 2, Batch 95: Training loss = 0.0600
2025-04-03 18:20:21,174 - INFO - Epoch 2, Batch 96: Training loss = 0.0500
2025-04-03 18:20:21,274 - INFO - Epoch 2, Batch 97: Training loss = 0.0400
2025-04-03 18:20:21,375 - INFO - Epoch 2, Batch 98: Training loss = 0.0300
2025-04-03 18:20:21,475 - INFO - Epoch 2, Batch 99: Training loss = 0.0200
2025-04-03 18:20:21,575 - INFO - Epoch 2, Batch 100: Training loss = 0.0100
2025-04-03 18:20:21,676 - INFO - Epoch 2: Validation loss = 0.0900, Validation accuracy = 0.8200
2025-04-03 18:20:21,676 - INFO - Epoch 2: MSE = 0.0900, RMSE = 0.3000, MAE = 0.0720, RÂ² = 0.9100
2025-04-03 18:20:21,676 - INFO - Epoch 3 started.
2025-04-03 18:20:21,676 - INFO - Epoch 3, Batch 1: Training loss = 1.0000
2025-04-03 18:20:21,776 - INFO - Epoch 3, Batch 2: Training loss = 0.9900
2025-04-03 18:20:21,877 - INFO - Epoch 3, Batch 3: Training loss = 0.9800
2025-04-03 18:20:21,977 - INFO - Epoch 3, Batch 4: Training loss = 0.9700
2025-04-03 18:20:22,078 - INFO - Epoch 3, Batch 5: Training loss = 0.9600
2025-04-03 18:20:22,178 - INFO - Epoch 3, Batch 6: Training loss = 0.9500
2025-04-03 18:20:22,279 - INFO - Epoch 3, Batch 7: Training loss = 0.9400
2025-04-03 18:20:22,380 - INFO - Epoch 3, Batch 8: Training loss = 0.9300
2025-04-03 18:20:22,481 - INFO - Epoch 3, Batch 9: Training loss = 0.9200
2025-04-03 18:20:22,581 - INFO - Epoch 3, Batch 10: Training loss = 0.9100
2025-04-03 18:20:22,682 - INFO - Epoch 3, Batch 11: Training loss = 0.9000
2025-04-03 18:20:22,782 - INFO - Epoch 3, Batch 12: Training loss = 0.8900
2025-04-03 18:20:22,883 - INFO - Epoch 3, Batch 13: Training loss = 0.8800
2025-04-03 18:20:22,984 - INFO - Epoch 3, Batch 14: Training loss = 0.8700
2025-04-03 18:20:23,084 - INFO - Epoch 3, Batch 15: Training loss = 0.8600
2025-04-03 18:20:23,185 - INFO - Epoch 3, Batch 16: Training loss = 0.8500
2025-04-03 18:20:23,285 - INFO - Epoch 3, Batch 17: Training loss = 0.8400
2025-04-03 18:20:23,386 - INFO - Epoch 3, Batch 18: Training loss = 0.8300
2025-04-03 18:20:23,487 - INFO - Epoch 3, Batch 19: Training loss = 0.8200
2025-04-03 18:20:23,587 - INFO - Epoch 3, Batch 20: Training loss = 0.8100
2025-04-03 18:20:23,688 - INFO - Epoch 3, Batch 21: Training loss = 0.8000
2025-04-03 18:20:23,788 - INFO - Epoch 3, Batch 22: Training loss = 0.7900
2025-04-03 18:20:23,889 - INFO - Epoch 3, Batch 23: Training loss = 0.7800
2025-04-03 18:20:23,990 - INFO - Epoch 3, Batch 24: Training loss = 0.7700
2025-04-03 18:20:24,090 - INFO - Epoch 3, Batch 25: Training loss = 0.7600
2025-04-03 18:20:24,191 - INFO - Epoch 3, Batch 26: Training loss = 0.7500
2025-04-03 18:20:24,292 - INFO - Epoch 3, Batch 27: Training loss = 0.7400
2025-04-03 18:20:24,393 - INFO - Epoch 3, Batch 28: Training loss = 0.7300
2025-04-03 18:20:24,495 - INFO - Epoch 3, Batch 29: Training loss = 0.7200
2025-04-03 18:20:24,596 - INFO - Epoch 3, Batch 30: Training loss = 0.7100
2025-04-03 18:20:24,697 - INFO - Epoch 3, Batch 31: Training loss = 0.7000
2025-04-03 18:20:24,798 - INFO - Epoch 3, Batch 32: Training loss = 0.6900
2025-04-03 18:20:24,898 - INFO - Epoch 3, Batch 33: Training loss = 0.6800
2025-04-03 18:20:24,999 - INFO - Epoch 3, Batch 34: Training loss = 0.6700
2025-04-03 18:20:25,099 - INFO - Epoch 3, Batch 35: Training loss = 0.6600
2025-04-03 18:20:25,199 - INFO - Epoch 3, Batch 36: Training loss = 0.6500
2025-04-03 18:20:25,300 - INFO - Epoch 3, Batch 37: Training loss = 0.6400
2025-04-03 18:20:25,401 - INFO - Epoch 3, Batch 38: Training loss = 0.6300
2025-04-03 18:20:25,502 - INFO - Epoch 3, Batch 39: Training loss = 0.6200
2025-04-03 18:20:25,603 - INFO - Epoch 3, Batch 40: Training loss = 0.6100
2025-04-03 18:20:25,703 - INFO - Epoch 3, Batch 41: Training loss = 0.6000
2025-04-03 18:20:25,804 - INFO - Epoch 3, Batch 42: Training loss = 0.5900
2025-04-03 18:20:25,905 - INFO - Epoch 3, Batch 43: Training loss = 0.5800
2025-04-03 18:20:26,006 - INFO - Epoch 3, Batch 44: Training loss = 0.5700
2025-04-03 18:20:26,106 - INFO - Epoch 3, Batch 45: Training loss = 0.5600
2025-04-03 18:20:26,207 - INFO - Epoch 3, Batch 46: Training loss = 0.5500
2025-04-03 18:20:26,308 - INFO - Epoch 3, Batch 47: Training loss = 0.5400
2025-04-03 18:20:26,409 - INFO - Epoch 3, Batch 48: Training loss = 0.5300
2025-04-03 18:20:26,510 - INFO - Epoch 3, Batch 49: Training loss = 0.5200
2025-04-03 18:20:26,610 - INFO - Epoch 3, Batch 50: Training loss = 0.5100
2025-04-03 18:20:26,711 - INFO - Epoch 3, Batch 51: Training loss = 0.5000
2025-04-03 18:20:26,812 - INFO - Epoch 3, Batch 52: Training loss = 0.4900
2025-04-03 18:20:26,912 - INFO - Epoch 3, Batch 53: Training loss = 0.4800
2025-04-03 18:20:27,013 - INFO - Epoch 3, Batch 54: Training loss = 0.4700
2025-04-03 18:20:27,114 - INFO - Epoch 3, Batch 55: Training loss = 0.4600
2025-04-03 18:20:27,214 - INFO - Epoch 3, Batch 56: Training loss = 0.4500
2025-04-03 18:20:27,315 - INFO - Epoch 3, Batch 57: Training loss = 0.4400
2025-04-03 18:20:27,415 - INFO - Epoch 3, Batch 58: Training loss = 0.4300
2025-04-03 18:20:27,516 - INFO - Epoch 3, Batch 59: Training loss = 0.4200
2025-04-03 18:20:27,617 - INFO - Epoch 3, Batch 60: Training loss = 0.4100
2025-04-03 18:20:27,718 - INFO - Epoch 3, Batch 61: Training loss = 0.4000
2025-04-03 18:20:27,819 - INFO - Epoch 3, Batch 62: Training loss = 0.3900
2025-04-03 18:20:27,920 - INFO - Epoch 3, Batch 63: Training loss = 0.3800
2025-04-03 18:20:28,020 - INFO - Epoch 3, Batch 64: Training loss = 0.3700
2025-04-03 18:20:28,121 - INFO - Epoch 3, Batch 65: Training loss = 0.3600
2025-04-03 18:20:28,221 - INFO - Epoch 3, Batch 66: Training loss = 0.3500
2025-04-03 18:20:28,322 - INFO - Epoch 3, Batch 67: Training loss = 0.3400
2025-04-03 18:20:28,423 - INFO - Epoch 3, Batch 68: Training loss = 0.3300
2025-04-03 18:20:28,524 - INFO - Epoch 3, Batch 69: Training loss = 0.3200
2025-04-03 18:20:28,625 - INFO - Epoch 3, Batch 70: Training loss = 0.3100
2025-04-03 18:20:28,726 - INFO - Epoch 3, Batch 71: Training loss = 0.3000
2025-04-03 18:20:28,827 - INFO - Epoch 3, Batch 72: Training loss = 0.2900
2025-04-03 18:20:28,927 - INFO - Epoch 3, Batch 73: Training loss = 0.2800
2025-04-03 18:20:29,028 - INFO - Epoch 3, Batch 74: Training loss = 0.2700
2025-04-03 18:20:29,129 - INFO - Epoch 3, Batch 75: Training loss = 0.2600
2025-04-03 18:20:29,229 - INFO - Epoch 3, Batch 76: Training loss = 0.2500
2025-04-03 18:20:29,330 - INFO - Epoch 3, Batch 77: Training loss = 0.2400
2025-04-03 18:20:29,430 - INFO - Epoch 3, Batch 78: Training loss = 0.2300
2025-04-03 18:20:29,531 - INFO - Epoch 3, Batch 79: Training loss = 0.2200
2025-04-03 18:20:29,632 - INFO - Epoch 3, Batch 80: Training loss = 0.2100
2025-04-03 18:20:29,733 - INFO - Epoch 3, Batch 81: Training loss = 0.2000
2025-04-03 18:20:29,833 - INFO - Epoch 3, Batch 82: Training loss = 0.1900
2025-04-03 18:20:29,934 - INFO - Epoch 3, Batch 83: Training loss = 0.1800
2025-04-03 18:20:30,035 - INFO - Epoch 3, Batch 84: Training loss = 0.1700
2025-04-03 18:20:30,136 - INFO - Epoch 3, Batch 85: Training loss = 0.1600
2025-04-03 18:20:30,237 - INFO - Epoch 3, Batch 86: Training loss = 0.1500
2025-04-03 18:20:30,337 - INFO - Epoch 3, Batch 87: Training loss = 0.1400
2025-04-03 18:20:30,438 - INFO - Epoch 3, Batch 88: Training loss = 0.1300
2025-04-03 18:20:30,539 - INFO - Epoch 3, Batch 89: Training loss = 0.1200
2025-04-03 18:20:30,640 - INFO - Epoch 3, Batch 90: Training loss = 0.1100
2025-04-03 18:20:30,741 - INFO - Epoch 3, Batch 91: Training loss = 0.1000
2025-04-03 18:20:30,842 - INFO - Epoch 3, Batch 92: Training loss = 0.0900
2025-04-03 18:20:30,942 - INFO - Epoch 3, Batch 93: Training loss = 0.0800
2025-04-03 18:20:31,043 - INFO - Epoch 3, Batch 94: Training loss = 0.0700
2025-04-03 18:20:31,144 - INFO - Epoch 3, Batch 95: Training loss = 0.0600
2025-04-03 18:20:31,245 - INFO - Epoch 3, Batch 96: Training loss = 0.0500
2025-04-03 18:20:31,346 - INFO - Epoch 3, Batch 97: Training loss = 0.0400
2025-04-03 18:20:31,447 - INFO - Epoch 3, Batch 98: Training loss = 0.0300
2025-04-03 18:20:31,548 - INFO - Epoch 3, Batch 99: Training loss = 0.0200
2025-04-03 18:20:31,649 - INFO - Epoch 3, Batch 100: Training loss = 0.0100
2025-04-03 18:20:31,750 - INFO - Epoch 3: Validation loss = 0.0800, Validation accuracy = 0.8400
2025-04-03 18:20:31,750 - INFO - Epoch 3: MSE = 0.0800, RMSE = 0.2828, MAE = 0.0640, RÂ² = 0.9200
2025-04-03 18:20:31,751 - INFO - Epoch 4 started.
2025-04-03 18:20:31,751 - INFO - Epoch 4, Batch 1: Training loss = 1.0000
2025-04-03 18:20:31,851 - INFO - Epoch 4, Batch 2: Training loss = 0.9900
2025-04-03 18:20:31,952 - INFO - Epoch 4, Batch 3: Training loss = 0.9800
2025-04-03 18:20:32,053 - INFO - Epoch 4, Batch 4: Training loss = 0.9700
2025-04-03 18:20:32,153 - INFO - Epoch 4, Batch 5: Training loss = 0.9600
2025-04-03 18:20:32,254 - INFO - Epoch 4, Batch 6: Training loss = 0.9500
2025-04-03 18:20:32,355 - INFO - Epoch 4, Batch 7: Training loss = 0.9400
2025-04-03 18:20:32,455 - INFO - Epoch 4, Batch 8: Training loss = 0.9300
2025-04-03 18:20:32,556 - INFO - Epoch 4, Batch 9: Training loss = 0.9200
2025-04-03 18:20:32,657 - INFO - Epoch 4, Batch 10: Training loss = 0.9100
2025-04-03 18:20:32,757 - INFO - Epoch 4, Batch 11: Training loss = 0.9000
2025-04-03 18:20:32,858 - INFO - Epoch 4, Batch 12: Training loss = 0.8900
2025-04-03 18:20:32,959 - INFO - Epoch 4, Batch 13: Training loss = 0.8800
2025-04-03 18:20:33,060 - INFO - Epoch 4, Batch 14: Training loss = 0.8700
2025-04-03 18:20:33,161 - INFO - Epoch 4, Batch 15: Training loss = 0.8600
2025-04-03 18:20:33,262 - INFO - Epoch 4, Batch 16: Training loss = 0.8500
2025-04-03 18:20:33,363 - INFO - Epoch 4, Batch 17: Training loss = 0.8400
2025-04-03 18:20:33,464 - INFO - Epoch 4, Batch 18: Training loss = 0.8300
2025-04-03 18:20:33,565 - INFO - Epoch 4, Batch 19: Training loss = 0.8200
2025-04-03 18:20:33,666 - INFO - Epoch 4, Batch 20: Training loss = 0.8100
2025-04-03 18:20:33,768 - INFO - Epoch 4, Batch 21: Training loss = 0.8000
2025-04-03 18:20:33,868 - INFO - Epoch 4, Batch 22: Training loss = 0.7900
2025-04-03 18:20:33,969 - INFO - Epoch 4, Batch 23: Training loss = 0.7800
2025-04-03 18:20:34,070 - INFO - Epoch 4, Batch 24: Training loss = 0.7700
2025-04-03 18:20:34,171 - INFO - Epoch 4, Batch 25: Training loss = 0.7600
2025-04-03 18:20:34,272 - INFO - Epoch 4, Batch 26: Training loss = 0.7500
2025-04-03 18:20:34,372 - INFO - Epoch 4, Batch 27: Training loss = 0.7400
2025-04-03 18:20:34,473 - INFO - Epoch 4, Batch 28: Training loss = 0.7300
2025-04-03 18:20:34,574 - INFO - Epoch 4, Batch 29: Training loss = 0.7200
2025-04-03 18:20:34,675 - INFO - Epoch 4, Batch 30: Training loss = 0.7100
2025-04-03 18:20:34,776 - INFO - Epoch 4, Batch 31: Training loss = 0.7000
2025-04-03 18:20:34,877 - INFO - Epoch 4, Batch 32: Training loss = 0.6900
2025-04-03 18:20:34,978 - INFO - Epoch 4, Batch 33: Training loss = 0.6800
2025-04-03 18:20:35,079 - INFO - Epoch 4, Batch 34: Training loss = 0.6700
2025-04-03 18:20:35,180 - INFO - Epoch 4, Batch 35: Training loss = 0.6600
2025-04-03 18:20:35,281 - INFO - Epoch 4, Batch 36: Training loss = 0.6500
2025-04-03 18:20:35,382 - INFO - Epoch 4, Batch 37: Training loss = 0.6400
2025-04-03 18:20:35,483 - INFO - Epoch 4, Batch 38: Training loss = 0.6300
2025-04-03 18:20:35,584 - INFO - Epoch 4, Batch 39: Training loss = 0.6200
2025-04-03 18:20:35,685 - INFO - Epoch 4, Batch 40: Training loss = 0.6100
2025-04-03 18:20:35,787 - INFO - Epoch 4, Batch 41: Training loss = 0.6000
2025-04-03 18:20:35,888 - INFO - Epoch 4, Batch 42: Training loss = 0.5900
2025-04-03 18:20:35,988 - INFO - Epoch 4, Batch 43: Training loss = 0.5800
2025-04-03 18:20:36,089 - INFO - Epoch 4, Batch 44: Training loss = 0.5700
2025-04-03 18:20:36,190 - INFO - Epoch 4, Batch 45: Training loss = 0.5600
2025-04-03 18:20:36,290 - INFO - Epoch 4, Batch 46: Training loss = 0.5500
2025-04-03 18:20:36,391 - INFO - Epoch 4, Batch 47: Training loss = 0.5400
2025-04-03 18:20:36,491 - INFO - Epoch 4, Batch 48: Training loss = 0.5300
2025-04-03 18:20:36,593 - INFO - Epoch 4, Batch 49: Training loss = 0.5200
2025-04-03 18:20:36,694 - INFO - Epoch 4, Batch 50: Training loss = 0.5100
2025-04-03 18:20:36,795 - INFO - Epoch 4, Batch 51: Training loss = 0.5000
2025-04-03 18:20:36,896 - INFO - Epoch 4, Batch 52: Training loss = 0.4900
2025-04-03 18:20:36,997 - INFO - Epoch 4, Batch 53: Training loss = 0.4800
2025-04-03 18:20:37,098 - INFO - Epoch 4, Batch 54: Training loss = 0.4700
2025-04-03 18:20:37,199 - INFO - Epoch 4, Batch 55: Training loss = 0.4600
2025-04-03 18:20:37,300 - INFO - Epoch 4, Batch 56: Training loss = 0.4500
2025-04-03 18:20:37,401 - INFO - Epoch 4, Batch 57: Training loss = 0.4400
2025-04-03 18:20:37,500 - INFO - Epoch 4, Batch 58: Training loss = 0.4300
2025-04-03 18:20:37,600 - INFO - Epoch 4, Batch 59: Training loss = 0.4200
2025-04-03 18:20:37,701 - INFO - Epoch 4, Batch 60: Training loss = 0.4100
2025-04-03 18:20:37,803 - INFO - Epoch 4, Batch 61: Training loss = 0.4000
2025-04-03 18:20:37,904 - INFO - Epoch 4, Batch 62: Training loss = 0.3900
2025-04-03 18:20:38,005 - INFO - Epoch 4, Batch 63: Training loss = 0.3800
2025-04-03 18:20:38,106 - INFO - Epoch 4, Batch 64: Training loss = 0.3700
2025-04-03 18:20:38,206 - INFO - Epoch 4, Batch 65: Training loss = 0.3600
2025-04-03 18:20:38,307 - INFO - Epoch 4, Batch 66: Training loss = 0.3500
2025-04-03 18:20:38,408 - INFO - Epoch 4, Batch 67: Training loss = 0.3400
2025-04-03 18:20:38,508 - INFO - Epoch 4, Batch 68: Training loss = 0.3300
2025-04-03 18:20:38,609 - INFO - Epoch 4, Batch 69: Training loss = 0.3200
2025-04-03 18:20:38,710 - INFO - Epoch 4, Batch 70: Training loss = 0.3100
2025-04-03 18:20:38,811 - INFO - Epoch 4, Batch 71: Training loss = 0.3000
2025-04-03 18:20:38,912 - INFO - Epoch 4, Batch 72: Training loss = 0.2900
2025-04-03 18:20:39,012 - INFO - Epoch 4, Batch 73: Training loss = 0.2800
2025-04-03 18:20:39,113 - INFO - Epoch 4, Batch 74: Training loss = 0.2700
2025-04-03 18:20:39,214 - INFO - Epoch 4, Batch 75: Training loss = 0.2600
2025-04-03 18:20:39,315 - INFO - Epoch 4, Batch 76: Training loss = 0.2500
2025-04-03 18:20:39,416 - INFO - Epoch 4, Batch 77: Training loss = 0.2400
2025-04-03 18:20:39,517 - INFO - Epoch 4, Batch 78: Training loss = 0.2300
2025-04-03 18:20:39,618 - INFO - Epoch 4, Batch 79: Training loss = 0.2200
2025-04-03 18:20:39,719 - INFO - Epoch 4, Batch 80: Training loss = 0.2100
2025-04-03 18:20:39,820 - INFO - Epoch 4, Batch 81: Training loss = 0.2000
2025-04-03 18:20:39,921 - INFO - Epoch 4, Batch 82: Training loss = 0.1900
2025-04-03 18:20:40,022 - INFO - Epoch 4, Batch 83: Training loss = 0.1800
2025-04-03 18:20:40,123 - INFO - Epoch 4, Batch 84: Training loss = 0.1700
2025-04-03 18:20:40,224 - INFO - Epoch 4, Batch 85: Training loss = 0.1600
2025-04-03 18:20:40,325 - INFO - Epoch 4, Batch 86: Training loss = 0.1500
2025-04-03 18:20:40,426 - INFO - Epoch 4, Batch 87: Training loss = 0.1400
2025-04-03 18:20:40,526 - INFO - Epoch 4, Batch 88: Training loss = 0.1300
2025-04-03 18:20:40,627 - INFO - Epoch 4, Batch 89: Training loss = 0.1200
2025-04-03 18:20:40,728 - INFO - Epoch 4, Batch 90: Training loss = 0.1100
2025-04-03 18:20:40,828 - INFO - Epoch 4, Batch 91: Training loss = 0.1000
2025-04-03 18:20:40,929 - INFO - Epoch 4, Batch 92: Training loss = 0.0900
2025-04-03 18:20:41,030 - INFO - Epoch 4, Batch 93: Training loss = 0.0800
2025-04-03 18:20:41,131 - INFO - Epoch 4, Batch 94: Training loss = 0.0700
2025-04-03 18:20:41,231 - INFO - Epoch 4, Batch 95: Training loss = 0.0600
2025-04-03 18:20:41,332 - INFO - Epoch 4, Batch 96: Training loss = 0.0500
2025-04-03 18:20:41,432 - INFO - Epoch 4, Batch 97: Training loss = 0.0400
2025-04-03 18:20:41,533 - INFO - Epoch 4, Batch 98: Training loss = 0.0300
2025-04-03 18:20:41,634 - INFO - Epoch 4, Batch 99: Training loss = 0.0200
2025-04-03 18:20:41,735 - INFO - Epoch 4, Batch 100: Training loss = 0.0100
2025-04-03 18:20:41,836 - INFO - Epoch 4: Validation loss = 0.0700, Validation accuracy = 0.8600
2025-04-03 18:20:41,836 - INFO - Epoch 4: MSE = 0.0700, RMSE = 0.2646, MAE = 0.0560, RÂ² = 0.9300
2025-04-03 18:20:41,836 - INFO - Epoch 5 started.
2025-04-03 18:20:41,836 - INFO - Epoch 5, Batch 1: Training loss = 1.0000
2025-04-03 18:20:41,937 - INFO - Epoch 5, Batch 2: Training loss = 0.9900
2025-04-03 18:20:42,037 - INFO - Epoch 5, Batch 3: Training loss = 0.9800
2025-04-03 18:20:42,138 - INFO - Epoch 5, Batch 4: Training loss = 0.9700
2025-04-03 18:20:42,239 - INFO - Epoch 5, Batch 5: Training loss = 0.9600
2025-04-03 18:20:42,340 - INFO - Epoch 5, Batch 6: Training loss = 0.9500
2025-04-03 18:20:42,441 - INFO - Epoch 5, Batch 7: Training loss = 0.9400
2025-04-03 18:20:42,542 - INFO - Epoch 5, Batch 8: Training loss = 0.9300
2025-04-03 18:20:42,643 - INFO - Epoch 5, Batch 9: Training loss = 0.9200
2025-04-03 18:20:42,744 - INFO - Epoch 5, Batch 10: Training loss = 0.9100
2025-04-03 18:20:42,844 - INFO - Epoch 5, Batch 11: Training loss = 0.9000
2025-04-03 18:20:42,945 - INFO - Epoch 5, Batch 12: Training loss = 0.8900
2025-04-03 18:20:43,046 - INFO - Epoch 5, Batch 13: Training loss = 0.8800
2025-04-03 18:20:43,147 - INFO - Epoch 5, Batch 14: Training loss = 0.8700
2025-04-03 18:20:43,248 - INFO - Epoch 5, Batch 15: Training loss = 0.8600
2025-04-03 18:20:43,349 - INFO - Epoch 5, Batch 16: Training loss = 0.8500
2025-04-03 18:20:43,450 - INFO - Epoch 5, Batch 17: Training loss = 0.8400
2025-04-03 18:20:43,551 - INFO - Epoch 5, Batch 18: Training loss = 0.8300
2025-04-03 18:20:43,652 - INFO - Epoch 5, Batch 19: Training loss = 0.8200
2025-04-03 18:20:43,753 - INFO - Epoch 5, Batch 20: Training loss = 0.8100
2025-04-03 18:20:43,854 - INFO - Epoch 5, Batch 21: Training loss = 0.8000
2025-04-03 18:20:43,955 - INFO - Epoch 5, Batch 22: Training loss = 0.7900
2025-04-03 18:20:44,055 - INFO - Epoch 5, Batch 23: Training loss = 0.7800
2025-04-03 18:20:44,156 - INFO - Epoch 5, Batch 24: Training loss = 0.7700
2025-04-03 18:20:44,257 - INFO - Epoch 5, Batch 25: Training loss = 0.7600
2025-04-03 18:20:44,358 - INFO - Epoch 5, Batch 26: Training loss = 0.7500
2025-04-03 18:20:44,459 - INFO - Epoch 5, Batch 27: Training loss = 0.7400
2025-04-03 18:20:44,560 - INFO - Epoch 5, Batch 28: Training loss = 0.7300
2025-04-03 18:20:44,661 - INFO - Epoch 5, Batch 29: Training loss = 0.7200
2025-04-03 18:20:44,762 - INFO - Epoch 5, Batch 30: Training loss = 0.7100
2025-04-03 18:20:44,863 - INFO - Epoch 5, Batch 31: Training loss = 0.7000
2025-04-03 18:20:44,964 - INFO - Epoch 5, Batch 32: Training loss = 0.6900
2025-04-03 18:20:45,064 - INFO - Epoch 5, Batch 33: Training loss = 0.6800
2025-04-03 18:20:45,165 - INFO - Epoch 5, Batch 34: Training loss = 0.6700
2025-04-03 18:20:45,266 - INFO - Epoch 5, Batch 35: Training loss = 0.6600
2025-04-03 18:20:45,367 - INFO - Epoch 5, Batch 36: Training loss = 0.6500
2025-04-03 18:20:45,468 - INFO - Epoch 5, Batch 37: Training loss = 0.6400
2025-04-03 18:20:45,569 - INFO - Epoch 5, Batch 38: Training loss = 0.6300
2025-04-03 18:20:45,670 - INFO - Epoch 5, Batch 39: Training loss = 0.6200
2025-04-03 18:20:45,771 - INFO - Epoch 5, Batch 40: Training loss = 0.6100
2025-04-03 18:20:45,872 - INFO - Epoch 5, Batch 41: Training loss = 0.6000
2025-04-03 18:20:45,973 - INFO - Epoch 5, Batch 42: Training loss = 0.5900
2025-04-03 18:20:46,074 - INFO - Epoch 5, Batch 43: Training loss = 0.5800
2025-04-03 18:20:46,175 - INFO - Epoch 5, Batch 44: Training loss = 0.5700
2025-04-03 18:20:46,276 - INFO - Epoch 5, Batch 45: Training loss = 0.5600
2025-04-03 18:20:46,377 - INFO - Epoch 5, Batch 46: Training loss = 0.5500
2025-04-03 18:20:46,478 - INFO - Epoch 5, Batch 47: Training loss = 0.5400
2025-04-03 18:20:46,579 - INFO - Epoch 5, Batch 48: Training loss = 0.5300
2025-04-03 18:20:46,680 - INFO - Epoch 5, Batch 49: Training loss = 0.5200
2025-04-03 18:20:46,780 - INFO - Epoch 5, Batch 50: Training loss = 0.5100
2025-04-03 18:20:46,881 - INFO - Epoch 5, Batch 51: Training loss = 0.5000
2025-04-03 18:20:46,981 - INFO - Epoch 5, Batch 52: Training loss = 0.4900
2025-04-03 18:20:47,082 - INFO - Epoch 5, Batch 53: Training loss = 0.4800
2025-04-03 18:20:47,183 - INFO - Epoch 5, Batch 54: Training loss = 0.4700
2025-04-03 18:20:47,283 - INFO - Epoch 5, Batch 55: Training loss = 0.4600
2025-04-03 18:20:47,384 - INFO - Epoch 5, Batch 56: Training loss = 0.4500
2025-04-03 18:20:47,485 - INFO - Epoch 5, Batch 57: Training loss = 0.4400
2025-04-03 18:20:47,585 - INFO - Epoch 5, Batch 58: Training loss = 0.4300
2025-04-03 18:20:47,685 - INFO - Epoch 5, Batch 59: Training loss = 0.4200
2025-04-03 18:20:47,786 - INFO - Epoch 5, Batch 60: Training loss = 0.4100
2025-04-03 18:20:47,886 - INFO - Epoch 5, Batch 61: Training loss = 0.4000
2025-04-03 18:20:47,987 - INFO - Epoch 5, Batch 62: Training loss = 0.3900
2025-04-03 18:20:48,088 - INFO - Epoch 5, Batch 63: Training loss = 0.3800
2025-04-03 18:20:48,189 - INFO - Epoch 5, Batch 64: Training loss = 0.3700
2025-04-03 18:20:48,290 - INFO - Epoch 5, Batch 65: Training loss = 0.3600
2025-04-03 18:20:48,391 - INFO - Epoch 5, Batch 66: Training loss = 0.3500
2025-04-03 18:20:48,492 - INFO - Epoch 5, Batch 67: Training loss = 0.3400
2025-04-03 18:20:48,593 - INFO - Epoch 5, Batch 68: Training loss = 0.3300
2025-04-03 18:20:48,694 - INFO - Epoch 5, Batch 69: Training loss = 0.3200
2025-04-03 18:20:48,794 - INFO - Epoch 5, Batch 70: Training loss = 0.3100
2025-04-03 18:20:48,895 - INFO - Epoch 5, Batch 71: Training loss = 0.3000
2025-04-03 18:20:48,996 - INFO - Epoch 5, Batch 72: Training loss = 0.2900
2025-04-03 18:20:49,096 - INFO - Epoch 5, Batch 73: Training loss = 0.2800
2025-04-03 18:20:49,197 - INFO - Epoch 5, Batch 74: Training loss = 0.2700
2025-04-03 18:20:49,297 - INFO - Epoch 5, Batch 75: Training loss = 0.2600
2025-04-03 18:20:49,398 - INFO - Epoch 5, Batch 76: Training loss = 0.2500
2025-04-03 18:20:49,499 - INFO - Epoch 5, Batch 77: Training loss = 0.2400
2025-04-03 18:20:49,599 - INFO - Epoch 5, Batch 78: Training loss = 0.2300
2025-04-03 18:20:49,700 - INFO - Epoch 5, Batch 79: Training loss = 0.2200
2025-04-03 18:20:49,800 - INFO - Epoch 5, Batch 80: Training loss = 0.2100
2025-04-03 18:20:49,901 - INFO - Epoch 5, Batch 81: Training loss = 0.2000
2025-04-03 18:20:50,001 - INFO - Epoch 5, Batch 82: Training loss = 0.1900
2025-04-03 18:20:50,102 - INFO - Epoch 5, Batch 83: Training loss = 0.1800
2025-04-03 18:20:50,202 - INFO - Epoch 5, Batch 84: Training loss = 0.1700
2025-04-03 18:20:50,303 - INFO - Epoch 5, Batch 85: Training loss = 0.1600
2025-04-03 18:20:50,403 - INFO - Epoch 5, Batch 86: Training loss = 0.1500
2025-04-03 18:20:50,504 - INFO - Epoch 5, Batch 87: Training loss = 0.1400
2025-04-03 18:20:50,605 - INFO - Epoch 5, Batch 88: Training loss = 0.1300
2025-04-03 18:20:50,705 - INFO - Epoch 5, Batch 89: Training loss = 0.1200
2025-04-03 18:20:50,806 - INFO - Epoch 5, Batch 90: Training loss = 0.1100
2025-04-03 18:20:50,906 - INFO - Epoch 5, Batch 91: Training loss = 0.1000
2025-04-03 18:20:51,007 - INFO - Epoch 5, Batch 92: Training loss = 0.0900
2025-04-03 18:20:51,107 - INFO - Epoch 5, Batch 93: Training loss = 0.0800
2025-04-03 18:20:51,208 - INFO - Epoch 5, Batch 94: Training loss = 0.0700
2025-04-03 18:20:51,309 - INFO - Epoch 5, Batch 95: Training loss = 0.0600
2025-04-03 18:20:51,409 - INFO - Epoch 5, Batch 96: Training loss = 0.0500
2025-04-03 18:20:51,510 - INFO - Epoch 5, Batch 97: Training loss = 0.0400
2025-04-03 18:20:51,610 - INFO - Epoch 5, Batch 98: Training loss = 0.0300
2025-04-03 18:20:51,711 - INFO - Epoch 5, Batch 99: Training loss = 0.0200
2025-04-03 18:20:51,811 - INFO - Epoch 5, Batch 100: Training loss = 0.0100
2025-04-03 18:20:51,912 - INFO - Epoch 5: Validation loss = 0.0600, Validation accuracy = 0.8800
2025-04-03 18:20:51,912 - INFO - Epoch 5: MSE = 0.0600, RMSE = 0.2449, MAE = 0.0480, RÂ² = 0.9400
2025-04-03 18:20:51,912 - INFO - Epoch 6 started.
2025-04-03 18:20:51,912 - INFO - Epoch 6, Batch 1: Training loss = 1.0000
2025-04-03 18:20:52,013 - INFO - Epoch 6, Batch 2: Training loss = 0.9900
2025-04-03 18:20:52,113 - INFO - Epoch 6, Batch 3: Training loss = 0.9800
2025-04-03 18:20:52,214 - INFO - Epoch 6, Batch 4: Training loss = 0.9700
2025-04-03 18:20:52,314 - INFO - Epoch 6, Batch 5: Training loss = 0.9600
2025-04-03 18:20:52,415 - INFO - Epoch 6, Batch 6: Training loss = 0.9500
2025-04-03 18:20:52,515 - INFO - Epoch 6, Batch 7: Training loss = 0.9400
2025-04-03 18:20:52,616 - INFO - Epoch 6, Batch 8: Training loss = 0.9300
2025-04-03 18:20:52,716 - INFO - Epoch 6, Batch 9: Training loss = 0.9200
2025-04-03 18:20:52,817 - INFO - Epoch 6, Batch 10: Training loss = 0.9100
2025-04-03 18:20:52,918 - INFO - Epoch 6, Batch 11: Training loss = 0.9000
2025-04-03 18:20:53,019 - INFO - Epoch 6, Batch 12: Training loss = 0.8900
2025-04-03 18:20:53,119 - INFO - Epoch 6, Batch 13: Training loss = 0.8800
2025-04-03 18:20:53,220 - INFO - Epoch 6, Batch 14: Training loss = 0.8700
2025-04-03 18:20:53,321 - INFO - Epoch 6, Batch 15: Training loss = 0.8600
2025-04-03 18:20:53,422 - INFO - Epoch 6, Batch 16: Training loss = 0.8500
2025-04-03 18:20:53,523 - INFO - Epoch 6, Batch 17: Training loss = 0.8400
2025-04-03 18:20:53,624 - INFO - Epoch 6, Batch 18: Training loss = 0.8300
2025-04-03 18:20:53,724 - INFO - Epoch 6, Batch 19: Training loss = 0.8200
2025-04-03 18:20:53,825 - INFO - Epoch 6, Batch 20: Training loss = 0.8100
2025-04-03 18:20:53,925 - INFO - Epoch 6, Batch 21: Training loss = 0.8000
2025-04-03 18:20:54,026 - INFO - Epoch 6, Batch 22: Training loss = 0.7900
2025-04-03 18:20:54,127 - INFO - Epoch 6, Batch 23: Training loss = 0.7800
2025-04-03 18:20:54,227 - INFO - Epoch 6, Batch 24: Training loss = 0.7700
2025-04-03 18:20:54,328 - INFO - Epoch 6, Batch 25: Training loss = 0.7600
2025-04-03 18:20:54,428 - INFO - Epoch 6, Batch 26: Training loss = 0.7500
2025-04-03 18:20:54,529 - INFO - Epoch 6, Batch 27: Training loss = 0.7400
2025-04-03 18:20:54,629 - INFO - Epoch 6, Batch 28: Training loss = 0.7300
2025-04-03 18:20:54,730 - INFO - Epoch 6, Batch 29: Training loss = 0.7200
2025-04-03 18:20:54,830 - INFO - Epoch 6, Batch 30: Training loss = 0.7100
2025-04-03 18:20:54,931 - INFO - Epoch 6, Batch 31: Training loss = 0.7000
2025-04-03 18:20:55,032 - INFO - Epoch 6, Batch 32: Training loss = 0.6900
2025-04-03 18:20:55,133 - INFO - Epoch 6, Batch 33: Training loss = 0.6800
2025-04-03 18:20:55,233 - INFO - Epoch 6, Batch 34: Training loss = 0.6700
2025-04-03 18:20:55,334 - INFO - Epoch 6, Batch 35: Training loss = 0.6600
2025-04-03 18:20:55,435 - INFO - Epoch 6, Batch 36: Training loss = 0.6500
2025-04-03 18:20:55,536 - INFO - Epoch 6, Batch 37: Training loss = 0.6400
2025-04-03 18:20:55,637 - INFO - Epoch 6, Batch 38: Training loss = 0.6300
2025-04-03 18:20:55,738 - INFO - Epoch 6, Batch 39: Training loss = 0.6200
2025-04-03 18:20:55,839 - INFO - Epoch 6, Batch 40: Training loss = 0.6100
2025-04-03 18:20:55,939 - INFO - Epoch 6, Batch 41: Training loss = 0.6000
2025-04-03 18:20:56,040 - INFO - Epoch 6, Batch 42: Training loss = 0.5900
2025-04-03 18:20:56,141 - INFO - Epoch 6, Batch 43: Training loss = 0.5800
2025-04-03 18:20:56,242 - INFO - Epoch 6, Batch 44: Training loss = 0.5700
2025-04-03 18:20:56,343 - INFO - Epoch 6, Batch 45: Training loss = 0.5600
2025-04-03 18:20:56,444 - INFO - Epoch 6, Batch 46: Training loss = 0.5500
2025-04-03 18:20:56,545 - INFO - Epoch 6, Batch 47: Training loss = 0.5400
2025-04-03 18:20:56,646 - INFO - Epoch 6, Batch 48: Training loss = 0.5300
2025-04-03 18:20:56,747 - INFO - Epoch 6, Batch 49: Training loss = 0.5200
2025-04-03 18:20:56,848 - INFO - Epoch 6, Batch 50: Training loss = 0.5100
2025-04-03 18:20:56,949 - INFO - Epoch 6, Batch 51: Training loss = 0.5000
2025-04-03 18:20:57,049 - INFO - Epoch 6, Batch 52: Training loss = 0.4900
2025-04-03 18:20:57,150 - INFO - Epoch 6, Batch 53: Training loss = 0.4800
2025-04-03 18:20:57,251 - INFO - Epoch 6, Batch 54: Training loss = 0.4700
2025-04-03 18:20:57,352 - INFO - Epoch 6, Batch 55: Training loss = 0.4600
2025-04-03 18:20:57,454 - INFO - Epoch 6, Batch 56: Training loss = 0.4500
2025-04-03 18:20:57,555 - INFO - Epoch 6, Batch 57: Training loss = 0.4400
2025-04-03 18:20:57,656 - INFO - Epoch 6, Batch 58: Training loss = 0.4300
2025-04-03 18:20:57,757 - INFO - Epoch 6, Batch 59: Training loss = 0.4200
2025-04-03 18:20:57,858 - INFO - Epoch 6, Batch 60: Training loss = 0.4100
2025-04-03 18:20:57,959 - INFO - Epoch 6, Batch 61: Training loss = 0.4000
2025-04-03 18:20:58,060 - INFO - Epoch 6, Batch 62: Training loss = 0.3900
2025-04-03 18:20:58,161 - INFO - Epoch 6, Batch 63: Training loss = 0.3800
2025-04-03 18:20:58,261 - INFO - Epoch 6, Batch 64: Training loss = 0.3700
2025-04-03 18:20:58,362 - INFO - Epoch 6, Batch 65: Training loss = 0.3600
2025-04-03 18:20:58,463 - INFO - Epoch 6, Batch 66: Training loss = 0.3500
2025-04-03 18:20:58,565 - INFO - Epoch 6, Batch 67: Training loss = 0.3400
2025-04-03 18:20:58,666 - INFO - Epoch 6, Batch 68: Training loss = 0.3300
2025-04-03 18:20:58,767 - INFO - Epoch 6, Batch 69: Training loss = 0.3200
2025-04-03 18:20:58,869 - INFO - Epoch 6, Batch 70: Training loss = 0.3100
2025-04-03 18:20:58,969 - INFO - Epoch 6, Batch 71: Training loss = 0.3000
2025-04-03 18:20:59,070 - INFO - Epoch 6, Batch 72: Training loss = 0.2900
2025-04-03 18:20:59,171 - INFO - Epoch 6, Batch 73: Training loss = 0.2800
2025-04-03 18:20:59,272 - INFO - Epoch 6, Batch 74: Training loss = 0.2700
2025-04-03 18:20:59,373 - INFO - Epoch 6, Batch 75: Training loss = 0.2600
2025-04-03 18:20:59,474 - INFO - Epoch 6, Batch 76: Training loss = 0.2500
2025-04-03 18:20:59,575 - INFO - Epoch 6, Batch 77: Training loss = 0.2400
2025-04-03 18:20:59,676 - INFO - Epoch 6, Batch 78: Training loss = 0.2300
2025-04-03 18:20:59,777 - INFO - Epoch 6, Batch 79: Training loss = 0.2200
2025-04-03 18:20:59,878 - INFO - Epoch 6, Batch 80: Training loss = 0.2100
2025-04-03 18:20:59,979 - INFO - Epoch 6, Batch 81: Training loss = 0.2000
2025-04-03 18:21:00,079 - INFO - Epoch 6, Batch 82: Training loss = 0.1900
2025-04-03 18:21:00,180 - INFO - Epoch 6, Batch 83: Training loss = 0.1800
2025-04-03 18:21:00,281 - INFO - Epoch 6, Batch 84: Training loss = 0.1700
2025-04-03 18:21:00,381 - INFO - Epoch 6, Batch 85: Training loss = 0.1600
2025-04-03 18:21:00,482 - INFO - Epoch 6, Batch 86: Training loss = 0.1500
2025-04-03 18:21:00,582 - INFO - Epoch 6, Batch 87: Training loss = 0.1400
2025-04-03 18:21:00,683 - INFO - Epoch 6, Batch 88: Training loss = 0.1300
2025-04-03 18:21:00,784 - INFO - Epoch 6, Batch 89: Training loss = 0.1200
2025-04-03 18:21:00,885 - INFO - Epoch 6, Batch 90: Training loss = 0.1100
2025-04-03 18:21:00,985 - INFO - Epoch 6, Batch 91: Training loss = 0.1000
2025-04-03 18:21:01,086 - INFO - Epoch 6, Batch 92: Training loss = 0.0900
2025-04-03 18:21:01,187 - INFO - Epoch 6, Batch 93: Training loss = 0.0800
2025-04-03 18:21:01,288 - INFO - Epoch 6, Batch 94: Training loss = 0.0700
2025-04-03 18:21:01,389 - INFO - Epoch 6, Batch 95: Training loss = 0.0600
2025-04-03 18:21:01,489 - INFO - Epoch 6, Batch 96: Training loss = 0.0500
2025-04-03 18:21:01,590 - INFO - Epoch 6, Batch 97: Training loss = 0.0400
2025-04-03 18:21:01,690 - INFO - Epoch 6, Batch 98: Training loss = 0.0300
2025-04-03 18:21:01,791 - INFO - Epoch 6, Batch 99: Training loss = 0.0200
2025-04-03 18:21:01,892 - INFO - Epoch 6, Batch 100: Training loss = 0.0100
2025-04-03 18:21:01,993 - INFO - Epoch 6: Validation loss = 0.0500, Validation accuracy = 0.9000
2025-04-03 18:21:01,993 - INFO - Epoch 6: MSE = 0.0500, RMSE = 0.2236, MAE = 0.0400, RÂ² = 0.9500
2025-04-03 18:21:01,993 - INFO - Epoch 7 started.
2025-04-03 18:21:01,993 - INFO - Epoch 7, Batch 1: Training loss = 1.0000
2025-04-03 18:21:02,094 - INFO - Epoch 7, Batch 2: Training loss = 0.9900
2025-04-03 18:21:02,194 - INFO - Epoch 7, Batch 3: Training loss = 0.9800
2025-04-03 18:21:02,296 - INFO - Epoch 7, Batch 4: Training loss = 0.9700
2025-04-03 18:21:02,396 - INFO - Epoch 7, Batch 5: Training loss = 0.9600
2025-04-03 18:21:02,496 - INFO - Epoch 7, Batch 6: Training loss = 0.9500
2025-04-03 18:21:02,597 - INFO - Epoch 7, Batch 7: Training loss = 0.9400
2025-04-03 18:21:02,698 - INFO - Epoch 7, Batch 8: Training loss = 0.9300
2025-04-03 18:21:02,798 - INFO - Epoch 7, Batch 9: Training loss = 0.9200
2025-04-03 18:21:02,899 - INFO - Epoch 7, Batch 10: Training loss = 0.9100
2025-04-03 18:21:03,000 - INFO - Epoch 7, Batch 11: Training loss = 0.9000
2025-04-03 18:21:03,100 - INFO - Epoch 7, Batch 12: Training loss = 0.8900
2025-04-03 18:21:03,201 - INFO - Epoch 7, Batch 13: Training loss = 0.8800
2025-04-03 18:21:03,302 - INFO - Epoch 7, Batch 14: Training loss = 0.8700
2025-04-03 18:21:03,402 - INFO - Epoch 7, Batch 15: Training loss = 0.8600
2025-04-03 18:21:03,503 - INFO - Epoch 7, Batch 16: Training loss = 0.8500
2025-04-03 18:21:03,604 - INFO - Epoch 7, Batch 17: Training loss = 0.8400
2025-04-03 18:21:03,704 - INFO - Epoch 7, Batch 18: Training loss = 0.8300
2025-04-03 18:21:03,805 - INFO - Epoch 7, Batch 19: Training loss = 0.8200
2025-04-03 18:21:03,906 - INFO - Epoch 7, Batch 20: Training loss = 0.8100
2025-04-03 18:21:04,007 - INFO - Epoch 7, Batch 21: Training loss = 0.8000
2025-04-03 18:21:04,108 - INFO - Epoch 7, Batch 22: Training loss = 0.7900
2025-04-03 18:21:04,209 - INFO - Epoch 7, Batch 23: Training loss = 0.7800
2025-04-03 18:21:04,309 - INFO - Epoch 7, Batch 24: Training loss = 0.7700
2025-04-03 18:21:04,410 - INFO - Epoch 7, Batch 25: Training loss = 0.7600
2025-04-03 18:21:04,511 - INFO - Epoch 7, Batch 26: Training loss = 0.7500
2025-04-03 18:21:04,612 - INFO - Epoch 7, Batch 27: Training loss = 0.7400
2025-04-03 18:21:04,713 - INFO - Epoch 7, Batch 28: Training loss = 0.7300
2025-04-03 18:21:04,813 - INFO - Epoch 7, Batch 29: Training loss = 0.7200
2025-04-03 18:21:04,914 - INFO - Epoch 7, Batch 30: Training loss = 0.7100
2025-04-03 18:21:05,015 - INFO - Epoch 7, Batch 31: Training loss = 0.7000
2025-04-03 18:21:05,116 - INFO - Epoch 7, Batch 32: Training loss = 0.6900
2025-04-03 18:21:05,216 - INFO - Epoch 7, Batch 33: Training loss = 0.6800
2025-04-03 18:21:05,317 - INFO - Epoch 7, Batch 34: Training loss = 0.6700
2025-04-03 18:21:05,417 - INFO - Epoch 7, Batch 35: Training loss = 0.6600
2025-04-03 18:21:05,518 - INFO - Epoch 7, Batch 36: Training loss = 0.6500
2025-04-03 18:21:05,619 - INFO - Epoch 7, Batch 37: Training loss = 0.6400
2025-04-03 18:21:05,719 - INFO - Epoch 7, Batch 38: Training loss = 0.6300
2025-04-03 18:21:05,820 - INFO - Epoch 7, Batch 39: Training loss = 0.6200
2025-04-03 18:21:05,921 - INFO - Epoch 7, Batch 40: Training loss = 0.6100
2025-04-03 18:21:06,021 - INFO - Epoch 7, Batch 41: Training loss = 0.6000
2025-04-03 18:21:06,121 - INFO - Epoch 7, Batch 42: Training loss = 0.5900
2025-04-03 18:21:06,222 - INFO - Epoch 7, Batch 43: Training loss = 0.5800
2025-04-03 18:21:06,323 - INFO - Epoch 7, Batch 44: Training loss = 0.5700
2025-04-03 18:21:06,423 - INFO - Epoch 7, Batch 45: Training loss = 0.5600
2025-04-03 18:21:06,524 - INFO - Epoch 7, Batch 46: Training loss = 0.5500
2025-04-03 18:21:06,624 - INFO - Epoch 7, Batch 47: Training loss = 0.5400
2025-04-03 18:21:06,725 - INFO - Epoch 7, Batch 48: Training loss = 0.5300
2025-04-03 18:21:06,826 - INFO - Epoch 7, Batch 49: Training loss = 0.5200
2025-04-03 18:21:06,926 - INFO - Epoch 7, Batch 50: Training loss = 0.5100
2025-04-03 18:21:07,027 - INFO - Epoch 7, Batch 51: Training loss = 0.5000
2025-04-03 18:21:07,127 - INFO - Epoch 7, Batch 52: Training loss = 0.4900
2025-04-03 18:21:07,228 - INFO - Epoch 7, Batch 53: Training loss = 0.4800
2025-04-03 18:21:07,328 - INFO - Epoch 7, Batch 54: Training loss = 0.4700
2025-04-03 18:21:07,429 - INFO - Epoch 7, Batch 55: Training loss = 0.4600
2025-04-03 18:21:07,529 - INFO - Epoch 7, Batch 56: Training loss = 0.4500
2025-04-03 18:21:07,629 - INFO - Epoch 7, Batch 57: Training loss = 0.4400
2025-04-03 18:21:07,730 - INFO - Epoch 7, Batch 58: Training loss = 0.4300
2025-04-03 18:21:07,831 - INFO - Epoch 7, Batch 59: Training loss = 0.4200
2025-04-03 18:21:07,931 - INFO - Epoch 7, Batch 60: Training loss = 0.4100
2025-04-03 18:21:08,032 - INFO - Epoch 7, Batch 61: Training loss = 0.4000
2025-04-03 18:21:08,133 - INFO - Epoch 7, Batch 62: Training loss = 0.3900
2025-04-03 18:21:08,234 - INFO - Epoch 7, Batch 63: Training loss = 0.3800
2025-04-03 18:21:08,334 - INFO - Epoch 7, Batch 64: Training loss = 0.3700
2025-04-03 18:21:08,435 - INFO - Epoch 7, Batch 65: Training loss = 0.3600
2025-04-03 18:21:08,535 - INFO - Epoch 7, Batch 66: Training loss = 0.3500
2025-04-03 18:21:08,636 - INFO - Epoch 7, Batch 67: Training loss = 0.3400
2025-04-03 18:21:08,737 - INFO - Epoch 7, Batch 68: Training loss = 0.3300
2025-04-03 18:21:08,837 - INFO - Epoch 7, Batch 69: Training loss = 0.3200
2025-04-03 18:21:08,938 - INFO - Epoch 7, Batch 70: Training loss = 0.3100
2025-04-03 18:21:09,038 - INFO - Epoch 7, Batch 71: Training loss = 0.3000
2025-04-03 18:21:09,139 - INFO - Epoch 7, Batch 72: Training loss = 0.2900
2025-04-03 18:21:09,239 - INFO - Epoch 7, Batch 73: Training loss = 0.2800
2025-04-03 18:21:09,339 - INFO - Epoch 7, Batch 74: Training loss = 0.2700
2025-04-03 18:21:09,440 - INFO - Epoch 7, Batch 75: Training loss = 0.2600
2025-04-03 18:21:09,540 - INFO - Epoch 7, Batch 76: Training loss = 0.2500
2025-04-03 18:21:09,641 - INFO - Epoch 7, Batch 77: Training loss = 0.2400
2025-04-03 18:21:09,742 - INFO - Epoch 7, Batch 78: Training loss = 0.2300
2025-04-03 18:21:09,842 - INFO - Epoch 7, Batch 79: Training loss = 0.2200
2025-04-03 18:21:09,943 - INFO - Epoch 7, Batch 80: Training loss = 0.2100
2025-04-03 18:21:10,044 - INFO - Epoch 7, Batch 81: Training loss = 0.2000
2025-04-03 18:21:10,145 - INFO - Epoch 7, Batch 82: Training loss = 0.1900
2025-04-03 18:21:10,246 - INFO - Epoch 7, Batch 83: Training loss = 0.1800
2025-04-03 18:21:10,347 - INFO - Epoch 7, Batch 84: Training loss = 0.1700
2025-04-03 18:21:10,447 - INFO - Epoch 7, Batch 85: Training loss = 0.1600
2025-04-03 18:21:10,549 - INFO - Epoch 7, Batch 86: Training loss = 0.1500
2025-04-03 18:21:10,650 - INFO - Epoch 7, Batch 87: Training loss = 0.1400
2025-04-03 18:21:10,751 - INFO - Epoch 7, Batch 88: Training loss = 0.1300
2025-04-03 18:21:10,851 - INFO - Epoch 7, Batch 89: Training loss = 0.1200
2025-04-03 18:21:10,952 - INFO - Epoch 7, Batch 90: Training loss = 0.1100
2025-04-03 18:21:11,053 - INFO - Epoch 7, Batch 91: Training loss = 0.1000
2025-04-03 18:21:11,154 - INFO - Epoch 7, Batch 92: Training loss = 0.0900
2025-04-03 18:21:11,255 - INFO - Epoch 7, Batch 93: Training loss = 0.0800
2025-04-03 18:21:11,355 - INFO - Epoch 7, Batch 94: Training loss = 0.0700
2025-04-03 18:21:11,456 - INFO - Epoch 7, Batch 95: Training loss = 0.0600
2025-04-03 18:21:11,557 - INFO - Epoch 7, Batch 96: Training loss = 0.0500
2025-04-03 18:21:11,658 - INFO - Epoch 7, Batch 97: Training loss = 0.0400
2025-04-03 18:21:11,758 - INFO - Epoch 7, Batch 98: Training loss = 0.0300
2025-04-03 18:21:11,859 - INFO - Epoch 7, Batch 99: Training loss = 0.0200
2025-04-03 18:21:11,960 - INFO - Epoch 7, Batch 100: Training loss = 0.0100
2025-04-03 18:21:12,061 - INFO - Epoch 7: Validation loss = 0.0400, Validation accuracy = 0.9200
2025-04-03 18:21:12,062 - INFO - Epoch 7: MSE = 0.0400, RMSE = 0.2000, MAE = 0.0320, RÂ² = 0.9600
2025-04-03 18:21:12,062 - INFO - Epoch 8 started.
2025-04-03 18:21:12,063 - INFO - Epoch 8, Batch 1: Training loss = 1.0000
2025-04-03 18:21:12,163 - INFO - Epoch 8, Batch 2: Training loss = 0.9900
2025-04-03 18:21:12,264 - INFO - Epoch 8, Batch 3: Training loss = 0.9800
2025-04-03 18:21:12,365 - INFO - Epoch 8, Batch 4: Training loss = 0.9700
2025-04-03 18:21:12,466 - INFO - Epoch 8, Batch 5: Training loss = 0.9600
2025-04-03 18:21:12,567 - INFO - Epoch 8, Batch 6: Training loss = 0.9500
2025-04-03 18:21:12,668 - INFO - Epoch 8, Batch 7: Training loss = 0.9400
2025-04-03 18:21:12,770 - INFO - Epoch 8, Batch 8: Training loss = 0.9300
2025-04-03 18:21:12,870 - INFO - Epoch 8, Batch 9: Training loss = 0.9200
2025-04-03 18:21:12,971 - INFO - Epoch 8, Batch 10: Training loss = 0.9100
2025-04-03 18:21:13,072 - INFO - Epoch 8, Batch 11: Training loss = 0.9000
2025-04-03 18:21:13,173 - INFO - Epoch 8, Batch 12: Training loss = 0.8900
2025-04-03 18:21:13,274 - INFO - Epoch 8, Batch 13: Training loss = 0.8800
2025-04-03 18:21:13,375 - INFO - Epoch 8, Batch 14: Training loss = 0.8700
2025-04-03 18:21:13,475 - INFO - Epoch 8, Batch 15: Training loss = 0.8600
2025-04-03 18:21:13,576 - INFO - Epoch 8, Batch 16: Training loss = 0.8500
2025-04-03 18:21:13,677 - INFO - Epoch 8, Batch 17: Training loss = 0.8400
2025-04-03 18:21:13,778 - INFO - Epoch 8, Batch 18: Training loss = 0.8300
2025-04-03 18:21:13,879 - INFO - Epoch 8, Batch 19: Training loss = 0.8200
2025-04-03 18:21:13,979 - INFO - Epoch 8, Batch 20: Training loss = 0.8100
2025-04-03 18:21:14,081 - INFO - Epoch 8, Batch 21: Training loss = 0.8000
2025-04-03 18:21:14,182 - INFO - Epoch 8, Batch 22: Training loss = 0.7900
2025-04-03 18:21:14,282 - INFO - Epoch 8, Batch 23: Training loss = 0.7800
2025-04-03 18:21:14,384 - INFO - Epoch 8, Batch 24: Training loss = 0.7700
2025-04-03 18:21:14,485 - INFO - Epoch 8, Batch 25: Training loss = 0.7600
2025-04-03 18:21:14,586 - INFO - Epoch 8, Batch 26: Training loss = 0.7500
2025-04-03 18:21:14,686 - INFO - Epoch 8, Batch 27: Training loss = 0.7400
2025-04-03 18:21:14,787 - INFO - Epoch 8, Batch 28: Training loss = 0.7300
2025-04-03 18:21:14,888 - INFO - Epoch 8, Batch 29: Training loss = 0.7200
2025-04-03 18:21:14,989 - INFO - Epoch 8, Batch 30: Training loss = 0.7100
2025-04-03 18:21:15,090 - INFO - Epoch 8, Batch 31: Training loss = 0.7000
2025-04-03 18:21:15,191 - INFO - Epoch 8, Batch 32: Training loss = 0.6900
2025-04-03 18:21:15,292 - INFO - Epoch 8, Batch 33: Training loss = 0.6800
2025-04-03 18:21:15,392 - INFO - Epoch 8, Batch 34: Training loss = 0.6700
2025-04-03 18:21:15,493 - INFO - Epoch 8, Batch 35: Training loss = 0.6600
2025-04-03 18:21:15,594 - INFO - Epoch 8, Batch 36: Training loss = 0.6500
2025-04-03 18:21:15,695 - INFO - Epoch 8, Batch 37: Training loss = 0.6400
2025-04-03 18:21:15,795 - INFO - Epoch 8, Batch 38: Training loss = 0.6300
2025-04-03 18:21:15,896 - INFO - Epoch 8, Batch 39: Training loss = 0.6200
2025-04-03 18:21:15,996 - INFO - Epoch 8, Batch 40: Training loss = 0.6100
2025-04-03 18:21:16,097 - INFO - Epoch 8, Batch 41: Training loss = 0.6000
2025-04-03 18:21:16,198 - INFO - Epoch 8, Batch 42: Training loss = 0.5900
2025-04-03 18:21:16,299 - INFO - Epoch 8, Batch 43: Training loss = 0.5800
2025-04-03 18:21:16,400 - INFO - Epoch 8, Batch 44: Training loss = 0.5700
2025-04-03 18:21:16,501 - INFO - Epoch 8, Batch 45: Training loss = 0.5600
2025-04-03 18:21:16,602 - INFO - Epoch 8, Batch 46: Training loss = 0.5500
2025-04-03 18:21:16,703 - INFO - Epoch 8, Batch 47: Training loss = 0.5400
2025-04-03 18:21:16,805 - INFO - Epoch 8, Batch 48: Training loss = 0.5300
2025-04-03 18:21:16,906 - INFO - Epoch 8, Batch 49: Training loss = 0.5200
2025-04-03 18:21:17,006 - INFO - Epoch 8, Batch 50: Training loss = 0.5100
2025-04-03 18:21:17,107 - INFO - Epoch 8, Batch 51: Training loss = 0.5000
2025-04-03 18:21:17,208 - INFO - Epoch 8, Batch 52: Training loss = 0.4900
2025-04-03 18:21:17,309 - INFO - Epoch 8, Batch 53: Training loss = 0.4800
2025-04-03 18:21:17,410 - INFO - Epoch 8, Batch 54: Training loss = 0.4700
2025-04-03 18:21:17,511 - INFO - Epoch 8, Batch 55: Training loss = 0.4600
2025-04-03 18:21:17,612 - INFO - Epoch 8, Batch 56: Training loss = 0.4500
2025-04-03 18:21:17,713 - INFO - Epoch 8, Batch 57: Training loss = 0.4400
2025-04-03 18:21:17,813 - INFO - Epoch 8, Batch 58: Training loss = 0.4300
2025-04-03 18:21:17,914 - INFO - Epoch 8, Batch 59: Training loss = 0.4200
2025-04-03 18:21:18,015 - INFO - Epoch 8, Batch 60: Training loss = 0.4100
2025-04-03 18:21:18,115 - INFO - Epoch 8, Batch 61: Training loss = 0.4000
2025-04-03 18:21:18,216 - INFO - Epoch 8, Batch 62: Training loss = 0.3900
2025-04-03 18:21:18,316 - INFO - Epoch 8, Batch 63: Training loss = 0.3800
2025-04-03 18:21:18,417 - INFO - Epoch 8, Batch 64: Training loss = 0.3700
2025-04-03 18:21:18,518 - INFO - Epoch 8, Batch 65: Training loss = 0.3600
2025-04-03 18:21:18,619 - INFO - Epoch 8, Batch 66: Training loss = 0.3500
2025-04-03 18:21:18,720 - INFO - Epoch 8, Batch 67: Training loss = 0.3400
2025-04-03 18:21:18,820 - INFO - Epoch 8, Batch 68: Training loss = 0.3300
2025-04-03 18:21:18,921 - INFO - Epoch 8, Batch 69: Training loss = 0.3200
2025-04-03 18:21:19,021 - INFO - Epoch 8, Batch 70: Training loss = 0.3100
2025-04-03 18:21:19,123 - INFO - Epoch 8, Batch 71: Training loss = 0.3000
2025-04-03 18:21:19,223 - INFO - Epoch 8, Batch 72: Training loss = 0.2900
2025-04-03 18:21:19,324 - INFO - Epoch 8, Batch 73: Training loss = 0.2800
2025-04-03 18:21:19,424 - INFO - Epoch 8, Batch 74: Training loss = 0.2700
2025-04-03 18:21:19,525 - INFO - Epoch 8, Batch 75: Training loss = 0.2600
2025-04-03 18:21:19,626 - INFO - Epoch 8, Batch 76: Training loss = 0.2500
2025-04-03 18:21:19,726 - INFO - Epoch 8, Batch 77: Training loss = 0.2400
2025-04-03 18:21:19,827 - INFO - Epoch 8, Batch 78: Training loss = 0.2300
2025-04-03 18:21:19,927 - INFO - Epoch 8, Batch 79: Training loss = 0.2200
2025-04-03 18:21:20,028 - INFO - Epoch 8, Batch 80: Training loss = 0.2100
2025-04-03 18:21:20,129 - INFO - Epoch 8, Batch 81: Training loss = 0.2000
2025-04-03 18:21:20,229 - INFO - Epoch 8, Batch 82: Training loss = 0.1900
2025-04-03 18:21:20,330 - INFO - Epoch 8, Batch 83: Training loss = 0.1800
2025-04-03 18:21:20,431 - INFO - Epoch 8, Batch 84: Training loss = 0.1700
2025-04-03 18:21:20,531 - INFO - Epoch 8, Batch 85: Training loss = 0.1600
2025-04-03 18:21:20,632 - INFO - Epoch 8, Batch 86: Training loss = 0.1500
2025-04-03 18:21:20,733 - INFO - Epoch 8, Batch 87: Training loss = 0.1400
2025-04-03 18:21:20,834 - INFO - Epoch 8, Batch 88: Training loss = 0.1300
2025-04-03 18:21:20,935 - INFO - Epoch 8, Batch 89: Training loss = 0.1200
2025-04-03 18:21:21,035 - INFO - Epoch 8, Batch 90: Training loss = 0.1100
2025-04-03 18:21:21,136 - INFO - Epoch 8, Batch 91: Training loss = 0.1000
2025-04-03 18:21:21,237 - INFO - Epoch 8, Batch 92: Training loss = 0.0900
2025-04-03 18:21:21,337 - INFO - Epoch 8, Batch 93: Training loss = 0.0800
2025-04-03 18:21:21,438 - INFO - Epoch 8, Batch 94: Training loss = 0.0700
2025-04-03 18:21:21,539 - INFO - Epoch 8, Batch 95: Training loss = 0.0600
2025-04-03 18:21:21,639 - INFO - Epoch 8, Batch 96: Training loss = 0.0500
2025-04-03 18:21:21,740 - INFO - Epoch 8, Batch 97: Training loss = 0.0400
2025-04-03 18:21:21,841 - INFO - Epoch 8, Batch 98: Training loss = 0.0300
2025-04-03 18:21:21,942 - INFO - Epoch 8, Batch 99: Training loss = 0.0200
2025-04-03 18:21:22,043 - INFO - Epoch 8, Batch 100: Training loss = 0.0100
2025-04-03 18:21:22,143 - INFO - Epoch 8: Validation loss = 0.0300, Validation accuracy = 0.9400
2025-04-03 18:21:22,144 - INFO - Epoch 8: MSE = 0.0300, RMSE = 0.1732, MAE = 0.0240, RÂ² = 0.9700
2025-04-03 18:21:22,144 - INFO - Epoch 9 started.
2025-04-03 18:21:22,144 - INFO - Epoch 9, Batch 1: Training loss = 1.0000
2025-04-03 18:21:22,244 - INFO - Epoch 9, Batch 2: Training loss = 0.9900
2025-04-03 18:21:22,345 - INFO - Epoch 9, Batch 3: Training loss = 0.9800
2025-04-03 18:21:22,446 - INFO - Epoch 9, Batch 4: Training loss = 0.9700
2025-04-03 18:21:22,546 - INFO - Epoch 9, Batch 5: Training loss = 0.9600
2025-04-03 18:21:22,647 - INFO - Epoch 9, Batch 6: Training loss = 0.9500
2025-04-03 18:21:22,748 - INFO - Epoch 9, Batch 7: Training loss = 0.9400
2025-04-03 18:21:22,849 - INFO - Epoch 9, Batch 8: Training loss = 0.9300
2025-04-03 18:21:22,950 - INFO - Epoch 9, Batch 9: Training loss = 0.9200
2025-04-03 18:21:23,051 - INFO - Epoch 9, Batch 10: Training loss = 0.9100
2025-04-03 18:21:23,152 - INFO - Epoch 9, Batch 11: Training loss = 0.9000
2025-04-03 18:21:23,253 - INFO - Epoch 9, Batch 12: Training loss = 0.8900
2025-04-03 18:21:23,354 - INFO - Epoch 9, Batch 13: Training loss = 0.8800
2025-04-03 18:21:23,455 - INFO - Epoch 9, Batch 14: Training loss = 0.8700
2025-04-03 18:21:23,555 - INFO - Epoch 9, Batch 15: Training loss = 0.8600
2025-04-03 18:21:23,656 - INFO - Epoch 9, Batch 16: Training loss = 0.8500
2025-04-03 18:21:23,756 - INFO - Epoch 9, Batch 17: Training loss = 0.8400
2025-04-03 18:21:23,857 - INFO - Epoch 9, Batch 18: Training loss = 0.8300
2025-04-03 18:21:23,958 - INFO - Epoch 9, Batch 19: Training loss = 0.8200
2025-04-03 18:21:24,059 - INFO - Epoch 9, Batch 20: Training loss = 0.8100
2025-04-03 18:21:24,160 - INFO - Epoch 9, Batch 21: Training loss = 0.8000
2025-04-03 18:21:24,261 - INFO - Epoch 9, Batch 22: Training loss = 0.7900
2025-04-03 18:21:24,361 - INFO - Epoch 9, Batch 23: Training loss = 0.7800
2025-04-03 18:21:24,462 - INFO - Epoch 9, Batch 24: Training loss = 0.7700
2025-04-03 18:21:24,562 - INFO - Epoch 9, Batch 25: Training loss = 0.7600
2025-04-03 18:21:24,663 - INFO - Epoch 9, Batch 26: Training loss = 0.7500
2025-04-03 18:21:24,764 - INFO - Epoch 9, Batch 27: Training loss = 0.7400
2025-04-03 18:21:24,864 - INFO - Epoch 9, Batch 28: Training loss = 0.7300
2025-04-03 18:21:24,965 - INFO - Epoch 9, Batch 29: Training loss = 0.7200
2025-04-03 18:21:25,066 - INFO - Epoch 9, Batch 30: Training loss = 0.7100
2025-04-03 18:21:25,167 - INFO - Epoch 9, Batch 31: Training loss = 0.7000
2025-04-03 18:21:25,267 - INFO - Epoch 9, Batch 32: Training loss = 0.6900
2025-04-03 18:21:25,368 - INFO - Epoch 9, Batch 33: Training loss = 0.6800
2025-04-03 18:21:25,469 - INFO - Epoch 9, Batch 34: Training loss = 0.6700
2025-04-03 18:21:25,570 - INFO - Epoch 9, Batch 35: Training loss = 0.6600
2025-04-03 18:21:25,671 - INFO - Epoch 9, Batch 36: Training loss = 0.6500
2025-04-03 18:21:25,772 - INFO - Epoch 9, Batch 37: Training loss = 0.6400
2025-04-03 18:21:25,872 - INFO - Epoch 9, Batch 38: Training loss = 0.6300
2025-04-03 18:21:25,973 - INFO - Epoch 9, Batch 39: Training loss = 0.6200
2025-04-03 18:21:26,074 - INFO - Epoch 9, Batch 40: Training loss = 0.6100
2025-04-03 18:21:26,175 - INFO - Epoch 9, Batch 41: Training loss = 0.6000
2025-04-03 18:21:26,276 - INFO - Epoch 9, Batch 42: Training loss = 0.5900
2025-04-03 18:21:26,377 - INFO - Epoch 9, Batch 43: Training loss = 0.5800
2025-04-03 18:21:26,478 - INFO - Epoch 9, Batch 44: Training loss = 0.5700
2025-04-03 18:21:26,578 - INFO - Epoch 9, Batch 45: Training loss = 0.5600
2025-04-03 18:21:26,679 - INFO - Epoch 9, Batch 46: Training loss = 0.5500
2025-04-03 18:21:26,779 - INFO - Epoch 9, Batch 47: Training loss = 0.5400
2025-04-03 18:21:26,880 - INFO - Epoch 9, Batch 48: Training loss = 0.5300
2025-04-03 18:21:26,981 - INFO - Epoch 9, Batch 49: Training loss = 0.5200
2025-04-03 18:21:27,081 - INFO - Epoch 9, Batch 50: Training loss = 0.5100
2025-04-03 18:21:27,182 - INFO - Epoch 9, Batch 51: Training loss = 0.5000
2025-04-03 18:21:27,283 - INFO - Epoch 9, Batch 52: Training loss = 0.4900
2025-04-03 18:21:27,384 - INFO - Epoch 9, Batch 53: Training loss = 0.4800
2025-04-03 18:21:27,485 - INFO - Epoch 9, Batch 54: Training loss = 0.4700
2025-04-03 18:21:27,586 - INFO - Epoch 9, Batch 55: Training loss = 0.4600
2025-04-03 18:21:27,687 - INFO - Epoch 9, Batch 56: Training loss = 0.4500
2025-04-03 18:21:27,787 - INFO - Epoch 9, Batch 57: Training loss = 0.4400
2025-04-03 18:21:27,888 - INFO - Epoch 9, Batch 58: Training loss = 0.4300
2025-04-03 18:21:27,989 - INFO - Epoch 9, Batch 59: Training loss = 0.4200
2025-04-03 18:21:28,090 - INFO - Epoch 9, Batch 60: Training loss = 0.4100
2025-04-03 18:21:28,191 - INFO - Epoch 9, Batch 61: Training loss = 0.4000
2025-04-03 18:21:28,292 - INFO - Epoch 9, Batch 62: Training loss = 0.3900
2025-04-03 18:21:28,392 - INFO - Epoch 9, Batch 63: Training loss = 0.3800
2025-04-03 18:21:28,493 - INFO - Epoch 9, Batch 64: Training loss = 0.3700
2025-04-03 18:21:28,594 - INFO - Epoch 9, Batch 65: Training loss = 0.3600
2025-04-03 18:21:28,695 - INFO - Epoch 9, Batch 66: Training loss = 0.3500
2025-04-03 18:21:28,796 - INFO - Epoch 9, Batch 67: Training loss = 0.3400
2025-04-03 18:21:28,897 - INFO - Epoch 9, Batch 68: Training loss = 0.3300
2025-04-03 18:21:28,997 - INFO - Epoch 9, Batch 69: Training loss = 0.3200
2025-04-03 18:21:29,098 - INFO - Epoch 9, Batch 70: Training loss = 0.3100
2025-04-03 18:21:29,199 - INFO - Epoch 9, Batch 71: Training loss = 0.3000
2025-04-03 18:21:29,299 - INFO - Epoch 9, Batch 72: Training loss = 0.2900
2025-04-03 18:21:29,400 - INFO - Epoch 9, Batch 73: Training loss = 0.2800
2025-04-03 18:21:29,501 - INFO - Epoch 9, Batch 74: Training loss = 0.2700
2025-04-03 18:21:29,602 - INFO - Epoch 9, Batch 75: Training loss = 0.2600
2025-04-03 18:21:29,704 - INFO - Epoch 9, Batch 76: Training loss = 0.2500
2025-04-03 18:21:29,804 - INFO - Epoch 9, Batch 77: Training loss = 0.2400
2025-04-03 18:21:29,905 - INFO - Epoch 9, Batch 78: Training loss = 0.2300
2025-04-03 18:21:30,006 - INFO - Epoch 9, Batch 79: Training loss = 0.2200
2025-04-03 18:21:30,106 - INFO - Epoch 9, Batch 80: Training loss = 0.2100
2025-04-03 18:21:30,207 - INFO - Epoch 9, Batch 81: Training loss = 0.2000
2025-04-03 18:21:30,308 - INFO - Epoch 9, Batch 82: Training loss = 0.1900
2025-04-03 18:21:30,409 - INFO - Epoch 9, Batch 83: Training loss = 0.1800
2025-04-03 18:21:30,510 - INFO - Epoch 9, Batch 84: Training loss = 0.1700
2025-04-03 18:21:30,610 - INFO - Epoch 9, Batch 85: Training loss = 0.1600
2025-04-03 18:21:30,711 - INFO - Epoch 9, Batch 86: Training loss = 0.1500
2025-04-03 18:21:30,812 - INFO - Epoch 9, Batch 87: Training loss = 0.1400
2025-04-03 18:21:30,913 - INFO - Epoch 9, Batch 88: Training loss = 0.1300
2025-04-03 18:21:31,014 - INFO - Epoch 9, Batch 89: Training loss = 0.1200
2025-04-03 18:21:31,114 - INFO - Epoch 9, Batch 90: Training loss = 0.1100
2025-04-03 18:21:31,215 - INFO - Epoch 9, Batch 91: Training loss = 0.1000
2025-04-03 18:21:31,316 - INFO - Epoch 9, Batch 92: Training loss = 0.0900
2025-04-03 18:21:31,417 - INFO - Epoch 9, Batch 93: Training loss = 0.0800
2025-04-03 18:21:31,517 - INFO - Epoch 9, Batch 94: Training loss = 0.0700
2025-04-03 18:21:31,618 - INFO - Epoch 9, Batch 95: Training loss = 0.0600
2025-04-03 18:21:31,718 - INFO - Epoch 9, Batch 96: Training loss = 0.0500
2025-04-03 18:21:31,819 - INFO - Epoch 9, Batch 97: Training loss = 0.0400
2025-04-03 18:21:31,919 - INFO - Epoch 9, Batch 98: Training loss = 0.0300
2025-04-03 18:21:32,020 - INFO - Epoch 9, Batch 99: Training loss = 0.0200
2025-04-03 18:21:32,121 - INFO - Epoch 9, Batch 100: Training loss = 0.0100
2025-04-03 18:21:32,221 - INFO - Epoch 9: Validation loss = 0.0200, Validation accuracy = 0.9600
2025-04-03 18:21:32,222 - INFO - Epoch 9: MSE = 0.0200, RMSE = 0.1414, MAE = 0.0160, RÂ² = 0.9800
2025-04-03 18:21:32,222 - INFO - Epoch 10 started.
2025-04-03 18:21:32,222 - INFO - Epoch 10, Batch 1: Training loss = 1.0000
2025-04-03 18:21:32,323 - INFO - Epoch 10, Batch 2: Training loss = 0.9900
2025-04-03 18:21:32,424 - INFO - Epoch 10, Batch 3: Training loss = 0.9800
2025-04-03 18:21:32,524 - INFO - Epoch 10, Batch 4: Training loss = 0.9700
2025-04-03 18:21:32,625 - INFO - Epoch 10, Batch 5: Training loss = 0.9600
2025-04-03 18:21:32,726 - INFO - Epoch 10, Batch 6: Training loss = 0.9500
2025-04-03 18:21:32,826 - INFO - Epoch 10, Batch 7: Training loss = 0.9400
2025-04-03 18:21:32,927 - INFO - Epoch 10, Batch 8: Training loss = 0.9300
2025-04-03 18:21:33,028 - INFO - Epoch 10, Batch 9: Training loss = 0.9200
2025-04-03 18:21:33,129 - INFO - Epoch 10, Batch 10: Training loss = 0.9100
2025-04-03 18:21:33,230 - INFO - Epoch 10, Batch 11: Training loss = 0.9000
2025-04-03 18:21:33,332 - INFO - Epoch 10, Batch 12: Training loss = 0.8900
2025-04-03 18:21:33,432 - INFO - Epoch 10, Batch 13: Training loss = 0.8800
2025-04-03 18:21:33,533 - INFO - Epoch 10, Batch 14: Training loss = 0.8700
2025-04-03 18:21:33,634 - INFO - Epoch 10, Batch 15: Training loss = 0.8600
2025-04-03 18:21:33,735 - INFO - Epoch 10, Batch 16: Training loss = 0.8500
2025-04-03 18:21:33,835 - INFO - Epoch 10, Batch 17: Training loss = 0.8400
2025-04-03 18:21:33,936 - INFO - Epoch 10, Batch 18: Training loss = 0.8300
2025-04-03 18:21:34,037 - INFO - Epoch 10, Batch 19: Training loss = 0.8200
2025-04-03 18:21:34,138 - INFO - Epoch 10, Batch 20: Training loss = 0.8100
2025-04-03 18:21:34,239 - INFO - Epoch 10, Batch 21: Training loss = 0.8000
2025-04-03 18:21:34,340 - INFO - Epoch 10, Batch 22: Training loss = 0.7900
2025-04-03 18:21:34,440 - INFO - Epoch 10, Batch 23: Training loss = 0.7800
2025-04-03 18:21:34,541 - INFO - Epoch 10, Batch 24: Training loss = 0.7700
2025-04-03 18:21:34,642 - INFO - Epoch 10, Batch 25: Training loss = 0.7600
2025-04-03 18:21:34,743 - INFO - Epoch 10, Batch 26: Training loss = 0.7500
2025-04-03 18:21:34,844 - INFO - Epoch 10, Batch 27: Training loss = 0.7400
2025-04-03 18:21:34,945 - INFO - Epoch 10, Batch 28: Training loss = 0.7300
2025-04-03 18:21:35,046 - INFO - Epoch 10, Batch 29: Training loss = 0.7200
2025-04-03 18:21:35,147 - INFO - Epoch 10, Batch 30: Training loss = 0.7100
2025-04-03 18:21:35,248 - INFO - Epoch 10, Batch 31: Training loss = 0.7000
2025-04-03 18:21:35,349 - INFO - Epoch 10, Batch 32: Training loss = 0.6900
2025-04-03 18:21:35,449 - INFO - Epoch 10, Batch 33: Training loss = 0.6800
2025-04-03 18:21:35,550 - INFO - Epoch 10, Batch 34: Training loss = 0.6700
2025-04-03 18:21:35,651 - INFO - Epoch 10, Batch 35: Training loss = 0.6600
2025-04-03 18:21:35,752 - INFO - Epoch 10, Batch 36: Training loss = 0.6500
2025-04-03 18:21:35,853 - INFO - Epoch 10, Batch 37: Training loss = 0.6400
2025-04-03 18:21:35,953 - INFO - Epoch 10, Batch 38: Training loss = 0.6300
2025-04-03 18:21:36,054 - INFO - Epoch 10, Batch 39: Training loss = 0.6200
2025-04-03 18:21:36,155 - INFO - Epoch 10, Batch 40: Training loss = 0.6100
2025-04-03 18:21:36,256 - INFO - Epoch 10, Batch 41: Training loss = 0.6000
2025-04-03 18:21:36,357 - INFO - Epoch 10, Batch 42: Training loss = 0.5900
2025-04-03 18:21:36,458 - INFO - Epoch 10, Batch 43: Training loss = 0.5800
2025-04-03 18:21:36,559 - INFO - Epoch 10, Batch 44: Training loss = 0.5700
2025-04-03 18:21:36,660 - INFO - Epoch 10, Batch 45: Training loss = 0.5600
2025-04-03 18:21:36,760 - INFO - Epoch 10, Batch 46: Training loss = 0.5500
2025-04-03 18:21:36,861 - INFO - Epoch 10, Batch 47: Training loss = 0.5400
2025-04-03 18:21:36,962 - INFO - Epoch 10, Batch 48: Training loss = 0.5300
2025-04-03 18:21:37,063 - INFO - Epoch 10, Batch 49: Training loss = 0.5200
2025-04-03 18:21:37,164 - INFO - Epoch 10, Batch 50: Training loss = 0.5100
2025-04-03 18:21:37,265 - INFO - Epoch 10, Batch 51: Training loss = 0.5000
2025-04-03 18:21:37,366 - INFO - Epoch 10, Batch 52: Training loss = 0.4900
2025-04-03 18:21:37,467 - INFO - Epoch 10, Batch 53: Training loss = 0.4800
2025-04-03 18:21:37,566 - INFO - Epoch 10, Batch 54: Training loss = 0.4700
2025-04-03 18:21:37,667 - INFO - Epoch 10, Batch 55: Training loss = 0.4600
2025-04-03 18:21:37,768 - INFO - Epoch 10, Batch 56: Training loss = 0.4500
2025-04-03 18:21:37,869 - INFO - Epoch 10, Batch 57: Training loss = 0.4400
2025-04-03 18:21:37,969 - INFO - Epoch 10, Batch 58: Training loss = 0.4300
2025-04-03 18:21:38,070 - INFO - Epoch 10, Batch 59: Training loss = 0.4200
2025-04-03 18:21:38,171 - INFO - Epoch 10, Batch 60: Training loss = 0.4100
2025-04-03 18:21:38,272 - INFO - Epoch 10, Batch 61: Training loss = 0.4000
2025-04-03 18:21:38,373 - INFO - Epoch 10, Batch 62: Training loss = 0.3900
2025-04-03 18:21:38,474 - INFO - Epoch 10, Batch 63: Training loss = 0.3800
2025-04-03 18:21:38,575 - INFO - Epoch 10, Batch 64: Training loss = 0.3700
2025-04-03 18:21:38,677 - INFO - Epoch 10, Batch 65: Training loss = 0.3600
2025-04-03 18:21:38,778 - INFO - Epoch 10, Batch 66: Training loss = 0.3500
2025-04-03 18:21:38,879 - INFO - Epoch 10, Batch 67: Training loss = 0.3400
2025-04-03 18:21:38,979 - INFO - Epoch 10, Batch 68: Training loss = 0.3300
2025-04-03 18:21:39,080 - INFO - Epoch 10, Batch 69: Training loss = 0.3200
2025-04-03 18:21:39,181 - INFO - Epoch 10, Batch 70: Training loss = 0.3100
2025-04-03 18:21:39,281 - INFO - Epoch 10, Batch 71: Training loss = 0.3000
2025-04-03 18:21:39,382 - INFO - Epoch 10, Batch 72: Training loss = 0.2900
2025-04-03 18:21:39,483 - INFO - Epoch 10, Batch 73: Training loss = 0.2800
2025-04-03 18:21:39,585 - INFO - Epoch 10, Batch 74: Training loss = 0.2700
2025-04-03 18:21:39,686 - INFO - Epoch 10, Batch 75: Training loss = 0.2600
2025-04-03 18:21:39,787 - INFO - Epoch 10, Batch 76: Training loss = 0.2500
2025-04-03 18:21:39,888 - INFO - Epoch 10, Batch 77: Training loss = 0.2400
2025-04-03 18:21:39,989 - INFO - Epoch 10, Batch 78: Training loss = 0.2300
2025-04-03 18:21:40,090 - INFO - Epoch 10, Batch 79: Training loss = 0.2200
2025-04-03 18:21:40,190 - INFO - Epoch 10, Batch 80: Training loss = 0.2100
2025-04-03 18:21:40,291 - INFO - Epoch 10, Batch 81: Training loss = 0.2000
2025-04-03 18:21:40,392 - INFO - Epoch 10, Batch 82: Training loss = 0.1900
2025-04-03 18:21:40,493 - INFO - Epoch 10, Batch 83: Training loss = 0.1800
2025-04-03 18:21:40,594 - INFO - Epoch 10, Batch 84: Training loss = 0.1700
2025-04-03 18:21:40,695 - INFO - Epoch 10, Batch 85: Training loss = 0.1600
2025-04-03 18:21:40,796 - INFO - Epoch 10, Batch 86: Training loss = 0.1500
2025-04-03 18:21:40,896 - INFO - Epoch 10, Batch 87: Training loss = 0.1400
2025-04-03 18:21:40,997 - INFO - Epoch 10, Batch 88: Training loss = 0.1300
2025-04-03 18:21:41,098 - INFO - Epoch 10, Batch 89: Training loss = 0.1200
2025-04-03 18:21:41,198 - INFO - Epoch 10, Batch 90: Training loss = 0.1100
2025-04-03 18:21:41,299 - INFO - Epoch 10, Batch 91: Training loss = 0.1000
2025-04-03 18:21:41,400 - INFO - Epoch 10, Batch 92: Training loss = 0.0900
2025-04-03 18:21:41,501 - INFO - Epoch 10, Batch 93: Training loss = 0.0800
2025-04-03 18:21:41,602 - INFO - Epoch 10, Batch 94: Training loss = 0.0700
2025-04-03 18:21:41,703 - INFO - Epoch 10, Batch 95: Training loss = 0.0600
2025-04-03 18:21:41,803 - INFO - Epoch 10, Batch 96: Training loss = 0.0500
2025-04-03 18:21:41,904 - INFO - Epoch 10, Batch 97: Training loss = 0.0400
2025-04-03 18:21:42,005 - INFO - Epoch 10, Batch 98: Training loss = 0.0300
2025-04-03 18:21:42,106 - INFO - Epoch 10, Batch 99: Training loss = 0.0200
2025-04-03 18:21:42,206 - INFO - Epoch 10, Batch 100: Training loss = 0.0100
2025-04-03 18:21:42,307 - INFO - Epoch 10: Validation loss = 0.0100, Validation accuracy = 0.9800
2025-04-03 18:21:42,307 - INFO - Epoch 10: MSE = 0.0100, RMSE = 0.1000, MAE = 0.0080, RÂ² = 0.9900
2025-04-03 18:21:42,307 - INFO - Regression training process completed.
2025-04-03 18:26:32,819 - INFO - Starting regression training process...
2025-04-03 18:26:32,819 - INFO - Epoch 1 started.
2025-04-03 18:26:32,820 - INFO - Epoch 1, Batch 1: Training loss = 1.0000
2025-04-03 18:26:32,920 - INFO - Epoch 1, Batch 2: Training loss = 0.9900
2025-04-03 18:26:33,021 - INFO - Epoch 1, Batch 3: Training loss = 0.9800
2025-04-03 18:26:33,121 - INFO - Epoch 1, Batch 4: Training loss = 0.9700
2025-04-03 18:26:33,222 - INFO - Epoch 1, Batch 5: Training loss = 0.9600
2025-04-03 18:26:33,323 - INFO - Epoch 1, Batch 6: Training loss = 0.9500
2025-04-03 18:26:33,424 - INFO - Epoch 1, Batch 7: Training loss = 0.9400
2025-04-03 18:26:33,525 - INFO - Epoch 1, Batch 8: Training loss = 0.9300
2025-04-03 18:26:33,626 - INFO - Epoch 1, Batch 9: Training loss = 0.9200
2025-04-03 18:26:33,727 - INFO - Epoch 1, Batch 10: Training loss = 0.9100
2025-04-03 18:26:33,828 - INFO - Epoch 1, Batch 11: Training loss = 0.9000
2025-04-03 18:26:33,929 - INFO - Epoch 1, Batch 12: Training loss = 0.8900
2025-04-03 18:26:34,029 - INFO - Epoch 1, Batch 13: Training loss = 0.8800
2025-04-03 18:26:34,130 - INFO - Epoch 1, Batch 14: Training loss = 0.8700
2025-04-03 18:26:34,231 - INFO - Epoch 1, Batch 15: Training loss = 0.8600
2025-04-03 18:26:34,332 - INFO - Epoch 1, Batch 16: Training loss = 0.8500
2025-04-03 18:26:34,433 - INFO - Epoch 1, Batch 17: Training loss = 0.8400
2025-04-03 18:26:34,534 - INFO - Epoch 1, Batch 18: Training loss = 0.8300
2025-04-03 18:26:34,634 - INFO - Epoch 1, Batch 19: Training loss = 0.8200
2025-04-03 18:26:34,735 - INFO - Epoch 1, Batch 20: Training loss = 0.8100
2025-04-03 18:26:34,836 - INFO - Epoch 1, Batch 21: Training loss = 0.8000
2025-04-03 18:26:34,937 - INFO - Epoch 1, Batch 22: Training loss = 0.7900
2025-04-03 18:26:35,038 - INFO - Epoch 1, Batch 23: Training loss = 0.7800
2025-04-03 18:26:35,139 - INFO - Epoch 1, Batch 24: Training loss = 0.7700
2025-04-03 18:26:35,240 - INFO - Epoch 1, Batch 25: Training loss = 0.7600
2025-04-03 18:26:35,341 - INFO - Epoch 1, Batch 26: Training loss = 0.7500
2025-04-03 18:26:35,441 - INFO - Epoch 1, Batch 27: Training loss = 0.7400
2025-04-03 18:26:35,542 - INFO - Epoch 1, Batch 28: Training loss = 0.7300
2025-04-03 18:26:35,643 - INFO - Epoch 1, Batch 29: Training loss = 0.7200
2025-04-03 18:26:35,743 - INFO - Epoch 1, Batch 30: Training loss = 0.7100
2025-04-03 18:26:35,844 - INFO - Epoch 1, Batch 31: Training loss = 0.7000
2025-04-03 18:26:35,945 - INFO - Epoch 1, Batch 32: Training loss = 0.6900
2025-04-03 18:26:36,046 - INFO - Epoch 1, Batch 33: Training loss = 0.6800
2025-04-03 18:26:36,146 - INFO - Epoch 1, Batch 34: Training loss = 0.6700
2025-04-03 18:26:36,247 - INFO - Epoch 1, Batch 35: Training loss = 0.6600
2025-04-03 18:26:36,347 - INFO - Epoch 1, Batch 36: Training loss = 0.6500
2025-04-03 18:26:36,449 - INFO - Epoch 1, Batch 37: Training loss = 0.6400
2025-04-03 18:26:36,550 - INFO - Epoch 1, Batch 38: Training loss = 0.6300
2025-04-03 18:26:36,651 - INFO - Epoch 1, Batch 39: Training loss = 0.6200
2025-04-03 18:26:36,752 - INFO - Epoch 1, Batch 40: Training loss = 0.6100
2025-04-03 18:26:36,853 - INFO - Epoch 1, Batch 41: Training loss = 0.6000
2025-04-03 18:26:36,953 - INFO - Epoch 1, Batch 42: Training loss = 0.5900
2025-04-03 18:26:37,054 - INFO - Epoch 1, Batch 43: Training loss = 0.5800
2025-04-03 18:26:37,155 - INFO - Epoch 1, Batch 44: Training loss = 0.5700
2025-04-03 18:26:37,255 - INFO - Epoch 1, Batch 45: Training loss = 0.5600
2025-04-03 18:26:37,357 - INFO - Epoch 1, Batch 46: Training loss = 0.5500
2025-04-03 18:26:37,458 - INFO - Epoch 1, Batch 47: Training loss = 0.5400
2025-04-03 18:26:37,557 - INFO - Epoch 1, Batch 48: Training loss = 0.5300
2025-04-03 18:26:37,657 - INFO - Epoch 1, Batch 49: Training loss = 0.5200
2025-04-03 18:26:37,758 - INFO - Epoch 1, Batch 50: Training loss = 0.5100
2025-04-03 18:26:37,859 - INFO - Epoch 1, Batch 51: Training loss = 0.5000
2025-04-03 18:26:37,960 - INFO - Epoch 1, Batch 52: Training loss = 0.4900
2025-04-03 18:26:38,061 - INFO - Epoch 1, Batch 53: Training loss = 0.4800
2025-04-03 18:26:38,161 - INFO - Epoch 1, Batch 54: Training loss = 0.4700
2025-04-03 18:26:38,262 - INFO - Epoch 1, Batch 55: Training loss = 0.4600
2025-04-03 18:26:38,363 - INFO - Epoch 1, Batch 56: Training loss = 0.4500
2025-04-03 18:26:38,464 - INFO - Epoch 1, Batch 57: Training loss = 0.4400
2025-04-03 18:26:38,565 - INFO - Epoch 1, Batch 58: Training loss = 0.4300
2025-04-03 18:26:38,665 - INFO - Epoch 1, Batch 59: Training loss = 0.4200
2025-04-03 18:26:38,766 - INFO - Epoch 1, Batch 60: Training loss = 0.4100
2025-04-03 18:26:38,867 - INFO - Epoch 1, Batch 61: Training loss = 0.4000
2025-04-03 18:26:38,968 - INFO - Epoch 1, Batch 62: Training loss = 0.3900
2025-04-03 18:26:39,068 - INFO - Epoch 1, Batch 63: Training loss = 0.3800
2025-04-03 18:26:39,169 - INFO - Epoch 1, Batch 64: Training loss = 0.3700
2025-04-03 18:26:39,269 - INFO - Epoch 1, Batch 65: Training loss = 0.3600
2025-04-03 18:26:39,370 - INFO - Epoch 1, Batch 66: Training loss = 0.3500
2025-04-03 18:26:39,470 - INFO - Epoch 1, Batch 67: Training loss = 0.3400
2025-04-03 18:26:39,571 - INFO - Epoch 1, Batch 68: Training loss = 0.3300
2025-04-03 18:26:39,671 - INFO - Epoch 1, Batch 69: Training loss = 0.3200
2025-04-03 18:26:39,772 - INFO - Epoch 1, Batch 70: Training loss = 0.3100
2025-04-03 18:26:39,872 - INFO - Epoch 1, Batch 71: Training loss = 0.3000
2025-04-03 18:26:39,973 - INFO - Epoch 1, Batch 72: Training loss = 0.2900
2025-04-03 18:26:40,074 - INFO - Epoch 1, Batch 73: Training loss = 0.2800
2025-04-03 18:26:40,175 - INFO - Epoch 1, Batch 74: Training loss = 0.2700
2025-04-03 18:26:40,276 - INFO - Epoch 1, Batch 75: Training loss = 0.2600
2025-04-03 18:26:40,376 - INFO - Epoch 1, Batch 76: Training loss = 0.2500
2025-04-03 18:26:40,477 - INFO - Epoch 1, Batch 77: Training loss = 0.2400
2025-04-03 18:26:40,577 - INFO - Epoch 1, Batch 78: Training loss = 0.2300
2025-04-03 18:26:40,678 - INFO - Epoch 1, Batch 79: Training loss = 0.2200
2025-04-03 18:26:40,779 - INFO - Epoch 1, Batch 80: Training loss = 0.2100
2025-04-03 18:26:40,880 - INFO - Epoch 1, Batch 81: Training loss = 0.2000
2025-04-03 18:26:40,980 - INFO - Epoch 1, Batch 82: Training loss = 0.1900
2025-04-03 18:26:41,081 - INFO - Epoch 1, Batch 83: Training loss = 0.1800
2025-04-03 18:26:41,181 - INFO - Epoch 1, Batch 84: Training loss = 0.1700
2025-04-03 18:26:41,282 - INFO - Epoch 1, Batch 85: Training loss = 0.1600
2025-04-03 18:26:41,382 - INFO - Epoch 1, Batch 86: Training loss = 0.1500
2025-04-03 18:26:41,483 - INFO - Epoch 1, Batch 87: Training loss = 0.1400
2025-04-03 18:26:41,584 - INFO - Epoch 1, Batch 88: Training loss = 0.1300
2025-04-03 18:26:41,685 - INFO - Epoch 1, Batch 89: Training loss = 0.1200
2025-04-03 18:26:41,785 - INFO - Epoch 1, Batch 90: Training loss = 0.1100
2025-04-03 18:26:41,886 - INFO - Epoch 1, Batch 91: Training loss = 0.1000
2025-04-03 18:26:41,987 - INFO - Epoch 1, Batch 92: Training loss = 0.0900
2025-04-03 18:26:42,088 - INFO - Epoch 1, Batch 93: Training loss = 0.0800
2025-04-03 18:26:42,188 - INFO - Epoch 1, Batch 94: Training loss = 0.0700
2025-04-03 18:26:42,289 - INFO - Epoch 1, Batch 95: Training loss = 0.0600
2025-04-03 18:26:42,390 - INFO - Epoch 1, Batch 96: Training loss = 0.0500
2025-04-03 18:26:42,490 - INFO - Epoch 1, Batch 97: Training loss = 0.0400
2025-04-03 18:26:42,591 - INFO - Epoch 1, Batch 98: Training loss = 0.0300
2025-04-03 18:26:42,692 - INFO - Epoch 1, Batch 99: Training loss = 0.0200
2025-04-03 18:26:42,793 - INFO - Epoch 1, Batch 100: Training loss = 0.0100
2025-04-03 18:26:42,894 - INFO - Epoch 1: Validation loss = 0.1000, Validation accuracy = 0.8000
2025-04-03 18:26:42,894 - INFO - Epoch 1: MSE = 0.1000, RMSE = 0.3162, MAE = 0.0800, RÂ² = 0.9000
2025-04-03 18:26:42,894 - INFO - Epoch 2 started.
2025-04-03 18:26:42,895 - INFO - Epoch 2, Batch 1: Training loss = 1.0000
2025-04-03 18:26:42,995 - INFO - Epoch 2, Batch 2: Training loss = 0.9900
2025-04-03 18:26:43,096 - INFO - Epoch 2, Batch 3: Training loss = 0.9800
2025-04-03 18:26:43,197 - INFO - Epoch 2, Batch 4: Training loss = 0.9700
2025-04-03 18:26:43,298 - INFO - Epoch 2, Batch 5: Training loss = 0.9600
2025-04-03 18:26:43,398 - INFO - Epoch 2, Batch 6: Training loss = 0.9500
2025-04-03 18:26:43,500 - INFO - Epoch 2, Batch 7: Training loss = 0.9400
2025-04-03 18:26:43,601 - INFO - Epoch 2, Batch 8: Training loss = 0.9300
2025-04-03 18:26:43,701 - INFO - Epoch 2, Batch 9: Training loss = 0.9200
2025-04-03 18:26:43,803 - INFO - Epoch 2, Batch 10: Training loss = 0.9100
2025-04-03 18:26:43,903 - INFO - Epoch 2, Batch 11: Training loss = 0.9000
2025-04-03 18:26:44,004 - INFO - Epoch 2, Batch 12: Training loss = 0.8900
2025-04-03 18:26:44,105 - INFO - Epoch 2, Batch 13: Training loss = 0.8800
2025-04-03 18:26:44,206 - INFO - Epoch 2, Batch 14: Training loss = 0.8700
2025-04-03 18:26:44,306 - INFO - Epoch 2, Batch 15: Training loss = 0.8600
2025-04-03 18:26:44,407 - INFO - Epoch 2, Batch 16: Training loss = 0.8500
2025-04-03 18:26:44,508 - INFO - Epoch 2, Batch 17: Training loss = 0.8400
2025-04-03 18:26:44,609 - INFO - Epoch 2, Batch 18: Training loss = 0.8300
2025-04-03 18:26:44,710 - INFO - Epoch 2, Batch 19: Training loss = 0.8200
2025-04-03 18:26:44,811 - INFO - Epoch 2, Batch 20: Training loss = 0.8100
2025-04-03 18:26:44,911 - INFO - Epoch 2, Batch 21: Training loss = 0.8000
2025-04-03 18:26:45,012 - INFO - Epoch 2, Batch 22: Training loss = 0.7900
2025-04-03 18:26:45,113 - INFO - Epoch 2, Batch 23: Training loss = 0.7800
2025-04-03 18:26:45,214 - INFO - Epoch 2, Batch 24: Training loss = 0.7700
2025-04-03 18:26:45,314 - INFO - Epoch 2, Batch 25: Training loss = 0.7600
2025-04-03 18:26:45,415 - INFO - Epoch 2, Batch 26: Training loss = 0.7500
2025-04-03 18:26:45,516 - INFO - Epoch 2, Batch 27: Training loss = 0.7400
2025-04-03 18:26:45,616 - INFO - Epoch 2, Batch 28: Training loss = 0.7300
2025-04-03 18:26:45,717 - INFO - Epoch 2, Batch 29: Training loss = 0.7200
2025-04-03 18:26:45,817 - INFO - Epoch 2, Batch 30: Training loss = 0.7100
2025-04-03 18:26:45,918 - INFO - Epoch 2, Batch 31: Training loss = 0.7000
2025-04-03 18:26:46,018 - INFO - Epoch 2, Batch 32: Training loss = 0.6900
2025-04-03 18:26:46,119 - INFO - Epoch 2, Batch 33: Training loss = 0.6800
2025-04-03 18:26:46,219 - INFO - Epoch 2, Batch 34: Training loss = 0.6700
2025-04-03 18:26:46,320 - INFO - Epoch 2, Batch 35: Training loss = 0.6600
2025-04-03 18:26:46,422 - INFO - Epoch 2, Batch 36: Training loss = 0.6500
2025-04-03 18:26:46,523 - INFO - Epoch 2, Batch 37: Training loss = 0.6400
2025-04-03 18:26:46,627 - INFO - Epoch 2, Batch 38: Training loss = 0.6300
2025-04-03 18:26:46,728 - INFO - Epoch 2, Batch 39: Training loss = 0.6200
2025-04-03 18:26:46,828 - INFO - Epoch 2, Batch 40: Training loss = 0.6100
2025-04-03 18:26:46,929 - INFO - Epoch 2, Batch 41: Training loss = 0.6000
2025-04-03 18:26:47,029 - INFO - Epoch 2, Batch 42: Training loss = 0.5900
2025-04-03 18:26:47,130 - INFO - Epoch 2, Batch 43: Training loss = 0.5800
2025-04-03 18:26:47,230 - INFO - Epoch 2, Batch 44: Training loss = 0.5700
2025-04-03 18:26:47,331 - INFO - Epoch 2, Batch 45: Training loss = 0.5600
2025-04-03 18:26:47,431 - INFO - Epoch 2, Batch 46: Training loss = 0.5500
2025-04-03 18:26:47,532 - INFO - Epoch 2, Batch 47: Training loss = 0.5400
2025-04-03 18:26:47,632 - INFO - Epoch 2, Batch 48: Training loss = 0.5300
2025-04-03 18:26:47,733 - INFO - Epoch 2, Batch 49: Training loss = 0.5200
2025-04-03 18:26:47,833 - INFO - Epoch 2, Batch 50: Training loss = 0.5100
2025-04-03 18:26:47,934 - INFO - Epoch 2, Batch 51: Training loss = 0.5000
2025-04-03 18:26:48,035 - INFO - Epoch 2, Batch 52: Training loss = 0.4900
2025-04-03 18:26:48,135 - INFO - Epoch 2, Batch 53: Training loss = 0.4800
2025-04-03 18:26:48,236 - INFO - Epoch 2, Batch 54: Training loss = 0.4700
2025-04-03 18:26:48,336 - INFO - Epoch 2, Batch 55: Training loss = 0.4600
2025-04-03 18:26:48,436 - INFO - Epoch 2, Batch 56: Training loss = 0.4500
2025-04-03 18:26:48,537 - INFO - Epoch 2, Batch 57: Training loss = 0.4400
2025-04-03 18:26:48,638 - INFO - Epoch 2, Batch 58: Training loss = 0.4300
2025-04-03 18:26:48,738 - INFO - Epoch 2, Batch 59: Training loss = 0.4200
2025-04-03 18:26:48,839 - INFO - Epoch 2, Batch 60: Training loss = 0.4100
2025-04-03 18:26:48,939 - INFO - Epoch 2, Batch 61: Training loss = 0.4000
2025-04-03 18:26:49,040 - INFO - Epoch 2, Batch 62: Training loss = 0.3900
2025-04-03 18:26:49,140 - INFO - Epoch 2, Batch 63: Training loss = 0.3800
2025-04-03 18:26:49,241 - INFO - Epoch 2, Batch 64: Training loss = 0.3700
2025-04-03 18:26:49,342 - INFO - Epoch 2, Batch 65: Training loss = 0.3600
2025-04-03 18:26:49,442 - INFO - Epoch 2, Batch 66: Training loss = 0.3500
2025-04-03 18:26:49,543 - INFO - Epoch 2, Batch 67: Training loss = 0.3400
2025-04-03 18:26:49,643 - INFO - Epoch 2, Batch 68: Training loss = 0.3300
2025-04-03 18:26:49,744 - INFO - Epoch 2, Batch 69: Training loss = 0.3200
2025-04-03 18:26:49,845 - INFO - Epoch 2, Batch 70: Training loss = 0.3100
2025-04-03 18:26:49,945 - INFO - Epoch 2, Batch 71: Training loss = 0.3000
2025-04-03 18:26:50,046 - INFO - Epoch 2, Batch 72: Training loss = 0.2900
2025-04-03 18:26:50,146 - INFO - Epoch 2, Batch 73: Training loss = 0.2800
2025-04-03 18:26:50,247 - INFO - Epoch 2, Batch 74: Training loss = 0.2700
2025-04-03 18:26:50,347 - INFO - Epoch 2, Batch 75: Training loss = 0.2600
2025-04-03 18:26:50,448 - INFO - Epoch 2, Batch 76: Training loss = 0.2500
2025-04-03 18:26:50,549 - INFO - Epoch 2, Batch 77: Training loss = 0.2400
2025-04-03 18:26:50,649 - INFO - Epoch 2, Batch 78: Training loss = 0.2300
2025-04-03 18:26:50,750 - INFO - Epoch 2, Batch 79: Training loss = 0.2200
2025-04-03 18:26:50,851 - INFO - Epoch 2, Batch 80: Training loss = 0.2100
2025-04-03 18:26:50,952 - INFO - Epoch 2, Batch 81: Training loss = 0.2000
2025-04-03 18:26:51,052 - INFO - Epoch 2, Batch 82: Training loss = 0.1900
2025-04-03 18:26:51,153 - INFO - Epoch 2, Batch 83: Training loss = 0.1800
2025-04-03 18:26:51,254 - INFO - Epoch 2, Batch 84: Training loss = 0.1700
2025-04-03 18:26:51,354 - INFO - Epoch 2, Batch 85: Training loss = 0.1600
2025-04-03 18:26:51,454 - INFO - Epoch 2, Batch 86: Training loss = 0.1500
2025-04-03 18:26:51,555 - INFO - Epoch 2, Batch 87: Training loss = 0.1400
2025-04-03 18:26:51,655 - INFO - Epoch 2, Batch 88: Training loss = 0.1300
2025-04-03 18:26:51,756 - INFO - Epoch 2, Batch 89: Training loss = 0.1200
2025-04-03 18:26:51,856 - INFO - Epoch 2, Batch 90: Training loss = 0.1100
2025-04-03 18:26:51,957 - INFO - Epoch 2, Batch 91: Training loss = 0.1000
2025-04-03 18:26:52,057 - INFO - Epoch 2, Batch 92: Training loss = 0.0900
2025-04-03 18:26:52,158 - INFO - Epoch 2, Batch 93: Training loss = 0.0800
2025-04-03 18:26:52,258 - INFO - Epoch 2, Batch 94: Training loss = 0.0700
2025-04-03 18:26:52,359 - INFO - Epoch 2, Batch 95: Training loss = 0.0600
2025-04-03 18:26:52,459 - INFO - Epoch 2, Batch 96: Training loss = 0.0500
2025-04-03 18:26:52,560 - INFO - Epoch 2, Batch 97: Training loss = 0.0400
2025-04-03 18:26:52,661 - INFO - Epoch 2, Batch 98: Training loss = 0.0300
2025-04-03 18:26:52,761 - INFO - Epoch 2, Batch 99: Training loss = 0.0200
2025-04-03 18:26:52,862 - INFO - Epoch 2, Batch 100: Training loss = 0.0100
2025-04-03 18:26:52,962 - INFO - Epoch 2: Validation loss = 0.0900, Validation accuracy = 0.8200
2025-04-03 18:26:52,962 - INFO - Epoch 2: MSE = 0.0900, RMSE = 0.3000, MAE = 0.0720, RÂ² = 0.9100
2025-04-03 18:26:52,963 - INFO - Epoch 3 started.
2025-04-03 18:26:52,963 - INFO - Epoch 3, Batch 1: Training loss = 1.0000
2025-04-03 18:26:53,063 - INFO - Epoch 3, Batch 2: Training loss = 0.9900
2025-04-03 18:26:53,164 - INFO - Epoch 3, Batch 3: Training loss = 0.9800
2025-04-03 18:26:53,264 - INFO - Epoch 3, Batch 4: Training loss = 0.9700
2025-04-03 18:26:53,365 - INFO - Epoch 3, Batch 5: Training loss = 0.9600
2025-04-03 18:26:53,465 - INFO - Epoch 3, Batch 6: Training loss = 0.9500
2025-04-03 18:26:53,566 - INFO - Epoch 3, Batch 7: Training loss = 0.9400
2025-04-03 18:26:53,667 - INFO - Epoch 3, Batch 8: Training loss = 0.9300
2025-04-03 18:26:53,768 - INFO - Epoch 3, Batch 9: Training loss = 0.9200
2025-04-03 18:26:53,869 - INFO - Epoch 3, Batch 10: Training loss = 0.9100
2025-04-03 18:26:53,970 - INFO - Epoch 3, Batch 11: Training loss = 0.9000
2025-04-03 18:26:54,070 - INFO - Epoch 3, Batch 12: Training loss = 0.8900
2025-04-03 18:26:54,172 - INFO - Epoch 3, Batch 13: Training loss = 0.8800
2025-04-03 18:26:54,272 - INFO - Epoch 3, Batch 14: Training loss = 0.8700
2025-04-03 18:26:54,373 - INFO - Epoch 3, Batch 15: Training loss = 0.8600
2025-04-03 18:26:54,473 - INFO - Epoch 3, Batch 16: Training loss = 0.8500
2025-04-03 18:26:54,574 - INFO - Epoch 3, Batch 17: Training loss = 0.8400
2025-04-03 18:26:54,674 - INFO - Epoch 3, Batch 18: Training loss = 0.8300
2025-04-03 18:26:54,775 - INFO - Epoch 3, Batch 19: Training loss = 0.8200
2025-04-03 18:26:54,875 - INFO - Epoch 3, Batch 20: Training loss = 0.8100
2025-04-03 18:26:54,976 - INFO - Epoch 3, Batch 21: Training loss = 0.8000
2025-04-03 18:26:55,077 - INFO - Epoch 3, Batch 22: Training loss = 0.7900
2025-04-03 18:26:55,178 - INFO - Epoch 3, Batch 23: Training loss = 0.7800
2025-04-03 18:26:55,278 - INFO - Epoch 3, Batch 24: Training loss = 0.7700
2025-04-03 18:26:55,379 - INFO - Epoch 3, Batch 25: Training loss = 0.7600
2025-04-03 18:26:55,480 - INFO - Epoch 3, Batch 26: Training loss = 0.7500
2025-04-03 18:26:55,581 - INFO - Epoch 3, Batch 27: Training loss = 0.7400
2025-04-03 18:26:55,682 - INFO - Epoch 3, Batch 28: Training loss = 0.7300
2025-04-03 18:26:55,783 - INFO - Epoch 3, Batch 29: Training loss = 0.7200
2025-04-03 18:26:55,884 - INFO - Epoch 3, Batch 30: Training loss = 0.7100
2025-04-03 18:26:55,984 - INFO - Epoch 3, Batch 31: Training loss = 0.7000
2025-04-03 18:26:56,085 - INFO - Epoch 3, Batch 32: Training loss = 0.6900
2025-04-03 18:26:56,186 - INFO - Epoch 3, Batch 33: Training loss = 0.6800
2025-04-03 18:26:56,286 - INFO - Epoch 3, Batch 34: Training loss = 0.6700
2025-04-03 18:26:56,388 - INFO - Epoch 3, Batch 35: Training loss = 0.6600
2025-04-03 18:26:56,489 - INFO - Epoch 3, Batch 36: Training loss = 0.6500
2025-04-03 18:26:56,589 - INFO - Epoch 3, Batch 37: Training loss = 0.6400
2025-04-03 18:26:56,690 - INFO - Epoch 3, Batch 38: Training loss = 0.6300
2025-04-03 18:26:56,791 - INFO - Epoch 3, Batch 39: Training loss = 0.6200
2025-04-03 18:26:56,892 - INFO - Epoch 3, Batch 40: Training loss = 0.6100
2025-04-03 18:26:56,992 - INFO - Epoch 3, Batch 41: Training loss = 0.6000
2025-04-03 18:26:57,094 - INFO - Epoch 3, Batch 42: Training loss = 0.5900
2025-04-03 18:26:57,195 - INFO - Epoch 3, Batch 43: Training loss = 0.5800
2025-04-03 18:26:57,296 - INFO - Epoch 3, Batch 44: Training loss = 0.5700
2025-04-03 18:26:57,397 - INFO - Epoch 3, Batch 45: Training loss = 0.5600
2025-04-03 18:26:57,498 - INFO - Epoch 3, Batch 46: Training loss = 0.5500
2025-04-03 18:26:57,599 - INFO - Epoch 3, Batch 47: Training loss = 0.5400
2025-04-03 18:26:57,700 - INFO - Epoch 3, Batch 48: Training loss = 0.5300
2025-04-03 18:26:57,801 - INFO - Epoch 3, Batch 49: Training loss = 0.5200
2025-04-03 18:26:57,902 - INFO - Epoch 3, Batch 50: Training loss = 0.5100
2025-04-03 18:26:58,003 - INFO - Epoch 3, Batch 51: Training loss = 0.5000
2025-04-03 18:26:58,104 - INFO - Epoch 3, Batch 52: Training loss = 0.4900
2025-04-03 18:26:58,204 - INFO - Epoch 3, Batch 53: Training loss = 0.4800
2025-04-03 18:26:58,305 - INFO - Epoch 3, Batch 54: Training loss = 0.4700
2025-04-03 18:26:58,406 - INFO - Epoch 3, Batch 55: Training loss = 0.4600
2025-04-03 18:26:58,508 - INFO - Epoch 3, Batch 56: Training loss = 0.4500
2025-04-03 18:26:58,609 - INFO - Epoch 3, Batch 57: Training loss = 0.4400
2025-04-03 18:26:58,709 - INFO - Epoch 3, Batch 58: Training loss = 0.4300
2025-04-03 18:26:58,810 - INFO - Epoch 3, Batch 59: Training loss = 0.4200
2025-04-03 18:26:58,911 - INFO - Epoch 3, Batch 60: Training loss = 0.4100
2025-04-03 18:26:59,012 - INFO - Epoch 3, Batch 61: Training loss = 0.4000
2025-04-03 18:26:59,113 - INFO - Epoch 3, Batch 62: Training loss = 0.3900
2025-04-03 18:26:59,213 - INFO - Epoch 3, Batch 63: Training loss = 0.3800
2025-04-03 18:26:59,314 - INFO - Epoch 3, Batch 64: Training loss = 0.3700
2025-04-03 18:26:59,414 - INFO - Epoch 3, Batch 65: Training loss = 0.3600
2025-04-03 18:26:59,515 - INFO - Epoch 3, Batch 66: Training loss = 0.3500
2025-04-03 18:26:59,616 - INFO - Epoch 3, Batch 67: Training loss = 0.3400
2025-04-03 18:26:59,717 - INFO - Epoch 3, Batch 68: Training loss = 0.3300
2025-04-03 18:26:59,818 - INFO - Epoch 3, Batch 69: Training loss = 0.3200
2025-04-03 18:26:59,919 - INFO - Epoch 3, Batch 70: Training loss = 0.3100
2025-04-03 18:27:00,020 - INFO - Epoch 3, Batch 71: Training loss = 0.3000
2025-04-03 18:27:00,121 - INFO - Epoch 3, Batch 72: Training loss = 0.2900
2025-04-03 18:27:00,221 - INFO - Epoch 3, Batch 73: Training loss = 0.2800
2025-04-03 18:27:00,322 - INFO - Epoch 3, Batch 74: Training loss = 0.2700
2025-04-03 18:27:00,422 - INFO - Epoch 3, Batch 75: Training loss = 0.2600
2025-04-03 18:27:00,523 - INFO - Epoch 3, Batch 76: Training loss = 0.2500
2025-04-03 18:27:00,623 - INFO - Epoch 3, Batch 77: Training loss = 0.2400
2025-04-03 18:27:00,724 - INFO - Epoch 3, Batch 78: Training loss = 0.2300
2025-04-03 18:27:00,825 - INFO - Epoch 3, Batch 79: Training loss = 0.2200
2025-04-03 18:27:00,925 - INFO - Epoch 3, Batch 80: Training loss = 0.2100
2025-04-03 18:27:01,025 - INFO - Epoch 3, Batch 81: Training loss = 0.2000
2025-04-03 18:27:01,126 - INFO - Epoch 3, Batch 82: Training loss = 0.1900
2025-04-03 18:27:01,227 - INFO - Epoch 3, Batch 83: Training loss = 0.1800
2025-04-03 18:27:01,327 - INFO - Epoch 3, Batch 84: Training loss = 0.1700
2025-04-03 18:27:01,428 - INFO - Epoch 3, Batch 85: Training loss = 0.1600
2025-04-03 18:27:01,528 - INFO - Epoch 3, Batch 86: Training loss = 0.1500
2025-04-03 18:27:01,629 - INFO - Epoch 3, Batch 87: Training loss = 0.1400
2025-04-03 18:27:01,730 - INFO - Epoch 3, Batch 88: Training loss = 0.1300
2025-04-03 18:27:01,830 - INFO - Epoch 3, Batch 89: Training loss = 0.1200
2025-04-03 18:27:01,931 - INFO - Epoch 3, Batch 90: Training loss = 0.1100
2025-04-03 18:27:02,032 - INFO - Epoch 3, Batch 91: Training loss = 0.1000
2025-04-03 18:27:02,132 - INFO - Epoch 3, Batch 92: Training loss = 0.0900
2025-04-03 18:27:02,233 - INFO - Epoch 3, Batch 93: Training loss = 0.0800
2025-04-03 18:27:02,334 - INFO - Epoch 3, Batch 94: Training loss = 0.0700
2025-04-03 18:27:02,435 - INFO - Epoch 3, Batch 95: Training loss = 0.0600
2025-04-03 18:27:02,535 - INFO - Epoch 3, Batch 96: Training loss = 0.0500
2025-04-03 18:27:02,636 - INFO - Epoch 3, Batch 97: Training loss = 0.0400
2025-04-03 18:27:02,737 - INFO - Epoch 3, Batch 98: Training loss = 0.0300
2025-04-03 18:27:02,837 - INFO - Epoch 3, Batch 99: Training loss = 0.0200
2025-04-03 18:27:02,938 - INFO - Epoch 3, Batch 100: Training loss = 0.0100
2025-04-03 18:27:03,038 - INFO - Epoch 3: Validation loss = 0.0800, Validation accuracy = 0.8400
2025-04-03 18:27:03,038 - INFO - Epoch 3: MSE = 0.0800, RMSE = 0.2828, MAE = 0.0640, RÂ² = 0.9200
2025-04-03 18:27:03,038 - INFO - Epoch 4 started.
2025-04-03 18:27:03,039 - INFO - Epoch 4, Batch 1: Training loss = 1.0000
2025-04-03 18:27:03,139 - INFO - Epoch 4, Batch 2: Training loss = 0.9900
2025-04-03 18:27:03,239 - INFO - Epoch 4, Batch 3: Training loss = 0.9800
2025-04-03 18:27:03,340 - INFO - Epoch 4, Batch 4: Training loss = 0.9700
2025-04-03 18:27:03,440 - INFO - Epoch 4, Batch 5: Training loss = 0.9600
2025-04-03 18:27:03,541 - INFO - Epoch 4, Batch 6: Training loss = 0.9500
2025-04-03 18:27:03,641 - INFO - Epoch 4, Batch 7: Training loss = 0.9400
2025-04-03 18:27:03,742 - INFO - Epoch 4, Batch 8: Training loss = 0.9300
2025-04-03 18:27:03,843 - INFO - Epoch 4, Batch 9: Training loss = 0.9200
2025-04-03 18:27:03,943 - INFO - Epoch 4, Batch 10: Training loss = 0.9100
2025-04-03 18:27:04,044 - INFO - Epoch 4, Batch 11: Training loss = 0.9000
2025-04-03 18:27:04,145 - INFO - Epoch 4, Batch 12: Training loss = 0.8900
2025-04-03 18:27:04,246 - INFO - Epoch 4, Batch 13: Training loss = 0.8800
2025-04-03 18:27:04,347 - INFO - Epoch 4, Batch 14: Training loss = 0.8700
2025-04-03 18:27:04,447 - INFO - Epoch 4, Batch 15: Training loss = 0.8600
2025-04-03 18:27:04,548 - INFO - Epoch 4, Batch 16: Training loss = 0.8500
2025-04-03 18:27:04,649 - INFO - Epoch 4, Batch 17: Training loss = 0.8400
2025-04-03 18:27:04,750 - INFO - Epoch 4, Batch 18: Training loss = 0.8300
2025-04-03 18:27:04,850 - INFO - Epoch 4, Batch 19: Training loss = 0.8200
2025-04-03 18:27:04,951 - INFO - Epoch 4, Batch 20: Training loss = 0.8100
2025-04-03 18:27:05,052 - INFO - Epoch 4, Batch 21: Training loss = 0.8000
2025-04-03 18:27:05,152 - INFO - Epoch 4, Batch 22: Training loss = 0.7900
2025-04-03 18:27:05,253 - INFO - Epoch 4, Batch 23: Training loss = 0.7800
2025-04-03 18:27:05,353 - INFO - Epoch 4, Batch 24: Training loss = 0.7700
2025-04-03 18:27:05,454 - INFO - Epoch 4, Batch 25: Training loss = 0.7600
2025-04-03 18:27:05,554 - INFO - Epoch 4, Batch 26: Training loss = 0.7500
2025-04-03 18:27:05,654 - INFO - Epoch 4, Batch 27: Training loss = 0.7400
2025-04-03 18:27:05,755 - INFO - Epoch 4, Batch 28: Training loss = 0.7300
2025-04-03 18:27:05,856 - INFO - Epoch 4, Batch 29: Training loss = 0.7200
2025-04-03 18:27:05,956 - INFO - Epoch 4, Batch 30: Training loss = 0.7100
2025-04-03 18:27:06,057 - INFO - Epoch 4, Batch 31: Training loss = 0.7000
2025-04-03 18:27:06,157 - INFO - Epoch 4, Batch 32: Training loss = 0.6900
2025-04-03 18:27:06,258 - INFO - Epoch 4, Batch 33: Training loss = 0.6800
2025-04-03 18:27:06,359 - INFO - Epoch 4, Batch 34: Training loss = 0.6700
2025-04-03 18:27:06,460 - INFO - Epoch 4, Batch 35: Training loss = 0.6600
2025-04-03 18:27:06,561 - INFO - Epoch 4, Batch 36: Training loss = 0.6500
2025-04-03 18:27:06,661 - INFO - Epoch 4, Batch 37: Training loss = 0.6400
2025-04-03 18:27:06,762 - INFO - Epoch 4, Batch 38: Training loss = 0.6300
2025-04-03 18:27:06,862 - INFO - Epoch 4, Batch 39: Training loss = 0.6200
2025-04-03 18:27:06,963 - INFO - Epoch 4, Batch 40: Training loss = 0.6100
2025-04-03 18:27:07,063 - INFO - Epoch 4, Batch 41: Training loss = 0.6000
2025-04-03 18:27:07,164 - INFO - Epoch 4, Batch 42: Training loss = 0.5900
2025-04-03 18:27:07,265 - INFO - Epoch 4, Batch 43: Training loss = 0.5800
2025-04-03 18:27:07,365 - INFO - Epoch 4, Batch 44: Training loss = 0.5700
2025-04-03 18:27:07,466 - INFO - Epoch 4, Batch 45: Training loss = 0.5600
2025-04-03 18:27:07,565 - INFO - Epoch 4, Batch 46: Training loss = 0.5500
2025-04-03 18:27:07,665 - INFO - Epoch 4, Batch 47: Training loss = 0.5400
2025-04-03 18:27:07,766 - INFO - Epoch 4, Batch 48: Training loss = 0.5300
2025-04-03 18:27:07,868 - INFO - Epoch 4, Batch 49: Training loss = 0.5200
2025-04-03 18:27:07,968 - INFO - Epoch 4, Batch 50: Training loss = 0.5100
2025-04-03 18:27:08,069 - INFO - Epoch 4, Batch 51: Training loss = 0.5000
2025-04-03 18:27:08,169 - INFO - Epoch 4, Batch 52: Training loss = 0.4900
2025-04-03 18:27:08,269 - INFO - Epoch 4, Batch 53: Training loss = 0.4800
2025-04-03 18:27:08,370 - INFO - Epoch 4, Batch 54: Training loss = 0.4700
2025-04-03 18:27:08,470 - INFO - Epoch 4, Batch 55: Training loss = 0.4600
2025-04-03 18:27:08,571 - INFO - Epoch 4, Batch 56: Training loss = 0.4500
2025-04-03 18:27:08,672 - INFO - Epoch 4, Batch 57: Training loss = 0.4400
2025-04-03 18:27:08,772 - INFO - Epoch 4, Batch 58: Training loss = 0.4300
2025-04-03 18:27:08,873 - INFO - Epoch 4, Batch 59: Training loss = 0.4200
2025-04-03 18:27:08,973 - INFO - Epoch 4, Batch 60: Training loss = 0.4100
2025-04-03 18:27:09,074 - INFO - Epoch 4, Batch 61: Training loss = 0.4000
2025-04-03 18:27:09,174 - INFO - Epoch 4, Batch 62: Training loss = 0.3900
2025-04-03 18:27:09,275 - INFO - Epoch 4, Batch 63: Training loss = 0.3800
2025-04-03 18:27:09,376 - INFO - Epoch 4, Batch 64: Training loss = 0.3700
2025-04-03 18:27:09,476 - INFO - Epoch 4, Batch 65: Training loss = 0.3600
2025-04-03 18:27:09,577 - INFO - Epoch 4, Batch 66: Training loss = 0.3500
2025-04-03 18:27:09,678 - INFO - Epoch 4, Batch 67: Training loss = 0.3400
2025-04-03 18:27:09,779 - INFO - Epoch 4, Batch 68: Training loss = 0.3300
2025-04-03 18:27:09,879 - INFO - Epoch 4, Batch 69: Training loss = 0.3200
2025-04-03 18:27:09,980 - INFO - Epoch 4, Batch 70: Training loss = 0.3100
2025-04-03 18:27:10,081 - INFO - Epoch 4, Batch 71: Training loss = 0.3000
2025-04-03 18:27:10,182 - INFO - Epoch 4, Batch 72: Training loss = 0.2900
2025-04-03 18:27:10,282 - INFO - Epoch 4, Batch 73: Training loss = 0.2800
2025-04-03 18:27:10,383 - INFO - Epoch 4, Batch 74: Training loss = 0.2700
2025-04-03 18:27:10,483 - INFO - Epoch 4, Batch 75: Training loss = 0.2600
2025-04-03 18:27:10,584 - INFO - Epoch 4, Batch 76: Training loss = 0.2500
2025-04-03 18:27:10,684 - INFO - Epoch 4, Batch 77: Training loss = 0.2400
2025-04-03 18:27:10,785 - INFO - Epoch 4, Batch 78: Training loss = 0.2300
2025-04-03 18:27:10,886 - INFO - Epoch 4, Batch 79: Training loss = 0.2200
2025-04-03 18:27:10,986 - INFO - Epoch 4, Batch 80: Training loss = 0.2100
2025-04-03 18:27:11,086 - INFO - Epoch 4, Batch 81: Training loss = 0.2000
2025-04-03 18:27:11,187 - INFO - Epoch 4, Batch 82: Training loss = 0.1900
2025-04-03 18:27:11,288 - INFO - Epoch 4, Batch 83: Training loss = 0.1800
2025-04-03 18:27:11,388 - INFO - Epoch 4, Batch 84: Training loss = 0.1700
2025-04-03 18:27:11,489 - INFO - Epoch 4, Batch 85: Training loss = 0.1600
2025-04-03 18:27:11,589 - INFO - Epoch 4, Batch 86: Training loss = 0.1500
2025-04-03 18:27:11,690 - INFO - Epoch 4, Batch 87: Training loss = 0.1400
2025-04-03 18:27:11,791 - INFO - Epoch 4, Batch 88: Training loss = 0.1300
2025-04-03 18:27:11,891 - INFO - Epoch 4, Batch 89: Training loss = 0.1200
2025-04-03 18:27:11,992 - INFO - Epoch 4, Batch 90: Training loss = 0.1100
2025-04-03 18:27:12,092 - INFO - Epoch 4, Batch 91: Training loss = 0.1000
2025-04-03 18:27:12,193 - INFO - Epoch 4, Batch 92: Training loss = 0.0900
2025-04-03 18:27:12,294 - INFO - Epoch 4, Batch 93: Training loss = 0.0800
2025-04-03 18:27:12,394 - INFO - Epoch 4, Batch 94: Training loss = 0.0700
2025-04-03 18:27:12,495 - INFO - Epoch 4, Batch 95: Training loss = 0.0600
2025-04-03 18:27:12,596 - INFO - Epoch 4, Batch 96: Training loss = 0.0500
2025-04-03 18:27:12,697 - INFO - Epoch 4, Batch 97: Training loss = 0.0400
2025-04-03 18:27:12,799 - INFO - Epoch 4, Batch 98: Training loss = 0.0300
2025-04-03 18:27:12,900 - INFO - Epoch 4, Batch 99: Training loss = 0.0200
2025-04-03 18:27:13,000 - INFO - Epoch 4, Batch 100: Training loss = 0.0100
2025-04-03 18:27:13,101 - INFO - Epoch 4: Validation loss = 0.0700, Validation accuracy = 0.8600
2025-04-03 18:27:13,102 - INFO - Epoch 4: MSE = 0.0700, RMSE = 0.2646, MAE = 0.0560, RÂ² = 0.9300
2025-04-03 18:27:13,102 - INFO - Epoch 5 started.
2025-04-03 18:27:13,103 - INFO - Epoch 5, Batch 1: Training loss = 1.0000
2025-04-03 18:27:13,203 - INFO - Epoch 5, Batch 2: Training loss = 0.9900
2025-04-03 18:27:13,304 - INFO - Epoch 5, Batch 3: Training loss = 0.9800
2025-04-03 18:27:13,405 - INFO - Epoch 5, Batch 4: Training loss = 0.9700
2025-04-03 18:27:13,506 - INFO - Epoch 5, Batch 5: Training loss = 0.9600
2025-04-03 18:27:13,607 - INFO - Epoch 5, Batch 6: Training loss = 0.9500
2025-04-03 18:27:13,708 - INFO - Epoch 5, Batch 7: Training loss = 0.9400
2025-04-03 18:27:13,809 - INFO - Epoch 5, Batch 8: Training loss = 0.9300
2025-04-03 18:27:13,910 - INFO - Epoch 5, Batch 9: Training loss = 0.9200
2025-04-03 18:27:14,011 - INFO - Epoch 5, Batch 10: Training loss = 0.9100
2025-04-03 18:27:14,111 - INFO - Epoch 5, Batch 11: Training loss = 0.9000
2025-04-03 18:27:14,212 - INFO - Epoch 5, Batch 12: Training loss = 0.8900
2025-04-03 18:27:14,313 - INFO - Epoch 5, Batch 13: Training loss = 0.8800
2025-04-03 18:27:14,414 - INFO - Epoch 5, Batch 14: Training loss = 0.8700
2025-04-03 18:27:14,515 - INFO - Epoch 5, Batch 15: Training loss = 0.8600
2025-04-03 18:27:14,616 - INFO - Epoch 5, Batch 16: Training loss = 0.8500
2025-04-03 18:27:14,717 - INFO - Epoch 5, Batch 17: Training loss = 0.8400
2025-04-03 18:27:14,818 - INFO - Epoch 5, Batch 18: Training loss = 0.8300
2025-04-03 18:27:14,919 - INFO - Epoch 5, Batch 19: Training loss = 0.8200
2025-04-03 18:27:15,019 - INFO - Epoch 5, Batch 20: Training loss = 0.8100
2025-04-03 18:27:15,120 - INFO - Epoch 5, Batch 21: Training loss = 0.8000
2025-04-03 18:27:15,220 - INFO - Epoch 5, Batch 22: Training loss = 0.7900
2025-04-03 18:27:15,321 - INFO - Epoch 5, Batch 23: Training loss = 0.7800
2025-04-03 18:27:15,422 - INFO - Epoch 5, Batch 24: Training loss = 0.7700
2025-04-03 18:27:15,523 - INFO - Epoch 5, Batch 25: Training loss = 0.7600
2025-04-03 18:27:15,624 - INFO - Epoch 5, Batch 26: Training loss = 0.7500
2025-04-03 18:27:15,725 - INFO - Epoch 5, Batch 27: Training loss = 0.7400
2025-04-03 18:27:15,826 - INFO - Epoch 5, Batch 28: Training loss = 0.7300
2025-04-03 18:27:15,927 - INFO - Epoch 5, Batch 29: Training loss = 0.7200
2025-04-03 18:27:16,027 - INFO - Epoch 5, Batch 30: Training loss = 0.7100
2025-04-03 18:27:16,128 - INFO - Epoch 5, Batch 31: Training loss = 0.7000
2025-04-03 18:27:16,229 - INFO - Epoch 5, Batch 32: Training loss = 0.6900
2025-04-03 18:27:16,329 - INFO - Epoch 5, Batch 33: Training loss = 0.6800
2025-04-03 18:27:16,430 - INFO - Epoch 5, Batch 34: Training loss = 0.6700
2025-04-03 18:27:16,531 - INFO - Epoch 5, Batch 35: Training loss = 0.6600
2025-04-03 18:27:16,631 - INFO - Epoch 5, Batch 36: Training loss = 0.6500
2025-04-03 18:27:16,733 - INFO - Epoch 5, Batch 37: Training loss = 0.6400
2025-04-03 18:27:16,834 - INFO - Epoch 5, Batch 38: Training loss = 0.6300
2025-04-03 18:27:16,935 - INFO - Epoch 5, Batch 39: Training loss = 0.6200
2025-04-03 18:27:17,036 - INFO - Epoch 5, Batch 40: Training loss = 0.6100
2025-04-03 18:27:17,136 - INFO - Epoch 5, Batch 41: Training loss = 0.6000
2025-04-03 18:27:17,237 - INFO - Epoch 5, Batch 42: Training loss = 0.5900
2025-04-03 18:27:17,337 - INFO - Epoch 5, Batch 43: Training loss = 0.5800
2025-04-03 18:27:17,438 - INFO - Epoch 5, Batch 44: Training loss = 0.5700
2025-04-03 18:27:17,538 - INFO - Epoch 5, Batch 45: Training loss = 0.5600
2025-04-03 18:27:17,639 - INFO - Epoch 5, Batch 46: Training loss = 0.5500
2025-04-03 18:27:17,739 - INFO - Epoch 5, Batch 47: Training loss = 0.5400
2025-04-03 18:27:17,840 - INFO - Epoch 5, Batch 48: Training loss = 0.5300
2025-04-03 18:27:17,940 - INFO - Epoch 5, Batch 49: Training loss = 0.5200
2025-04-03 18:27:18,041 - INFO - Epoch 5, Batch 50: Training loss = 0.5100
2025-04-03 18:27:18,141 - INFO - Epoch 5, Batch 51: Training loss = 0.5000
2025-04-03 18:27:18,242 - INFO - Epoch 5, Batch 52: Training loss = 0.4900
2025-04-03 18:27:18,342 - INFO - Epoch 5, Batch 53: Training loss = 0.4800
2025-04-03 18:27:18,443 - INFO - Epoch 5, Batch 54: Training loss = 0.4700
2025-04-03 18:27:18,543 - INFO - Epoch 5, Batch 55: Training loss = 0.4600
2025-04-03 18:27:18,644 - INFO - Epoch 5, Batch 56: Training loss = 0.4500
2025-04-03 18:27:18,745 - INFO - Epoch 5, Batch 57: Training loss = 0.4400
2025-04-03 18:27:18,846 - INFO - Epoch 5, Batch 58: Training loss = 0.4300
2025-04-03 18:27:18,947 - INFO - Epoch 5, Batch 59: Training loss = 0.4200
2025-04-03 18:27:19,048 - INFO - Epoch 5, Batch 60: Training loss = 0.4100
2025-04-03 18:27:19,149 - INFO - Epoch 5, Batch 61: Training loss = 0.4000
2025-04-03 18:27:19,249 - INFO - Epoch 5, Batch 62: Training loss = 0.3900
2025-04-03 18:27:19,350 - INFO - Epoch 5, Batch 63: Training loss = 0.3800
2025-04-03 18:27:19,450 - INFO - Epoch 5, Batch 64: Training loss = 0.3700
2025-04-03 18:27:19,551 - INFO - Epoch 5, Batch 65: Training loss = 0.3600
2025-04-03 18:27:19,652 - INFO - Epoch 5, Batch 66: Training loss = 0.3500
2025-04-03 18:27:19,753 - INFO - Epoch 5, Batch 67: Training loss = 0.3400
2025-04-03 18:27:19,854 - INFO - Epoch 5, Batch 68: Training loss = 0.3300
2025-04-03 18:27:19,955 - INFO - Epoch 5, Batch 69: Training loss = 0.3200
2025-04-03 18:27:20,055 - INFO - Epoch 5, Batch 70: Training loss = 0.3100
2025-04-03 18:27:20,156 - INFO - Epoch 5, Batch 71: Training loss = 0.3000
2025-04-03 18:27:20,258 - INFO - Epoch 5, Batch 72: Training loss = 0.2900
2025-04-03 18:27:20,359 - INFO - Epoch 5, Batch 73: Training loss = 0.2800
2025-04-03 18:27:20,459 - INFO - Epoch 5, Batch 74: Training loss = 0.2700
2025-04-03 18:27:20,560 - INFO - Epoch 5, Batch 75: Training loss = 0.2600
2025-04-03 18:27:20,661 - INFO - Epoch 5, Batch 76: Training loss = 0.2500
2025-04-03 18:27:20,761 - INFO - Epoch 5, Batch 77: Training loss = 0.2400
2025-04-03 18:27:20,862 - INFO - Epoch 5, Batch 78: Training loss = 0.2300
2025-04-03 18:27:20,963 - INFO - Epoch 5, Batch 79: Training loss = 0.2200
2025-04-03 18:27:21,064 - INFO - Epoch 5, Batch 80: Training loss = 0.2100
2025-04-03 18:27:21,164 - INFO - Epoch 5, Batch 81: Training loss = 0.2000
2025-04-03 18:27:21,265 - INFO - Epoch 5, Batch 82: Training loss = 0.1900
2025-04-03 18:27:21,365 - INFO - Epoch 5, Batch 83: Training loss = 0.1800
2025-04-03 18:27:21,466 - INFO - Epoch 5, Batch 84: Training loss = 0.1700
2025-04-03 18:27:21,566 - INFO - Epoch 5, Batch 85: Training loss = 0.1600
2025-04-03 18:27:21,667 - INFO - Epoch 5, Batch 86: Training loss = 0.1500
2025-04-03 18:27:21,768 - INFO - Epoch 5, Batch 87: Training loss = 0.1400
2025-04-03 18:27:21,868 - INFO - Epoch 5, Batch 88: Training loss = 0.1300
2025-04-03 18:27:21,969 - INFO - Epoch 5, Batch 89: Training loss = 0.1200
2025-04-03 18:27:22,070 - INFO - Epoch 5, Batch 90: Training loss = 0.1100
2025-04-03 18:27:22,171 - INFO - Epoch 5, Batch 91: Training loss = 0.1000
2025-04-03 18:27:22,271 - INFO - Epoch 5, Batch 92: Training loss = 0.0900
2025-04-03 18:27:22,372 - INFO - Epoch 5, Batch 93: Training loss = 0.0800
2025-04-03 18:27:22,473 - INFO - Epoch 5, Batch 94: Training loss = 0.0700
2025-04-03 18:27:22,574 - INFO - Epoch 5, Batch 95: Training loss = 0.0600
2025-04-03 18:27:22,675 - INFO - Epoch 5, Batch 96: Training loss = 0.0500
2025-04-03 18:27:22,776 - INFO - Epoch 5, Batch 97: Training loss = 0.0400
2025-04-03 18:27:22,876 - INFO - Epoch 5, Batch 98: Training loss = 0.0300
2025-04-03 18:27:22,977 - INFO - Epoch 5, Batch 99: Training loss = 0.0200
2025-04-03 18:27:23,078 - INFO - Epoch 5, Batch 100: Training loss = 0.0100
2025-04-03 18:27:23,179 - INFO - Epoch 5: Validation loss = 0.0600, Validation accuracy = 0.8800
2025-04-03 18:27:23,179 - INFO - Epoch 5: MSE = 0.0600, RMSE = 0.2449, MAE = 0.0480, RÂ² = 0.9400
2025-04-03 18:27:23,180 - INFO - Epoch 6 started.
2025-04-03 18:27:23,180 - INFO - Epoch 6, Batch 1: Training loss = 1.0000
2025-04-03 18:27:23,280 - INFO - Epoch 6, Batch 2: Training loss = 0.9900
2025-04-03 18:27:23,381 - INFO - Epoch 6, Batch 3: Training loss = 0.9800
2025-04-03 18:27:23,482 - INFO - Epoch 6, Batch 4: Training loss = 0.9700
2025-04-03 18:27:23,583 - INFO - Epoch 6, Batch 5: Training loss = 0.9600
2025-04-03 18:27:23,684 - INFO - Epoch 6, Batch 6: Training loss = 0.9500
2025-04-03 18:27:23,785 - INFO - Epoch 6, Batch 7: Training loss = 0.9400
2025-04-03 18:27:23,886 - INFO - Epoch 6, Batch 8: Training loss = 0.9300
2025-04-03 18:27:23,987 - INFO - Epoch 6, Batch 9: Training loss = 0.9200
2025-04-03 18:27:24,088 - INFO - Epoch 6, Batch 10: Training loss = 0.9100
2025-04-03 18:27:24,189 - INFO - Epoch 6, Batch 11: Training loss = 0.9000
2025-04-03 18:27:24,290 - INFO - Epoch 6, Batch 12: Training loss = 0.8900
2025-04-03 18:27:24,390 - INFO - Epoch 6, Batch 13: Training loss = 0.8800
2025-04-03 18:27:24,491 - INFO - Epoch 6, Batch 14: Training loss = 0.8700
2025-04-03 18:27:24,592 - INFO - Epoch 6, Batch 15: Training loss = 0.8600
2025-04-03 18:27:24,693 - INFO - Epoch 6, Batch 16: Training loss = 0.8500
2025-04-03 18:27:24,794 - INFO - Epoch 6, Batch 17: Training loss = 0.8400
2025-04-03 18:27:24,894 - INFO - Epoch 6, Batch 18: Training loss = 0.8300
2025-04-03 18:27:24,995 - INFO - Epoch 6, Batch 19: Training loss = 0.8200
2025-04-03 18:27:25,095 - INFO - Epoch 6, Batch 20: Training loss = 0.8100
2025-04-03 18:27:25,196 - INFO - Epoch 6, Batch 21: Training loss = 0.8000
2025-04-03 18:27:25,297 - INFO - Epoch 6, Batch 22: Training loss = 0.7900
2025-04-03 18:27:25,398 - INFO - Epoch 6, Batch 23: Training loss = 0.7800
2025-04-03 18:27:25,499 - INFO - Epoch 6, Batch 24: Training loss = 0.7700
2025-04-03 18:27:25,600 - INFO - Epoch 6, Batch 25: Training loss = 0.7600
2025-04-03 18:27:25,701 - INFO - Epoch 6, Batch 26: Training loss = 0.7500
2025-04-03 18:27:25,802 - INFO - Epoch 6, Batch 27: Training loss = 0.7400
2025-04-03 18:27:25,903 - INFO - Epoch 6, Batch 28: Training loss = 0.7300
2025-04-03 18:27:26,004 - INFO - Epoch 6, Batch 29: Training loss = 0.7200
2025-04-03 18:27:26,104 - INFO - Epoch 6, Batch 30: Training loss = 0.7100
2025-04-03 18:27:26,205 - INFO - Epoch 6, Batch 31: Training loss = 0.7000
2025-04-03 18:27:26,305 - INFO - Epoch 6, Batch 32: Training loss = 0.6900
2025-04-03 18:27:26,406 - INFO - Epoch 6, Batch 33: Training loss = 0.6800
2025-04-03 18:27:26,507 - INFO - Epoch 6, Batch 34: Training loss = 0.6700
2025-04-03 18:27:26,608 - INFO - Epoch 6, Batch 35: Training loss = 0.6600
2025-04-03 18:27:26,709 - INFO - Epoch 6, Batch 36: Training loss = 0.6500
2025-04-03 18:27:26,809 - INFO - Epoch 6, Batch 37: Training loss = 0.6400
2025-04-03 18:27:26,910 - INFO - Epoch 6, Batch 38: Training loss = 0.6300
2025-04-03 18:27:27,011 - INFO - Epoch 6, Batch 39: Training loss = 0.6200
2025-04-03 18:27:27,112 - INFO - Epoch 6, Batch 40: Training loss = 0.6100
2025-04-03 18:27:27,213 - INFO - Epoch 6, Batch 41: Training loss = 0.6000
2025-04-03 18:27:27,313 - INFO - Epoch 6, Batch 42: Training loss = 0.5900
2025-04-03 18:27:27,414 - INFO - Epoch 6, Batch 43: Training loss = 0.5800
2025-04-03 18:27:27,515 - INFO - Epoch 6, Batch 44: Training loss = 0.5700
2025-04-03 18:27:27,616 - INFO - Epoch 6, Batch 45: Training loss = 0.5600
2025-04-03 18:27:27,717 - INFO - Epoch 6, Batch 46: Training loss = 0.5500
2025-04-03 18:27:27,818 - INFO - Epoch 6, Batch 47: Training loss = 0.5400
2025-04-03 18:27:27,918 - INFO - Epoch 6, Batch 48: Training loss = 0.5300
2025-04-03 18:27:28,019 - INFO - Epoch 6, Batch 49: Training loss = 0.5200
2025-04-03 18:27:28,120 - INFO - Epoch 6, Batch 50: Training loss = 0.5100
2025-04-03 18:27:28,220 - INFO - Epoch 6, Batch 51: Training loss = 0.5000
2025-04-03 18:27:28,321 - INFO - Epoch 6, Batch 52: Training loss = 0.4900
2025-04-03 18:27:28,422 - INFO - Epoch 6, Batch 53: Training loss = 0.4800
2025-04-03 18:27:28,523 - INFO - Epoch 6, Batch 54: Training loss = 0.4700
2025-04-03 18:27:28,624 - INFO - Epoch 6, Batch 55: Training loss = 0.4600
2025-04-03 18:27:28,725 - INFO - Epoch 6, Batch 56: Training loss = 0.4500
2025-04-03 18:27:28,826 - INFO - Epoch 6, Batch 57: Training loss = 0.4400
2025-04-03 18:27:28,926 - INFO - Epoch 6, Batch 58: Training loss = 0.4300
2025-04-03 18:27:29,027 - INFO - Epoch 6, Batch 59: Training loss = 0.4200
2025-04-03 18:27:29,128 - INFO - Epoch 6, Batch 60: Training loss = 0.4100
2025-04-03 18:27:29,229 - INFO - Epoch 6, Batch 61: Training loss = 0.4000
2025-04-03 18:27:29,329 - INFO - Epoch 6, Batch 62: Training loss = 0.3900
2025-04-03 18:27:29,429 - INFO - Epoch 6, Batch 63: Training loss = 0.3800
2025-04-03 18:27:29,530 - INFO - Epoch 6, Batch 64: Training loss = 0.3700
2025-04-03 18:27:29,631 - INFO - Epoch 6, Batch 65: Training loss = 0.3600
2025-04-03 18:27:29,732 - INFO - Epoch 6, Batch 66: Training loss = 0.3500
2025-04-03 18:27:29,832 - INFO - Epoch 6, Batch 67: Training loss = 0.3400
2025-04-03 18:27:29,933 - INFO - Epoch 6, Batch 68: Training loss = 0.3300
2025-04-03 18:27:30,033 - INFO - Epoch 6, Batch 69: Training loss = 0.3200
2025-04-03 18:27:30,134 - INFO - Epoch 6, Batch 70: Training loss = 0.3100
2025-04-03 18:27:30,235 - INFO - Epoch 6, Batch 71: Training loss = 0.3000
2025-04-03 18:27:30,336 - INFO - Epoch 6, Batch 72: Training loss = 0.2900
2025-04-03 18:27:30,436 - INFO - Epoch 6, Batch 73: Training loss = 0.2800
2025-04-03 18:27:30,537 - INFO - Epoch 6, Batch 74: Training loss = 0.2700
2025-04-03 18:27:30,638 - INFO - Epoch 6, Batch 75: Training loss = 0.2600
2025-04-03 18:27:30,739 - INFO - Epoch 6, Batch 76: Training loss = 0.2500
2025-04-03 18:27:30,839 - INFO - Epoch 6, Batch 77: Training loss = 0.2400
2025-04-03 18:27:30,940 - INFO - Epoch 6, Batch 78: Training loss = 0.2300
2025-04-03 18:27:31,040 - INFO - Epoch 6, Batch 79: Training loss = 0.2200
2025-04-03 18:27:31,142 - INFO - Epoch 6, Batch 80: Training loss = 0.2100
2025-04-03 18:27:31,243 - INFO - Epoch 6, Batch 81: Training loss = 0.2000
2025-04-03 18:27:31,344 - INFO - Epoch 6, Batch 82: Training loss = 0.1900
2025-04-03 18:27:31,445 - INFO - Epoch 6, Batch 83: Training loss = 0.1800
2025-04-03 18:27:31,545 - INFO - Epoch 6, Batch 84: Training loss = 0.1700
2025-04-03 18:27:31,646 - INFO - Epoch 6, Batch 85: Training loss = 0.1600
2025-04-03 18:27:31,747 - INFO - Epoch 6, Batch 86: Training loss = 0.1500
2025-04-03 18:27:31,847 - INFO - Epoch 6, Batch 87: Training loss = 0.1400
2025-04-03 18:27:31,948 - INFO - Epoch 6, Batch 88: Training loss = 0.1300
2025-04-03 18:27:32,049 - INFO - Epoch 6, Batch 89: Training loss = 0.1200
2025-04-03 18:27:32,149 - INFO - Epoch 6, Batch 90: Training loss = 0.1100
2025-04-03 18:27:32,250 - INFO - Epoch 6, Batch 91: Training loss = 0.1000
2025-04-03 18:27:32,351 - INFO - Epoch 6, Batch 92: Training loss = 0.0900
2025-04-03 18:27:32,451 - INFO - Epoch 6, Batch 93: Training loss = 0.0800
2025-04-03 18:27:32,552 - INFO - Epoch 6, Batch 94: Training loss = 0.0700
2025-04-03 18:27:32,652 - INFO - Epoch 6, Batch 95: Training loss = 0.0600
2025-04-03 18:27:32,753 - INFO - Epoch 6, Batch 96: Training loss = 0.0500
2025-04-03 18:27:32,853 - INFO - Epoch 6, Batch 97: Training loss = 0.0400
2025-04-03 18:27:32,954 - INFO - Epoch 6, Batch 98: Training loss = 0.0300
2025-04-03 18:27:33,054 - INFO - Epoch 6, Batch 99: Training loss = 0.0200
2025-04-03 18:27:33,155 - INFO - Epoch 6, Batch 100: Training loss = 0.0100
2025-04-03 18:27:33,255 - INFO - Epoch 6: Validation loss = 0.0500, Validation accuracy = 0.9000
2025-04-03 18:27:33,255 - INFO - Epoch 6: MSE = 0.0500, RMSE = 0.2236, MAE = 0.0400, RÂ² = 0.9500
2025-04-03 18:27:33,255 - INFO - Epoch 7 started.
2025-04-03 18:27:33,255 - INFO - Epoch 7, Batch 1: Training loss = 1.0000
2025-04-03 18:27:33,356 - INFO - Epoch 7, Batch 2: Training loss = 0.9900
2025-04-03 18:27:33,456 - INFO - Epoch 7, Batch 3: Training loss = 0.9800
2025-04-03 18:27:33,557 - INFO - Epoch 7, Batch 4: Training loss = 0.9700
2025-04-03 18:27:33,657 - INFO - Epoch 7, Batch 5: Training loss = 0.9600
2025-04-03 18:27:33,758 - INFO - Epoch 7, Batch 6: Training loss = 0.9500
2025-04-03 18:27:33,858 - INFO - Epoch 7, Batch 7: Training loss = 0.9400
2025-04-03 18:27:33,959 - INFO - Epoch 7, Batch 8: Training loss = 0.9300
2025-04-03 18:27:34,059 - INFO - Epoch 7, Batch 9: Training loss = 0.9200
2025-04-03 18:27:34,160 - INFO - Epoch 7, Batch 10: Training loss = 0.9100
2025-04-03 18:27:34,260 - INFO - Epoch 7, Batch 11: Training loss = 0.9000
2025-04-03 18:27:34,361 - INFO - Epoch 7, Batch 12: Training loss = 0.8900
2025-04-03 18:27:34,462 - INFO - Epoch 7, Batch 13: Training loss = 0.8800
2025-04-03 18:27:34,562 - INFO - Epoch 7, Batch 14: Training loss = 0.8700
2025-04-03 18:27:34,663 - INFO - Epoch 7, Batch 15: Training loss = 0.8600
2025-04-03 18:27:34,764 - INFO - Epoch 7, Batch 16: Training loss = 0.8500
2025-04-03 18:27:34,864 - INFO - Epoch 7, Batch 17: Training loss = 0.8400
2025-04-03 18:27:34,965 - INFO - Epoch 7, Batch 18: Training loss = 0.8300
2025-04-03 18:27:35,066 - INFO - Epoch 7, Batch 19: Training loss = 0.8200
2025-04-03 18:27:35,166 - INFO - Epoch 7, Batch 20: Training loss = 0.8100
2025-04-03 18:27:35,267 - INFO - Epoch 7, Batch 21: Training loss = 0.8000
2025-04-03 18:27:35,367 - INFO - Epoch 7, Batch 22: Training loss = 0.7900
2025-04-03 18:27:35,467 - INFO - Epoch 7, Batch 23: Training loss = 0.7800
2025-04-03 18:27:35,568 - INFO - Epoch 7, Batch 24: Training loss = 0.7700
2025-04-03 18:27:35,668 - INFO - Epoch 7, Batch 25: Training loss = 0.7600
2025-04-03 18:27:35,769 - INFO - Epoch 7, Batch 26: Training loss = 0.7500
2025-04-03 18:27:35,869 - INFO - Epoch 7, Batch 27: Training loss = 0.7400
2025-04-03 18:27:35,970 - INFO - Epoch 7, Batch 28: Training loss = 0.7300
2025-04-03 18:27:36,071 - INFO - Epoch 7, Batch 29: Training loss = 0.7200
2025-04-03 18:27:36,171 - INFO - Epoch 7, Batch 30: Training loss = 0.7100
2025-04-03 18:27:36,272 - INFO - Epoch 7, Batch 31: Training loss = 0.7000
2025-04-03 18:27:36,372 - INFO - Epoch 7, Batch 32: Training loss = 0.6900
2025-04-03 18:27:36,473 - INFO - Epoch 7, Batch 33: Training loss = 0.6800
2025-04-03 18:27:36,573 - INFO - Epoch 7, Batch 34: Training loss = 0.6700
2025-04-03 18:27:36,674 - INFO - Epoch 7, Batch 35: Training loss = 0.6600
2025-04-03 18:27:36,774 - INFO - Epoch 7, Batch 36: Training loss = 0.6500
2025-04-03 18:27:36,875 - INFO - Epoch 7, Batch 37: Training loss = 0.6400
2025-04-03 18:27:36,975 - INFO - Epoch 7, Batch 38: Training loss = 0.6300
2025-04-03 18:27:37,076 - INFO - Epoch 7, Batch 39: Training loss = 0.6200
2025-04-03 18:27:37,176 - INFO - Epoch 7, Batch 40: Training loss = 0.6100
2025-04-03 18:27:37,277 - INFO - Epoch 7, Batch 41: Training loss = 0.6000
2025-04-03 18:27:37,377 - INFO - Epoch 7, Batch 42: Training loss = 0.5900
2025-04-03 18:27:37,478 - INFO - Epoch 7, Batch 43: Training loss = 0.5800
2025-04-03 18:27:37,577 - INFO - Epoch 7, Batch 44: Training loss = 0.5700
2025-04-03 18:27:37,678 - INFO - Epoch 7, Batch 45: Training loss = 0.5600
2025-04-03 18:27:37,778 - INFO - Epoch 7, Batch 46: Training loss = 0.5500
2025-04-03 18:27:37,878 - INFO - Epoch 7, Batch 47: Training loss = 0.5400
2025-04-03 18:27:37,979 - INFO - Epoch 7, Batch 48: Training loss = 0.5300
2025-04-03 18:27:38,079 - INFO - Epoch 7, Batch 49: Training loss = 0.5200
2025-04-03 18:27:38,180 - INFO - Epoch 7, Batch 50: Training loss = 0.5100
2025-04-03 18:27:38,280 - INFO - Epoch 7, Batch 51: Training loss = 0.5000
2025-04-03 18:27:38,381 - INFO - Epoch 7, Batch 52: Training loss = 0.4900
2025-04-03 18:27:38,482 - INFO - Epoch 7, Batch 53: Training loss = 0.4800
2025-04-03 18:27:38,583 - INFO - Epoch 7, Batch 54: Training loss = 0.4700
2025-04-03 18:27:38,684 - INFO - Epoch 7, Batch 55: Training loss = 0.4600
2025-04-03 18:27:38,784 - INFO - Epoch 7, Batch 56: Training loss = 0.4500
2025-04-03 18:27:38,885 - INFO - Epoch 7, Batch 57: Training loss = 0.4400
2025-04-03 18:27:38,986 - INFO - Epoch 7, Batch 58: Training loss = 0.4300
2025-04-03 18:27:39,086 - INFO - Epoch 7, Batch 59: Training loss = 0.4200
2025-04-03 18:27:39,187 - INFO - Epoch 7, Batch 60: Training loss = 0.4100
2025-04-03 18:27:39,287 - INFO - Epoch 7, Batch 61: Training loss = 0.4000
2025-04-03 18:27:39,388 - INFO - Epoch 7, Batch 62: Training loss = 0.3900
2025-04-03 18:27:39,488 - INFO - Epoch 7, Batch 63: Training loss = 0.3800
2025-04-03 18:27:39,589 - INFO - Epoch 7, Batch 64: Training loss = 0.3700
2025-04-03 18:27:39,690 - INFO - Epoch 7, Batch 65: Training loss = 0.3600
2025-04-03 18:27:39,791 - INFO - Epoch 7, Batch 66: Training loss = 0.3500
2025-04-03 18:27:39,892 - INFO - Epoch 7, Batch 67: Training loss = 0.3400
2025-04-03 18:27:39,992 - INFO - Epoch 7, Batch 68: Training loss = 0.3300
2025-04-03 18:27:40,093 - INFO - Epoch 7, Batch 69: Training loss = 0.3200
2025-04-03 18:27:40,193 - INFO - Epoch 7, Batch 70: Training loss = 0.3100
2025-04-03 18:27:40,294 - INFO - Epoch 7, Batch 71: Training loss = 0.3000
2025-04-03 18:27:40,395 - INFO - Epoch 7, Batch 72: Training loss = 0.2900
2025-04-03 18:27:40,495 - INFO - Epoch 7, Batch 73: Training loss = 0.2800
2025-04-03 18:27:40,596 - INFO - Epoch 7, Batch 74: Training loss = 0.2700
2025-04-03 18:27:40,696 - INFO - Epoch 7, Batch 75: Training loss = 0.2600
2025-04-03 18:27:40,797 - INFO - Epoch 7, Batch 76: Training loss = 0.2500
2025-04-03 18:27:40,897 - INFO - Epoch 7, Batch 77: Training loss = 0.2400
2025-04-03 18:27:40,997 - INFO - Epoch 7, Batch 78: Training loss = 0.2300
2025-04-03 18:27:41,098 - INFO - Epoch 7, Batch 79: Training loss = 0.2200
2025-04-03 18:27:41,199 - INFO - Epoch 7, Batch 80: Training loss = 0.2100
2025-04-03 18:27:41,300 - INFO - Epoch 7, Batch 81: Training loss = 0.2000
2025-04-03 18:27:41,400 - INFO - Epoch 7, Batch 82: Training loss = 0.1900
2025-04-03 18:27:41,501 - INFO - Epoch 7, Batch 83: Training loss = 0.1800
2025-04-03 18:27:41,601 - INFO - Epoch 7, Batch 84: Training loss = 0.1700
2025-04-03 18:27:41,702 - INFO - Epoch 7, Batch 85: Training loss = 0.1600
2025-04-03 18:27:41,803 - INFO - Epoch 7, Batch 86: Training loss = 0.1500
2025-04-03 18:27:41,904 - INFO - Epoch 7, Batch 87: Training loss = 0.1400
2025-04-03 18:27:42,004 - INFO - Epoch 7, Batch 88: Training loss = 0.1300
2025-04-03 18:27:42,105 - INFO - Epoch 7, Batch 89: Training loss = 0.1200
2025-04-03 18:27:42,206 - INFO - Epoch 7, Batch 90: Training loss = 0.1100
2025-04-03 18:27:42,306 - INFO - Epoch 7, Batch 91: Training loss = 0.1000
2025-04-03 18:27:42,407 - INFO - Epoch 7, Batch 92: Training loss = 0.0900
2025-04-03 18:27:42,508 - INFO - Epoch 7, Batch 93: Training loss = 0.0800
2025-04-03 18:27:42,609 - INFO - Epoch 7, Batch 94: Training loss = 0.0700
2025-04-03 18:27:42,709 - INFO - Epoch 7, Batch 95: Training loss = 0.0600
2025-04-03 18:27:42,811 - INFO - Epoch 7, Batch 96: Training loss = 0.0500
2025-04-03 18:27:42,912 - INFO - Epoch 7, Batch 97: Training loss = 0.0400
2025-04-03 18:27:43,012 - INFO - Epoch 7, Batch 98: Training loss = 0.0300
2025-04-03 18:27:43,113 - INFO - Epoch 7, Batch 99: Training loss = 0.0200
2025-04-03 18:27:43,214 - INFO - Epoch 7, Batch 100: Training loss = 0.0100
2025-04-03 18:27:43,315 - INFO - Epoch 7: Validation loss = 0.0400, Validation accuracy = 0.9200
2025-04-03 18:27:43,315 - INFO - Epoch 7: MSE = 0.0400, RMSE = 0.2000, MAE = 0.0320, RÂ² = 0.9600
2025-04-03 18:27:43,315 - INFO - Epoch 8 started.
2025-04-03 18:27:43,315 - INFO - Epoch 8, Batch 1: Training loss = 1.0000
2025-04-03 18:27:43,416 - INFO - Epoch 8, Batch 2: Training loss = 0.9900
2025-04-03 18:27:43,516 - INFO - Epoch 8, Batch 3: Training loss = 0.9800
2025-04-03 18:27:43,617 - INFO - Epoch 8, Batch 4: Training loss = 0.9700
2025-04-03 18:27:43,718 - INFO - Epoch 8, Batch 5: Training loss = 0.9600
2025-04-03 18:27:43,819 - INFO - Epoch 8, Batch 6: Training loss = 0.9500
2025-04-03 18:27:43,919 - INFO - Epoch 8, Batch 7: Training loss = 0.9400
2025-04-03 18:27:44,020 - INFO - Epoch 8, Batch 8: Training loss = 0.9300
2025-04-03 18:27:44,120 - INFO - Epoch 8, Batch 9: Training loss = 0.9200
2025-04-03 18:27:44,221 - INFO - Epoch 8, Batch 10: Training loss = 0.9100
2025-04-03 18:27:44,321 - INFO - Epoch 8, Batch 11: Training loss = 0.9000
2025-04-03 18:27:44,422 - INFO - Epoch 8, Batch 12: Training loss = 0.8900
2025-04-03 18:27:44,523 - INFO - Epoch 8, Batch 13: Training loss = 0.8800
2025-04-03 18:27:44,623 - INFO - Epoch 8, Batch 14: Training loss = 0.8700
2025-04-03 18:27:44,724 - INFO - Epoch 8, Batch 15: Training loss = 0.8600
2025-04-03 18:27:44,824 - INFO - Epoch 8, Batch 16: Training loss = 0.8500
2025-04-03 18:27:44,924 - INFO - Epoch 8, Batch 17: Training loss = 0.8400
2025-04-03 18:27:45,025 - INFO - Epoch 8, Batch 18: Training loss = 0.8300
2025-04-03 18:27:45,125 - INFO - Epoch 8, Batch 19: Training loss = 0.8200
2025-04-03 18:27:45,226 - INFO - Epoch 8, Batch 20: Training loss = 0.8100
2025-04-03 18:27:45,326 - INFO - Epoch 8, Batch 21: Training loss = 0.8000
2025-04-03 18:27:45,428 - INFO - Epoch 8, Batch 22: Training loss = 0.7900
2025-04-03 18:27:45,529 - INFO - Epoch 8, Batch 23: Training loss = 0.7800
2025-04-03 18:27:45,629 - INFO - Epoch 8, Batch 24: Training loss = 0.7700
2025-04-03 18:27:45,730 - INFO - Epoch 8, Batch 25: Training loss = 0.7600
2025-04-03 18:27:45,831 - INFO - Epoch 8, Batch 26: Training loss = 0.7500
2025-04-03 18:27:45,932 - INFO - Epoch 8, Batch 27: Training loss = 0.7400
2025-04-03 18:27:46,032 - INFO - Epoch 8, Batch 28: Training loss = 0.7300
2025-04-03 18:27:46,133 - INFO - Epoch 8, Batch 29: Training loss = 0.7200
2025-04-03 18:27:46,234 - INFO - Epoch 8, Batch 30: Training loss = 0.7100
2025-04-03 18:27:46,335 - INFO - Epoch 8, Batch 31: Training loss = 0.7000
2025-04-03 18:27:46,435 - INFO - Epoch 8, Batch 32: Training loss = 0.6900
2025-04-03 18:27:46,536 - INFO - Epoch 8, Batch 33: Training loss = 0.6800
2025-04-03 18:27:46,637 - INFO - Epoch 8, Batch 34: Training loss = 0.6700
2025-04-03 18:27:46,737 - INFO - Epoch 8, Batch 35: Training loss = 0.6600
2025-04-03 18:27:46,838 - INFO - Epoch 8, Batch 36: Training loss = 0.6500
2025-04-03 18:27:46,939 - INFO - Epoch 8, Batch 37: Training loss = 0.6400
2025-04-03 18:27:47,039 - INFO - Epoch 8, Batch 38: Training loss = 0.6300
2025-04-03 18:27:47,140 - INFO - Epoch 8, Batch 39: Training loss = 0.6200
2025-04-03 18:27:47,241 - INFO - Epoch 8, Batch 40: Training loss = 0.6100
2025-04-03 18:27:47,342 - INFO - Epoch 8, Batch 41: Training loss = 0.6000
2025-04-03 18:27:47,442 - INFO - Epoch 8, Batch 42: Training loss = 0.5900
2025-04-03 18:27:47,543 - INFO - Epoch 8, Batch 43: Training loss = 0.5800
2025-04-03 18:27:47,644 - INFO - Epoch 8, Batch 44: Training loss = 0.5700
2025-04-03 18:27:47,745 - INFO - Epoch 8, Batch 45: Training loss = 0.5600
2025-04-03 18:27:47,845 - INFO - Epoch 8, Batch 46: Training loss = 0.5500
2025-04-03 18:27:47,945 - INFO - Epoch 8, Batch 47: Training loss = 0.5400
2025-04-03 18:27:48,046 - INFO - Epoch 8, Batch 48: Training loss = 0.5300
2025-04-03 18:27:48,146 - INFO - Epoch 8, Batch 49: Training loss = 0.5200
2025-04-03 18:27:48,247 - INFO - Epoch 8, Batch 50: Training loss = 0.5100
2025-04-03 18:27:48,347 - INFO - Epoch 8, Batch 51: Training loss = 0.5000
2025-04-03 18:27:48,448 - INFO - Epoch 8, Batch 52: Training loss = 0.4900
2025-04-03 18:27:48,548 - INFO - Epoch 8, Batch 53: Training loss = 0.4800
2025-04-03 18:27:48,649 - INFO - Epoch 8, Batch 54: Training loss = 0.4700
2025-04-03 18:27:48,749 - INFO - Epoch 8, Batch 55: Training loss = 0.4600
2025-04-03 18:27:48,850 - INFO - Epoch 8, Batch 56: Training loss = 0.4500
2025-04-03 18:27:48,950 - INFO - Epoch 8, Batch 57: Training loss = 0.4400
2025-04-03 18:27:49,051 - INFO - Epoch 8, Batch 58: Training loss = 0.4300
2025-04-03 18:27:49,151 - INFO - Epoch 8, Batch 59: Training loss = 0.4200
2025-04-03 18:27:49,252 - INFO - Epoch 8, Batch 60: Training loss = 0.4100
2025-04-03 18:27:49,352 - INFO - Epoch 8, Batch 61: Training loss = 0.4000
2025-04-03 18:27:49,453 - INFO - Epoch 8, Batch 62: Training loss = 0.3900
2025-04-03 18:27:49,554 - INFO - Epoch 8, Batch 63: Training loss = 0.3800
2025-04-03 18:27:49,654 - INFO - Epoch 8, Batch 64: Training loss = 0.3700
2025-04-03 18:27:49,755 - INFO - Epoch 8, Batch 65: Training loss = 0.3600
2025-04-03 18:27:49,856 - INFO - Epoch 8, Batch 66: Training loss = 0.3500
2025-04-03 18:27:49,957 - INFO - Epoch 8, Batch 67: Training loss = 0.3400
2025-04-03 18:27:50,058 - INFO - Epoch 8, Batch 68: Training loss = 0.3300
2025-04-03 18:27:50,159 - INFO - Epoch 8, Batch 69: Training loss = 0.3200
2025-04-03 18:27:50,260 - INFO - Epoch 8, Batch 70: Training loss = 0.3100
2025-04-03 18:27:50,360 - INFO - Epoch 8, Batch 71: Training loss = 0.3000
2025-04-03 18:27:50,461 - INFO - Epoch 8, Batch 72: Training loss = 0.2900
2025-04-03 18:27:50,562 - INFO - Epoch 8, Batch 73: Training loss = 0.2800
2025-04-03 18:27:50,663 - INFO - Epoch 8, Batch 74: Training loss = 0.2700
2025-04-03 18:27:50,763 - INFO - Epoch 8, Batch 75: Training loss = 0.2600
2025-04-03 18:27:50,864 - INFO - Epoch 8, Batch 76: Training loss = 0.2500
2025-04-03 18:27:50,965 - INFO - Epoch 8, Batch 77: Training loss = 0.2400
2025-04-03 18:27:51,065 - INFO - Epoch 8, Batch 78: Training loss = 0.2300
2025-04-03 18:27:51,166 - INFO - Epoch 8, Batch 79: Training loss = 0.2200
2025-04-03 18:27:51,266 - INFO - Epoch 8, Batch 80: Training loss = 0.2100
2025-04-03 18:27:51,367 - INFO - Epoch 8, Batch 81: Training loss = 0.2000
2025-04-03 18:27:51,467 - INFO - Epoch 8, Batch 82: Training loss = 0.1900
2025-04-03 18:27:51,568 - INFO - Epoch 8, Batch 83: Training loss = 0.1800
2025-04-03 18:27:51,668 - INFO - Epoch 8, Batch 84: Training loss = 0.1700
2025-04-03 18:27:51,769 - INFO - Epoch 8, Batch 85: Training loss = 0.1600
2025-04-03 18:27:51,869 - INFO - Epoch 8, Batch 86: Training loss = 0.1500
2025-04-03 18:27:51,969 - INFO - Epoch 8, Batch 87: Training loss = 0.1400
2025-04-03 18:27:52,070 - INFO - Epoch 8, Batch 88: Training loss = 0.1300
2025-04-03 18:27:52,170 - INFO - Epoch 8, Batch 89: Training loss = 0.1200
2025-04-03 18:27:52,271 - INFO - Epoch 8, Batch 90: Training loss = 0.1100
2025-04-03 18:27:52,371 - INFO - Epoch 8, Batch 91: Training loss = 0.1000
2025-04-03 18:27:52,471 - INFO - Epoch 8, Batch 92: Training loss = 0.0900
2025-04-03 18:27:52,572 - INFO - Epoch 8, Batch 93: Training loss = 0.0800
2025-04-03 18:27:52,672 - INFO - Epoch 8, Batch 94: Training loss = 0.0700
2025-04-03 18:27:52,773 - INFO - Epoch 8, Batch 95: Training loss = 0.0600
2025-04-03 18:27:52,873 - INFO - Epoch 8, Batch 96: Training loss = 0.0500
2025-04-03 18:27:52,973 - INFO - Epoch 8, Batch 97: Training loss = 0.0400
2025-04-03 18:27:53,074 - INFO - Epoch 8, Batch 98: Training loss = 0.0300
2025-04-03 18:27:53,174 - INFO - Epoch 8, Batch 99: Training loss = 0.0200
2025-04-03 18:27:53,275 - INFO - Epoch 8, Batch 100: Training loss = 0.0100
2025-04-03 18:27:53,375 - INFO - Epoch 8: Validation loss = 0.0300, Validation accuracy = 0.9400
2025-04-03 18:27:53,375 - INFO - Epoch 8: MSE = 0.0300, RMSE = 0.1732, MAE = 0.0240, RÂ² = 0.9700
2025-04-03 18:27:53,376 - INFO - Epoch 9 started.
2025-04-03 18:27:53,376 - INFO - Epoch 9, Batch 1: Training loss = 1.0000
2025-04-03 18:27:53,476 - INFO - Epoch 9, Batch 2: Training loss = 0.9900
2025-04-03 18:27:53,576 - INFO - Epoch 9, Batch 3: Training loss = 0.9800
2025-04-03 18:27:53,677 - INFO - Epoch 9, Batch 4: Training loss = 0.9700
2025-04-03 18:27:53,777 - INFO - Epoch 9, Batch 5: Training loss = 0.9600
2025-04-03 18:27:53,878 - INFO - Epoch 9, Batch 6: Training loss = 0.9500
2025-04-03 18:27:53,978 - INFO - Epoch 9, Batch 7: Training loss = 0.9400
2025-04-03 18:27:54,078 - INFO - Epoch 9, Batch 8: Training loss = 0.9300
2025-04-03 18:27:54,179 - INFO - Epoch 9, Batch 9: Training loss = 0.9200
2025-04-03 18:27:54,279 - INFO - Epoch 9, Batch 10: Training loss = 0.9100
2025-04-03 18:27:54,380 - INFO - Epoch 9, Batch 11: Training loss = 0.9000
2025-04-03 18:27:54,480 - INFO - Epoch 9, Batch 12: Training loss = 0.8900
2025-04-03 18:27:54,580 - INFO - Epoch 9, Batch 13: Training loss = 0.8800
2025-04-03 18:27:54,681 - INFO - Epoch 9, Batch 14: Training loss = 0.8700
2025-04-03 18:27:54,781 - INFO - Epoch 9, Batch 15: Training loss = 0.8600
2025-04-03 18:27:54,882 - INFO - Epoch 9, Batch 16: Training loss = 0.8500
2025-04-03 18:27:54,982 - INFO - Epoch 9, Batch 17: Training loss = 0.8400
2025-04-03 18:27:55,082 - INFO - Epoch 9, Batch 18: Training loss = 0.8300
2025-04-03 18:27:55,183 - INFO - Epoch 9, Batch 19: Training loss = 0.8200
2025-04-03 18:27:55,283 - INFO - Epoch 9, Batch 20: Training loss = 0.8100
2025-04-03 18:27:55,384 - INFO - Epoch 9, Batch 21: Training loss = 0.8000
2025-04-03 18:27:55,484 - INFO - Epoch 9, Batch 22: Training loss = 0.7900
2025-04-03 18:27:55,585 - INFO - Epoch 9, Batch 23: Training loss = 0.7800
2025-04-03 18:27:55,685 - INFO - Epoch 9, Batch 24: Training loss = 0.7700
2025-04-03 18:27:55,785 - INFO - Epoch 9, Batch 25: Training loss = 0.7600
2025-04-03 18:27:55,886 - INFO - Epoch 9, Batch 26: Training loss = 0.7500
2025-04-03 18:27:55,986 - INFO - Epoch 9, Batch 27: Training loss = 0.7400
2025-04-03 18:27:56,087 - INFO - Epoch 9, Batch 28: Training loss = 0.7300
2025-04-03 18:27:56,187 - INFO - Epoch 9, Batch 29: Training loss = 0.7200
2025-04-03 18:27:56,288 - INFO - Epoch 9, Batch 30: Training loss = 0.7100
2025-04-03 18:27:56,388 - INFO - Epoch 9, Batch 31: Training loss = 0.7000
2025-04-03 18:27:56,488 - INFO - Epoch 9, Batch 32: Training loss = 0.6900
2025-04-03 18:27:56,589 - INFO - Epoch 9, Batch 33: Training loss = 0.6800
2025-04-03 18:27:56,689 - INFO - Epoch 9, Batch 34: Training loss = 0.6700
2025-04-03 18:27:56,790 - INFO - Epoch 9, Batch 35: Training loss = 0.6600
2025-04-03 18:27:56,890 - INFO - Epoch 9, Batch 36: Training loss = 0.6500
2025-04-03 18:27:56,991 - INFO - Epoch 9, Batch 37: Training loss = 0.6400
2025-04-03 18:27:57,091 - INFO - Epoch 9, Batch 38: Training loss = 0.6300
2025-04-03 18:27:57,191 - INFO - Epoch 9, Batch 39: Training loss = 0.6200
2025-04-03 18:27:57,292 - INFO - Epoch 9, Batch 40: Training loss = 0.6100
2025-04-03 18:27:57,392 - INFO - Epoch 9, Batch 41: Training loss = 0.6000
2025-04-03 18:27:57,493 - INFO - Epoch 9, Batch 42: Training loss = 0.5900
2025-04-03 18:27:57,593 - INFO - Epoch 9, Batch 43: Training loss = 0.5800
2025-04-03 18:27:57,694 - INFO - Epoch 9, Batch 44: Training loss = 0.5700
2025-04-03 18:27:57,794 - INFO - Epoch 9, Batch 45: Training loss = 0.5600
2025-04-03 18:27:57,895 - INFO - Epoch 9, Batch 46: Training loss = 0.5500
2025-04-03 18:27:57,995 - INFO - Epoch 9, Batch 47: Training loss = 0.5400
2025-04-03 18:27:58,095 - INFO - Epoch 9, Batch 48: Training loss = 0.5300
2025-04-03 18:27:58,196 - INFO - Epoch 9, Batch 49: Training loss = 0.5200
2025-04-03 18:27:58,296 - INFO - Epoch 9, Batch 50: Training loss = 0.5100
2025-04-03 18:27:58,397 - INFO - Epoch 9, Batch 51: Training loss = 0.5000
2025-04-03 18:27:58,497 - INFO - Epoch 9, Batch 52: Training loss = 0.4900
2025-04-03 18:27:58,598 - INFO - Epoch 9, Batch 53: Training loss = 0.4800
2025-04-03 18:27:58,698 - INFO - Epoch 9, Batch 54: Training loss = 0.4700
2025-04-03 18:27:58,799 - INFO - Epoch 9, Batch 55: Training loss = 0.4600
2025-04-03 18:27:58,899 - INFO - Epoch 9, Batch 56: Training loss = 0.4500
2025-04-03 18:27:59,000 - INFO - Epoch 9, Batch 57: Training loss = 0.4400
2025-04-03 18:27:59,100 - INFO - Epoch 9, Batch 58: Training loss = 0.4300
2025-04-03 18:27:59,200 - INFO - Epoch 9, Batch 59: Training loss = 0.4200
2025-04-03 18:27:59,301 - INFO - Epoch 9, Batch 60: Training loss = 0.4100
2025-04-03 18:27:59,401 - INFO - Epoch 9, Batch 61: Training loss = 0.4000
2025-04-03 18:27:59,502 - INFO - Epoch 9, Batch 62: Training loss = 0.3900
2025-04-03 18:27:59,602 - INFO - Epoch 9, Batch 63: Training loss = 0.3800
2025-04-03 18:27:59,703 - INFO - Epoch 9, Batch 64: Training loss = 0.3700
2025-04-03 18:27:59,803 - INFO - Epoch 9, Batch 65: Training loss = 0.3600
2025-04-03 18:27:59,903 - INFO - Epoch 9, Batch 66: Training loss = 0.3500
2025-04-03 18:28:00,004 - INFO - Epoch 9, Batch 67: Training loss = 0.3400
2025-04-03 18:28:00,104 - INFO - Epoch 9, Batch 68: Training loss = 0.3300
2025-04-03 18:28:00,205 - INFO - Epoch 9, Batch 69: Training loss = 0.3200
2025-04-03 18:28:00,305 - INFO - Epoch 9, Batch 70: Training loss = 0.3100
2025-04-03 18:28:00,406 - INFO - Epoch 9, Batch 71: Training loss = 0.3000
2025-04-03 18:28:00,506 - INFO - Epoch 9, Batch 72: Training loss = 0.2900
2025-04-03 18:28:00,606 - INFO - Epoch 9, Batch 73: Training loss = 0.2800
2025-04-03 18:28:00,707 - INFO - Epoch 9, Batch 74: Training loss = 0.2700
2025-04-03 18:28:00,807 - INFO - Epoch 9, Batch 75: Training loss = 0.2600
2025-04-03 18:28:00,907 - INFO - Epoch 9, Batch 76: Training loss = 0.2500
2025-04-03 18:28:01,008 - INFO - Epoch 9, Batch 77: Training loss = 0.2400
2025-04-03 18:28:01,108 - INFO - Epoch 9, Batch 78: Training loss = 0.2300
2025-04-03 18:28:01,209 - INFO - Epoch 9, Batch 79: Training loss = 0.2200
2025-04-03 18:28:01,309 - INFO - Epoch 9, Batch 80: Training loss = 0.2100
2025-04-03 18:28:01,410 - INFO - Epoch 9, Batch 81: Training loss = 0.2000
2025-04-03 18:28:01,510 - INFO - Epoch 9, Batch 82: Training loss = 0.1900
2025-04-03 18:28:01,610 - INFO - Epoch 9, Batch 83: Training loss = 0.1800
2025-04-03 18:28:01,711 - INFO - Epoch 9, Batch 84: Training loss = 0.1700
2025-04-03 18:28:01,811 - INFO - Epoch 9, Batch 85: Training loss = 0.1600
2025-04-03 18:28:01,912 - INFO - Epoch 9, Batch 86: Training loss = 0.1500
2025-04-03 18:28:02,012 - INFO - Epoch 9, Batch 87: Training loss = 0.1400
2025-04-03 18:28:02,112 - INFO - Epoch 9, Batch 88: Training loss = 0.1300
2025-04-03 18:28:02,213 - INFO - Epoch 9, Batch 89: Training loss = 0.1200
2025-04-03 18:28:02,313 - INFO - Epoch 9, Batch 90: Training loss = 0.1100
2025-04-03 18:28:02,414 - INFO - Epoch 9, Batch 91: Training loss = 0.1000
2025-04-03 18:28:02,514 - INFO - Epoch 9, Batch 92: Training loss = 0.0900
2025-04-03 18:28:02,615 - INFO - Epoch 9, Batch 93: Training loss = 0.0800
2025-04-03 18:28:02,715 - INFO - Epoch 9, Batch 94: Training loss = 0.0700
2025-04-03 18:28:02,816 - INFO - Epoch 9, Batch 95: Training loss = 0.0600
2025-04-03 18:28:02,916 - INFO - Epoch 9, Batch 96: Training loss = 0.0500
2025-04-03 18:28:03,017 - INFO - Epoch 9, Batch 97: Training loss = 0.0400
2025-04-03 18:28:03,117 - INFO - Epoch 9, Batch 98: Training loss = 0.0300
2025-04-03 18:28:03,217 - INFO - Epoch 9, Batch 99: Training loss = 0.0200
2025-04-03 18:28:03,318 - INFO - Epoch 9, Batch 100: Training loss = 0.0100
2025-04-03 18:28:03,418 - INFO - Epoch 9: Validation loss = 0.0200, Validation accuracy = 0.9600
2025-04-03 18:28:03,418 - INFO - Epoch 9: MSE = 0.0200, RMSE = 0.1414, MAE = 0.0160, RÂ² = 0.9800
2025-04-03 18:28:03,419 - INFO - Epoch 10 started.
2025-04-03 18:28:03,419 - INFO - Epoch 10, Batch 1: Training loss = 1.0000
2025-04-03 18:28:03,519 - INFO - Epoch 10, Batch 2: Training loss = 0.9900
2025-04-03 18:28:03,619 - INFO - Epoch 10, Batch 3: Training loss = 0.9800
2025-04-03 18:28:03,720 - INFO - Epoch 10, Batch 4: Training loss = 0.9700
2025-04-03 18:28:03,821 - INFO - Epoch 10, Batch 5: Training loss = 0.9600
2025-04-03 18:28:03,921 - INFO - Epoch 10, Batch 6: Training loss = 0.9500
2025-04-03 18:28:04,022 - INFO - Epoch 10, Batch 7: Training loss = 0.9400
2025-04-03 18:28:04,122 - INFO - Epoch 10, Batch 8: Training loss = 0.9300
2025-04-03 18:28:04,223 - INFO - Epoch 10, Batch 9: Training loss = 0.9200
2025-04-03 18:28:04,323 - INFO - Epoch 10, Batch 10: Training loss = 0.9100
2025-04-03 18:28:04,424 - INFO - Epoch 10, Batch 11: Training loss = 0.9000
2025-04-03 18:28:04,525 - INFO - Epoch 10, Batch 12: Training loss = 0.8900
2025-04-03 18:28:04,625 - INFO - Epoch 10, Batch 13: Training loss = 0.8800
2025-04-03 18:28:04,726 - INFO - Epoch 10, Batch 14: Training loss = 0.8700
2025-04-03 18:28:04,827 - INFO - Epoch 10, Batch 15: Training loss = 0.8600
2025-04-03 18:28:04,927 - INFO - Epoch 10, Batch 16: Training loss = 0.8500
2025-04-03 18:28:05,028 - INFO - Epoch 10, Batch 17: Training loss = 0.8400
2025-04-03 18:28:05,128 - INFO - Epoch 10, Batch 18: Training loss = 0.8300
2025-04-03 18:28:05,229 - INFO - Epoch 10, Batch 19: Training loss = 0.8200
2025-04-03 18:28:05,330 - INFO - Epoch 10, Batch 20: Training loss = 0.8100
2025-04-03 18:28:05,430 - INFO - Epoch 10, Batch 21: Training loss = 0.8000
2025-04-03 18:28:05,531 - INFO - Epoch 10, Batch 22: Training loss = 0.7900
2025-04-03 18:28:05,631 - INFO - Epoch 10, Batch 23: Training loss = 0.7800
2025-04-03 18:28:05,731 - INFO - Epoch 10, Batch 24: Training loss = 0.7700
2025-04-03 18:28:05,832 - INFO - Epoch 10, Batch 25: Training loss = 0.7600
2025-04-03 18:28:05,933 - INFO - Epoch 10, Batch 26: Training loss = 0.7500
2025-04-03 18:28:06,033 - INFO - Epoch 10, Batch 27: Training loss = 0.7400
2025-04-03 18:28:06,134 - INFO - Epoch 10, Batch 28: Training loss = 0.7300
2025-04-03 18:28:06,234 - INFO - Epoch 10, Batch 29: Training loss = 0.7200
2025-04-03 18:28:06,335 - INFO - Epoch 10, Batch 30: Training loss = 0.7100
2025-04-03 18:28:06,435 - INFO - Epoch 10, Batch 31: Training loss = 0.7000
2025-04-03 18:28:06,536 - INFO - Epoch 10, Batch 32: Training loss = 0.6900
2025-04-03 18:28:06,636 - INFO - Epoch 10, Batch 33: Training loss = 0.6800
2025-04-03 18:28:06,737 - INFO - Epoch 10, Batch 34: Training loss = 0.6700
2025-04-03 18:28:06,837 - INFO - Epoch 10, Batch 35: Training loss = 0.6600
2025-04-03 18:28:06,938 - INFO - Epoch 10, Batch 36: Training loss = 0.6500
2025-04-03 18:28:07,038 - INFO - Epoch 10, Batch 37: Training loss = 0.6400
2025-04-03 18:28:07,139 - INFO - Epoch 10, Batch 38: Training loss = 0.6300
2025-04-03 18:28:07,239 - INFO - Epoch 10, Batch 39: Training loss = 0.6200
2025-04-03 18:28:07,339 - INFO - Epoch 10, Batch 40: Training loss = 0.6100
2025-04-03 18:28:07,440 - INFO - Epoch 10, Batch 41: Training loss = 0.6000
2025-04-03 18:28:07,540 - INFO - Epoch 10, Batch 42: Training loss = 0.5900
2025-04-03 18:28:07,639 - INFO - Epoch 10, Batch 43: Training loss = 0.5800
2025-04-03 18:28:07,739 - INFO - Epoch 10, Batch 44: Training loss = 0.5700
2025-04-03 18:28:07,840 - INFO - Epoch 10, Batch 45: Training loss = 0.5600
2025-04-03 18:28:07,940 - INFO - Epoch 10, Batch 46: Training loss = 0.5500
2025-04-03 18:28:08,041 - INFO - Epoch 10, Batch 47: Training loss = 0.5400
2025-04-03 18:28:08,141 - INFO - Epoch 10, Batch 48: Training loss = 0.5300
2025-04-03 18:28:08,241 - INFO - Epoch 10, Batch 49: Training loss = 0.5200
2025-04-03 18:28:08,345 - INFO - Epoch 10, Batch 50: Training loss = 0.5100
2025-04-03 18:28:08,445 - INFO - Epoch 10, Batch 51: Training loss = 0.5000
2025-04-03 18:28:08,545 - INFO - Epoch 10, Batch 52: Training loss = 0.4900
2025-04-03 18:28:08,646 - INFO - Epoch 10, Batch 53: Training loss = 0.4800
2025-04-03 18:28:08,747 - INFO - Epoch 10, Batch 54: Training loss = 0.4700
2025-04-03 18:28:08,847 - INFO - Epoch 10, Batch 55: Training loss = 0.4600
2025-04-03 18:28:08,948 - INFO - Epoch 10, Batch 56: Training loss = 0.4500
2025-04-03 18:28:09,049 - INFO - Epoch 10, Batch 57: Training loss = 0.4400
2025-04-03 18:28:09,150 - INFO - Epoch 10, Batch 58: Training loss = 0.4300
2025-04-03 18:28:09,250 - INFO - Epoch 10, Batch 59: Training loss = 0.4200
2025-04-03 18:28:09,350 - INFO - Epoch 10, Batch 60: Training loss = 0.4100
2025-04-03 18:28:09,451 - INFO - Epoch 10, Batch 61: Training loss = 0.4000
2025-04-03 18:28:09,551 - INFO - Epoch 10, Batch 62: Training loss = 0.3900
2025-04-03 18:28:09,652 - INFO - Epoch 10, Batch 63: Training loss = 0.3800
2025-04-03 18:28:09,752 - INFO - Epoch 10, Batch 64: Training loss = 0.3700
2025-04-03 18:28:09,853 - INFO - Epoch 10, Batch 65: Training loss = 0.3600
2025-04-03 18:28:09,953 - INFO - Epoch 10, Batch 66: Training loss = 0.3500
2025-04-03 18:28:10,053 - INFO - Epoch 10, Batch 67: Training loss = 0.3400
2025-04-03 18:28:10,154 - INFO - Epoch 10, Batch 68: Training loss = 0.3300
2025-04-03 18:28:10,254 - INFO - Epoch 10, Batch 69: Training loss = 0.3200
2025-04-03 18:28:10,355 - INFO - Epoch 10, Batch 70: Training loss = 0.3100
2025-04-03 18:28:10,455 - INFO - Epoch 10, Batch 71: Training loss = 0.3000
2025-04-03 18:28:10,555 - INFO - Epoch 10, Batch 72: Training loss = 0.2900
2025-04-03 18:28:10,656 - INFO - Epoch 10, Batch 73: Training loss = 0.2800
2025-04-03 18:28:10,756 - INFO - Epoch 10, Batch 74: Training loss = 0.2700
2025-04-03 18:28:10,857 - INFO - Epoch 10, Batch 75: Training loss = 0.2600
2025-04-03 18:28:10,957 - INFO - Epoch 10, Batch 76: Training loss = 0.2500
2025-04-03 18:28:11,057 - INFO - Epoch 10, Batch 77: Training loss = 0.2400
2025-04-03 18:28:11,158 - INFO - Epoch 10, Batch 78: Training loss = 0.2300
2025-04-03 18:28:11,258 - INFO - Epoch 10, Batch 79: Training loss = 0.2200
2025-04-03 18:28:11,359 - INFO - Epoch 10, Batch 80: Training loss = 0.2100
2025-04-03 18:28:11,459 - INFO - Epoch 10, Batch 81: Training loss = 0.2000
2025-04-03 18:28:11,559 - INFO - Epoch 10, Batch 82: Training loss = 0.1900
2025-04-03 18:28:11,660 - INFO - Epoch 10, Batch 83: Training loss = 0.1800
2025-04-03 18:28:11,760 - INFO - Epoch 10, Batch 84: Training loss = 0.1700
2025-04-03 18:28:11,861 - INFO - Epoch 10, Batch 85: Training loss = 0.1600
2025-04-03 18:28:11,961 - INFO - Epoch 10, Batch 86: Training loss = 0.1500
2025-04-03 18:28:12,062 - INFO - Epoch 10, Batch 87: Training loss = 0.1400
2025-04-03 18:28:12,162 - INFO - Epoch 10, Batch 88: Training loss = 0.1300
2025-04-03 18:28:12,262 - INFO - Epoch 10, Batch 89: Training loss = 0.1200
2025-04-03 18:28:12,363 - INFO - Epoch 10, Batch 90: Training loss = 0.1100
2025-04-03 18:28:12,463 - INFO - Epoch 10, Batch 91: Training loss = 0.1000
2025-04-03 18:28:12,564 - INFO - Epoch 10, Batch 92: Training loss = 0.0900
2025-04-03 18:28:12,664 - INFO - Epoch 10, Batch 93: Training loss = 0.0800
2025-04-03 18:28:12,765 - INFO - Epoch 10, Batch 94: Training loss = 0.0700
2025-04-03 18:28:12,865 - INFO - Epoch 10, Batch 95: Training loss = 0.0600
2025-04-03 18:28:12,966 - INFO - Epoch 10, Batch 96: Training loss = 0.0500
2025-04-03 18:28:13,066 - INFO - Epoch 10, Batch 97: Training loss = 0.0400
2025-04-03 18:28:13,166 - INFO - Epoch 10, Batch 98: Training loss = 0.0300
2025-04-03 18:28:13,267 - INFO - Epoch 10, Batch 99: Training loss = 0.0200
2025-04-03 18:28:13,367 - INFO - Epoch 10, Batch 100: Training loss = 0.0100
2025-04-03 18:28:13,468 - INFO - Epoch 10: Validation loss = 0.0100, Validation accuracy = 0.9800
2025-04-03 18:28:13,468 - INFO - Epoch 10: MSE = 0.0100, RMSE = 0.1000, MAE = 0.0080, RÂ² = 0.9900
2025-04-03 18:28:13,468 - INFO - Regression training process completed.
2025-04-04 15:58:06,846 - INFO - Starting regression training process...
2025-04-04 15:58:06,846 - INFO - Epoch 1 started.
2025-04-04 15:58:06,846 - INFO - Epoch 1, Batch 1: Training loss = 1.0000
2025-04-04 15:58:06,947 - INFO - Epoch 1, Batch 2: Training loss = 0.9900
2025-04-04 15:58:07,048 - INFO - Epoch 1, Batch 3: Training loss = 0.9800
2025-04-04 15:58:07,149 - INFO - Epoch 1, Batch 4: Training loss = 0.9700
2025-04-04 15:58:07,250 - INFO - Epoch 1, Batch 5: Training loss = 0.9600
2025-04-04 15:58:07,351 - INFO - Epoch 1, Batch 6: Training loss = 0.9500
2025-04-04 15:58:07,452 - INFO - Epoch 1, Batch 7: Training loss = 0.9400
2025-04-04 15:58:07,553 - INFO - Epoch 1, Batch 8: Training loss = 0.9300
2025-04-04 15:58:07,653 - INFO - Epoch 1, Batch 9: Training loss = 0.9200
2025-04-04 15:58:07,754 - INFO - Epoch 1, Batch 10: Training loss = 0.9100
2025-04-04 15:58:07,854 - INFO - Epoch 1, Batch 11: Training loss = 0.9000
2025-04-04 15:58:07,955 - INFO - Epoch 1, Batch 12: Training loss = 0.8900
2025-04-04 15:58:08,055 - INFO - Epoch 1, Batch 13: Training loss = 0.8800
2025-04-04 15:58:08,156 - INFO - Epoch 1, Batch 14: Training loss = 0.8700
2025-04-04 15:58:08,256 - INFO - Epoch 1, Batch 15: Training loss = 0.8600
2025-04-04 15:58:08,357 - INFO - Epoch 1, Batch 16: Training loss = 0.8500
2025-04-04 15:58:08,458 - INFO - Epoch 1, Batch 17: Training loss = 0.8400
2025-04-04 15:58:08,558 - INFO - Epoch 1, Batch 18: Training loss = 0.8300
2025-04-04 15:58:08,659 - INFO - Epoch 1, Batch 19: Training loss = 0.8200
2025-04-04 15:58:08,760 - INFO - Epoch 1, Batch 20: Training loss = 0.8100
2025-04-04 15:58:08,860 - INFO - Epoch 1, Batch 21: Training loss = 0.8000
2025-04-04 15:58:08,961 - INFO - Epoch 1, Batch 22: Training loss = 0.7900
2025-04-04 15:58:09,061 - INFO - Epoch 1, Batch 23: Training loss = 0.7800
2025-04-04 15:58:09,162 - INFO - Epoch 1, Batch 24: Training loss = 0.7700
2025-04-04 15:58:09,263 - INFO - Epoch 1, Batch 25: Training loss = 0.7600
2025-04-04 15:58:09,364 - INFO - Epoch 1, Batch 26: Training loss = 0.7500
2025-04-04 15:58:09,465 - INFO - Epoch 1, Batch 27: Training loss = 0.7400
2025-04-04 15:58:09,566 - INFO - Epoch 1, Batch 28: Training loss = 0.7300
2025-04-04 15:58:09,667 - INFO - Epoch 1, Batch 29: Training loss = 0.7200
2025-04-04 15:58:09,768 - INFO - Epoch 1, Batch 30: Training loss = 0.7100
2025-04-04 15:58:09,868 - INFO - Epoch 1, Batch 31: Training loss = 0.7000
2025-04-04 15:58:09,969 - INFO - Epoch 1, Batch 32: Training loss = 0.6900
2025-04-04 15:58:10,070 - INFO - Epoch 1, Batch 33: Training loss = 0.6800
2025-04-04 15:58:10,170 - INFO - Epoch 1, Batch 34: Training loss = 0.6700
2025-04-04 15:58:10,271 - INFO - Epoch 1, Batch 35: Training loss = 0.6600
2025-04-04 15:58:10,372 - INFO - Epoch 1, Batch 36: Training loss = 0.6500
2025-04-04 15:58:10,472 - INFO - Epoch 1, Batch 37: Training loss = 0.6400
2025-04-04 15:58:10,573 - INFO - Epoch 1, Batch 38: Training loss = 0.6300
2025-04-04 15:58:10,673 - INFO - Epoch 1, Batch 39: Training loss = 0.6200
2025-04-04 15:58:10,774 - INFO - Epoch 1, Batch 40: Training loss = 0.6100
2025-04-04 15:58:10,874 - INFO - Epoch 1, Batch 41: Training loss = 0.6000
2025-04-04 15:58:10,975 - INFO - Epoch 1, Batch 42: Training loss = 0.5900
2025-04-04 15:58:11,076 - INFO - Epoch 1, Batch 43: Training loss = 0.5800
2025-04-04 15:58:11,177 - INFO - Epoch 1, Batch 44: Training loss = 0.5700
2025-04-04 15:58:11,278 - INFO - Epoch 1, Batch 45: Training loss = 0.5600
2025-04-04 15:58:11,378 - INFO - Epoch 1, Batch 46: Training loss = 0.5500
2025-04-04 15:58:11,479 - INFO - Epoch 1, Batch 47: Training loss = 0.5400
2025-04-04 15:58:11,580 - INFO - Epoch 1, Batch 48: Training loss = 0.5300
2025-04-04 15:58:11,681 - INFO - Epoch 1, Batch 49: Training loss = 0.5200
2025-04-04 15:58:11,782 - INFO - Epoch 1, Batch 50: Training loss = 0.5100
2025-04-04 15:58:11,882 - INFO - Epoch 1, Batch 51: Training loss = 0.5000
2025-04-04 15:58:11,983 - INFO - Epoch 1, Batch 52: Training loss = 0.4900
2025-04-04 15:58:12,083 - INFO - Epoch 1, Batch 53: Training loss = 0.4800
2025-04-04 15:58:12,184 - INFO - Epoch 1, Batch 54: Training loss = 0.4700
2025-04-04 15:58:12,286 - INFO - Epoch 1, Batch 55: Training loss = 0.4600
2025-04-04 15:58:12,387 - INFO - Epoch 1, Batch 56: Training loss = 0.4500
2025-04-04 15:58:12,488 - INFO - Epoch 1, Batch 57: Training loss = 0.4400
2025-04-04 15:58:12,588 - INFO - Epoch 1, Batch 58: Training loss = 0.4300
2025-04-04 15:58:12,689 - INFO - Epoch 1, Batch 59: Training loss = 0.4200
2025-04-04 15:58:12,790 - INFO - Epoch 1, Batch 60: Training loss = 0.4100
2025-04-04 15:58:12,891 - INFO - Epoch 1, Batch 61: Training loss = 0.4000
2025-04-04 15:58:12,991 - INFO - Epoch 1, Batch 62: Training loss = 0.3900
2025-04-04 15:58:13,092 - INFO - Epoch 1, Batch 63: Training loss = 0.3800
2025-04-04 15:58:13,193 - INFO - Epoch 1, Batch 64: Training loss = 0.3700
2025-04-04 15:58:13,293 - INFO - Epoch 1, Batch 65: Training loss = 0.3600
2025-04-04 15:58:13,394 - INFO - Epoch 1, Batch 66: Training loss = 0.3500
2025-04-04 15:58:13,495 - INFO - Epoch 1, Batch 67: Training loss = 0.3400
2025-04-04 15:58:13,596 - INFO - Epoch 1, Batch 68: Training loss = 0.3300
2025-04-04 15:58:13,697 - INFO - Epoch 1, Batch 69: Training loss = 0.3200
2025-04-04 15:58:13,798 - INFO - Epoch 1, Batch 70: Training loss = 0.3100
2025-04-04 15:58:13,898 - INFO - Epoch 1, Batch 71: Training loss = 0.3000
2025-04-04 15:58:14,000 - INFO - Epoch 1, Batch 72: Training loss = 0.2900
2025-04-04 15:58:14,101 - INFO - Epoch 1, Batch 73: Training loss = 0.2800
2025-04-04 15:58:14,202 - INFO - Epoch 1, Batch 74: Training loss = 0.2700
2025-04-04 15:58:14,303 - INFO - Epoch 1, Batch 75: Training loss = 0.2600
2025-04-04 15:58:14,404 - INFO - Epoch 1, Batch 76: Training loss = 0.2500
2025-04-04 15:58:14,505 - INFO - Epoch 1, Batch 77: Training loss = 0.2400
2025-04-04 15:58:14,605 - INFO - Epoch 1, Batch 78: Training loss = 0.2300
2025-04-04 15:58:14,706 - INFO - Epoch 1, Batch 79: Training loss = 0.2200
2025-04-04 15:58:14,807 - INFO - Epoch 1, Batch 80: Training loss = 0.2100
2025-04-04 15:58:14,908 - INFO - Epoch 1, Batch 81: Training loss = 0.2000
2025-04-04 15:58:15,008 - INFO - Epoch 1, Batch 82: Training loss = 0.1900
2025-04-04 15:58:15,109 - INFO - Epoch 1, Batch 83: Training loss = 0.1800
2025-04-04 15:58:15,210 - INFO - Epoch 1, Batch 84: Training loss = 0.1700
2025-04-04 15:58:15,311 - INFO - Epoch 1, Batch 85: Training loss = 0.1600
2025-04-04 15:58:15,411 - INFO - Epoch 1, Batch 86: Training loss = 0.1500
2025-04-04 15:58:15,512 - INFO - Epoch 1, Batch 87: Training loss = 0.1400
2025-04-04 15:58:15,613 - INFO - Epoch 1, Batch 88: Training loss = 0.1300
2025-04-04 15:58:15,714 - INFO - Epoch 1, Batch 89: Training loss = 0.1200
2025-04-04 15:58:15,814 - INFO - Epoch 1, Batch 90: Training loss = 0.1100
2025-04-04 15:58:15,915 - INFO - Epoch 1, Batch 91: Training loss = 0.1000
2025-04-04 15:58:16,015 - INFO - Epoch 1, Batch 92: Training loss = 0.0900
2025-04-04 15:58:16,116 - INFO - Epoch 1, Batch 93: Training loss = 0.0800
2025-04-04 15:58:16,217 - INFO - Epoch 1, Batch 94: Training loss = 0.0700
2025-04-04 15:58:16,318 - INFO - Epoch 1, Batch 95: Training loss = 0.0600
2025-04-04 15:58:16,419 - INFO - Epoch 1, Batch 96: Training loss = 0.0500
2025-04-04 15:58:16,520 - INFO - Epoch 1, Batch 97: Training loss = 0.0400
2025-04-04 15:58:16,621 - INFO - Epoch 1, Batch 98: Training loss = 0.0300
2025-04-04 15:58:16,722 - INFO - Epoch 1, Batch 99: Training loss = 0.0200
2025-04-04 15:58:16,822 - INFO - Epoch 1, Batch 100: Training loss = 0.0100
2025-04-04 15:58:16,923 - INFO - Epoch 1: Validation loss = 0.1000, Validation accuracy = 0.8000
2025-04-04 15:58:16,923 - INFO - Epoch 1: MSE = 0.1000, RMSE = 0.3162, MAE = 0.0800, RÂ² = 0.9000
2025-04-04 15:58:16,923 - INFO - Epoch 2 started.
2025-04-04 15:58:16,924 - INFO - Epoch 2, Batch 1: Training loss = 1.0000
2025-04-04 15:58:17,024 - INFO - Epoch 2, Batch 2: Training loss = 0.9900
2025-04-04 15:58:17,124 - INFO - Epoch 2, Batch 3: Training loss = 0.9800
2025-04-04 15:58:17,225 - INFO - Epoch 2, Batch 4: Training loss = 0.9700
2025-04-04 15:58:17,325 - INFO - Epoch 2, Batch 5: Training loss = 0.9600
2025-04-04 15:58:17,426 - INFO - Epoch 2, Batch 6: Training loss = 0.9500
2025-04-04 15:58:17,526 - INFO - Epoch 2, Batch 7: Training loss = 0.9400
2025-04-04 15:58:17,627 - INFO - Epoch 2, Batch 8: Training loss = 0.9300
2025-04-04 15:58:17,727 - INFO - Epoch 2, Batch 9: Training loss = 0.9200
2025-04-04 15:58:17,828 - INFO - Epoch 2, Batch 10: Training loss = 0.9100
2025-04-04 15:58:17,928 - INFO - Epoch 2, Batch 11: Training loss = 0.9000
2025-04-04 15:58:18,029 - INFO - Epoch 2, Batch 12: Training loss = 0.8900
2025-04-04 15:58:18,130 - INFO - Epoch 2, Batch 13: Training loss = 0.8800
2025-04-04 15:58:18,230 - INFO - Epoch 2, Batch 14: Training loss = 0.8700
2025-04-04 15:58:18,331 - INFO - Epoch 2, Batch 15: Training loss = 0.8600
2025-04-04 15:58:18,431 - INFO - Epoch 2, Batch 16: Training loss = 0.8500
2025-04-04 15:58:18,532 - INFO - Epoch 2, Batch 17: Training loss = 0.8400
2025-04-04 15:58:18,632 - INFO - Epoch 2, Batch 18: Training loss = 0.8300
2025-04-04 15:58:18,733 - INFO - Epoch 2, Batch 19: Training loss = 0.8200
2025-04-04 15:58:18,834 - INFO - Epoch 2, Batch 20: Training loss = 0.8100
2025-04-04 15:58:18,935 - INFO - Epoch 2, Batch 21: Training loss = 0.8000
2025-04-04 15:58:19,035 - INFO - Epoch 2, Batch 22: Training loss = 0.7900
2025-04-04 15:58:19,136 - INFO - Epoch 2, Batch 23: Training loss = 0.7800
2025-04-04 15:58:19,237 - INFO - Epoch 2, Batch 24: Training loss = 0.7700
2025-04-04 15:58:19,338 - INFO - Epoch 2, Batch 25: Training loss = 0.7600
2025-04-04 15:58:19,438 - INFO - Epoch 2, Batch 26: Training loss = 0.7500
2025-04-04 15:58:19,539 - INFO - Epoch 2, Batch 27: Training loss = 0.7400
2025-04-04 15:58:19,639 - INFO - Epoch 2, Batch 28: Training loss = 0.7300
2025-04-04 15:58:19,740 - INFO - Epoch 2, Batch 29: Training loss = 0.7200
2025-04-04 15:58:19,840 - INFO - Epoch 2, Batch 30: Training loss = 0.7100
2025-04-04 15:58:19,941 - INFO - Epoch 2, Batch 31: Training loss = 0.7000
2025-04-04 15:58:20,041 - INFO - Epoch 2, Batch 32: Training loss = 0.6900
2025-04-04 15:58:20,142 - INFO - Epoch 2, Batch 33: Training loss = 0.6800
2025-04-04 15:58:20,243 - INFO - Epoch 2, Batch 34: Training loss = 0.6700
2025-04-04 15:58:20,344 - INFO - Epoch 2, Batch 35: Training loss = 0.6600
2025-04-04 15:58:20,445 - INFO - Epoch 2, Batch 36: Training loss = 0.6500
2025-04-04 15:58:20,546 - INFO - Epoch 2, Batch 37: Training loss = 0.6400
2025-04-04 15:58:20,647 - INFO - Epoch 2, Batch 38: Training loss = 0.6300
2025-04-04 15:58:20,748 - INFO - Epoch 2, Batch 39: Training loss = 0.6200
2025-04-04 15:58:20,849 - INFO - Epoch 2, Batch 40: Training loss = 0.6100
2025-04-04 15:58:20,950 - INFO - Epoch 2, Batch 41: Training loss = 0.6000
2025-04-04 15:58:21,051 - INFO - Epoch 2, Batch 42: Training loss = 0.5900
2025-04-04 15:58:21,151 - INFO - Epoch 2, Batch 43: Training loss = 0.5800
2025-04-04 15:58:21,252 - INFO - Epoch 2, Batch 44: Training loss = 0.5700
2025-04-04 15:58:21,353 - INFO - Epoch 2, Batch 45: Training loss = 0.5600
2025-04-04 15:58:21,453 - INFO - Epoch 2, Batch 46: Training loss = 0.5500
2025-04-04 15:58:21,554 - INFO - Epoch 2, Batch 47: Training loss = 0.5400
2025-04-04 15:58:21,655 - INFO - Epoch 2, Batch 48: Training loss = 0.5300
2025-04-04 15:58:21,755 - INFO - Epoch 2, Batch 49: Training loss = 0.5200
2025-04-04 15:58:21,856 - INFO - Epoch 2, Batch 50: Training loss = 0.5100
2025-04-04 15:58:21,956 - INFO - Epoch 2, Batch 51: Training loss = 0.5000
2025-04-04 15:58:22,057 - INFO - Epoch 2, Batch 52: Training loss = 0.4900
2025-04-04 15:58:22,158 - INFO - Epoch 2, Batch 53: Training loss = 0.4800
2025-04-04 15:58:22,259 - INFO - Epoch 2, Batch 54: Training loss = 0.4700
2025-04-04 15:58:22,359 - INFO - Epoch 2, Batch 55: Training loss = 0.4600
2025-04-04 15:58:22,460 - INFO - Epoch 2, Batch 56: Training loss = 0.4500
2025-04-04 15:58:22,561 - INFO - Epoch 2, Batch 57: Training loss = 0.4400
2025-04-04 15:58:22,663 - INFO - Epoch 2, Batch 58: Training loss = 0.4300
2025-04-04 15:58:22,763 - INFO - Epoch 2, Batch 59: Training loss = 0.4200
2025-04-04 15:58:22,864 - INFO - Epoch 2, Batch 60: Training loss = 0.4100
2025-04-04 15:58:22,965 - INFO - Epoch 2, Batch 61: Training loss = 0.4000
2025-04-04 15:58:23,066 - INFO - Epoch 2, Batch 62: Training loss = 0.3900
2025-04-04 15:58:23,167 - INFO - Epoch 2, Batch 63: Training loss = 0.3800
2025-04-04 15:58:23,267 - INFO - Epoch 2, Batch 64: Training loss = 0.3700
2025-04-04 15:58:23,368 - INFO - Epoch 2, Batch 65: Training loss = 0.3600
2025-04-04 15:58:23,469 - INFO - Epoch 2, Batch 66: Training loss = 0.3500
2025-04-04 15:58:23,570 - INFO - Epoch 2, Batch 67: Training loss = 0.3400
2025-04-04 15:58:23,671 - INFO - Epoch 2, Batch 68: Training loss = 0.3300
2025-04-04 15:58:23,771 - INFO - Epoch 2, Batch 69: Training loss = 0.3200
2025-04-04 15:58:23,872 - INFO - Epoch 2, Batch 70: Training loss = 0.3100
2025-04-04 15:58:23,972 - INFO - Epoch 2, Batch 71: Training loss = 0.3000
2025-04-04 15:58:24,072 - INFO - Epoch 2, Batch 72: Training loss = 0.2900
2025-04-04 15:58:24,173 - INFO - Epoch 2, Batch 73: Training loss = 0.2800
2025-04-04 15:58:24,274 - INFO - Epoch 2, Batch 74: Training loss = 0.2700
2025-04-04 15:58:24,375 - INFO - Epoch 2, Batch 75: Training loss = 0.2600
2025-04-04 15:58:24,476 - INFO - Epoch 2, Batch 76: Training loss = 0.2500
2025-04-04 15:58:24,576 - INFO - Epoch 2, Batch 77: Training loss = 0.2400
2025-04-04 15:58:24,677 - INFO - Epoch 2, Batch 78: Training loss = 0.2300
2025-04-04 15:58:24,778 - INFO - Epoch 2, Batch 79: Training loss = 0.2200
2025-04-04 15:58:24,879 - INFO - Epoch 2, Batch 80: Training loss = 0.2100
2025-04-04 15:58:24,980 - INFO - Epoch 2, Batch 81: Training loss = 0.2000
2025-04-04 15:58:25,080 - INFO - Epoch 2, Batch 82: Training loss = 0.1900
2025-04-04 15:58:25,181 - INFO - Epoch 2, Batch 83: Training loss = 0.1800
2025-04-04 15:58:25,281 - INFO - Epoch 2, Batch 84: Training loss = 0.1700
2025-04-04 15:58:25,382 - INFO - Epoch 2, Batch 85: Training loss = 0.1600
2025-04-04 15:58:25,482 - INFO - Epoch 2, Batch 86: Training loss = 0.1500
2025-04-04 15:58:25,583 - INFO - Epoch 2, Batch 87: Training loss = 0.1400
2025-04-04 15:58:25,684 - INFO - Epoch 2, Batch 88: Training loss = 0.1300
2025-04-04 15:58:25,785 - INFO - Epoch 2, Batch 89: Training loss = 0.1200
2025-04-04 15:58:25,885 - INFO - Epoch 2, Batch 90: Training loss = 0.1100
2025-04-04 15:58:25,986 - INFO - Epoch 2, Batch 91: Training loss = 0.1000
2025-04-04 15:58:26,086 - INFO - Epoch 2, Batch 92: Training loss = 0.0900
2025-04-04 15:58:26,187 - INFO - Epoch 2, Batch 93: Training loss = 0.0800
2025-04-04 15:58:26,288 - INFO - Epoch 2, Batch 94: Training loss = 0.0700
2025-04-04 15:58:26,388 - INFO - Epoch 2, Batch 95: Training loss = 0.0600
2025-04-04 15:58:26,489 - INFO - Epoch 2, Batch 96: Training loss = 0.0500
2025-04-04 15:58:26,590 - INFO - Epoch 2, Batch 97: Training loss = 0.0400
2025-04-04 15:58:26,690 - INFO - Epoch 2, Batch 98: Training loss = 0.0300
2025-04-04 15:58:26,791 - INFO - Epoch 2, Batch 99: Training loss = 0.0200
2025-04-04 15:58:26,892 - INFO - Epoch 2, Batch 100: Training loss = 0.0100
2025-04-04 15:58:26,993 - INFO - Epoch 2: Validation loss = 0.0900, Validation accuracy = 0.8200
2025-04-04 15:58:26,993 - INFO - Epoch 2: MSE = 0.0900, RMSE = 0.3000, MAE = 0.0720, RÂ² = 0.9100
2025-04-04 15:58:26,993 - INFO - Epoch 3 started.
2025-04-04 15:58:26,994 - INFO - Epoch 3, Batch 1: Training loss = 1.0000
2025-04-04 15:58:27,094 - INFO - Epoch 3, Batch 2: Training loss = 0.9900
2025-04-04 15:58:27,194 - INFO - Epoch 3, Batch 3: Training loss = 0.9800
2025-04-04 15:58:27,295 - INFO - Epoch 3, Batch 4: Training loss = 0.9700
2025-04-04 15:58:27,396 - INFO - Epoch 3, Batch 5: Training loss = 0.9600
2025-04-04 15:58:27,497 - INFO - Epoch 3, Batch 6: Training loss = 0.9500
2025-04-04 15:58:27,598 - INFO - Epoch 3, Batch 7: Training loss = 0.9400
2025-04-04 15:58:27,699 - INFO - Epoch 3, Batch 8: Training loss = 0.9300
2025-04-04 15:58:27,800 - INFO - Epoch 3, Batch 9: Training loss = 0.9200
2025-04-04 15:58:27,900 - INFO - Epoch 3, Batch 10: Training loss = 0.9100
2025-04-04 15:58:28,001 - INFO - Epoch 3, Batch 11: Training loss = 0.9000
2025-04-04 15:58:28,102 - INFO - Epoch 3, Batch 12: Training loss = 0.8900
2025-04-04 15:58:28,203 - INFO - Epoch 3, Batch 13: Training loss = 0.8800
2025-04-04 15:58:28,304 - INFO - Epoch 3, Batch 14: Training loss = 0.8700
2025-04-04 15:58:28,404 - INFO - Epoch 3, Batch 15: Training loss = 0.8600
2025-04-04 15:58:28,505 - INFO - Epoch 3, Batch 16: Training loss = 0.8500
2025-04-04 15:58:28,606 - INFO - Epoch 3, Batch 17: Training loss = 0.8400
2025-04-04 15:58:28,706 - INFO - Epoch 3, Batch 18: Training loss = 0.8300
2025-04-04 15:58:28,807 - INFO - Epoch 3, Batch 19: Training loss = 0.8200
2025-04-04 15:58:28,908 - INFO - Epoch 3, Batch 20: Training loss = 0.8100
2025-04-04 15:58:29,008 - INFO - Epoch 3, Batch 21: Training loss = 0.8000
2025-04-04 15:58:29,109 - INFO - Epoch 3, Batch 22: Training loss = 0.7900
2025-04-04 15:58:29,210 - INFO - Epoch 3, Batch 23: Training loss = 0.7800
2025-04-04 15:58:29,311 - INFO - Epoch 3, Batch 24: Training loss = 0.7700
2025-04-04 15:58:29,412 - INFO - Epoch 3, Batch 25: Training loss = 0.7600
2025-04-04 15:58:29,513 - INFO - Epoch 3, Batch 26: Training loss = 0.7500
2025-04-04 15:58:29,613 - INFO - Epoch 3, Batch 27: Training loss = 0.7400
2025-04-04 15:58:29,714 - INFO - Epoch 3, Batch 28: Training loss = 0.7300
2025-04-04 15:58:29,815 - INFO - Epoch 3, Batch 29: Training loss = 0.7200
2025-04-04 15:58:29,915 - INFO - Epoch 3, Batch 30: Training loss = 0.7100
2025-04-04 15:58:30,016 - INFO - Epoch 3, Batch 31: Training loss = 0.7000
2025-04-04 15:58:30,117 - INFO - Epoch 3, Batch 32: Training loss = 0.6900
2025-04-04 15:58:30,218 - INFO - Epoch 3, Batch 33: Training loss = 0.6800
2025-04-04 15:58:30,319 - INFO - Epoch 3, Batch 34: Training loss = 0.6700
2025-04-04 15:58:30,420 - INFO - Epoch 3, Batch 35: Training loss = 0.6600
2025-04-04 15:58:30,520 - INFO - Epoch 3, Batch 36: Training loss = 0.6500
2025-04-04 15:58:30,621 - INFO - Epoch 3, Batch 37: Training loss = 0.6400
2025-04-04 15:58:30,722 - INFO - Epoch 3, Batch 38: Training loss = 0.6300
2025-04-04 15:58:30,822 - INFO - Epoch 3, Batch 39: Training loss = 0.6200
2025-04-04 15:58:30,923 - INFO - Epoch 3, Batch 40: Training loss = 0.6100
2025-04-04 15:58:31,023 - INFO - Epoch 3, Batch 41: Training loss = 0.6000
2025-04-04 15:58:31,124 - INFO - Epoch 3, Batch 42: Training loss = 0.5900
2025-04-04 15:58:31,225 - INFO - Epoch 3, Batch 43: Training loss = 0.5800
2025-04-04 15:58:31,326 - INFO - Epoch 3, Batch 44: Training loss = 0.5700
2025-04-04 15:58:31,427 - INFO - Epoch 3, Batch 45: Training loss = 0.5600
2025-04-04 15:58:31,528 - INFO - Epoch 3, Batch 46: Training loss = 0.5500
2025-04-04 15:58:31,629 - INFO - Epoch 3, Batch 47: Training loss = 0.5400
2025-04-04 15:58:31,730 - INFO - Epoch 3, Batch 48: Training loss = 0.5300
2025-04-04 15:58:31,830 - INFO - Epoch 3, Batch 49: Training loss = 0.5200
2025-04-04 15:58:31,931 - INFO - Epoch 3, Batch 50: Training loss = 0.5100
2025-04-04 15:58:32,032 - INFO - Epoch 3, Batch 51: Training loss = 0.5000
2025-04-04 15:58:32,132 - INFO - Epoch 3, Batch 52: Training loss = 0.4900
2025-04-04 15:58:32,233 - INFO - Epoch 3, Batch 53: Training loss = 0.4800
2025-04-04 15:58:32,334 - INFO - Epoch 3, Batch 54: Training loss = 0.4700
2025-04-04 15:58:32,435 - INFO - Epoch 3, Batch 55: Training loss = 0.4600
2025-04-04 15:58:32,535 - INFO - Epoch 3, Batch 56: Training loss = 0.4500
2025-04-04 15:58:32,636 - INFO - Epoch 3, Batch 57: Training loss = 0.4400
2025-04-04 15:58:32,737 - INFO - Epoch 3, Batch 58: Training loss = 0.4300
2025-04-04 15:58:32,837 - INFO - Epoch 3, Batch 59: Training loss = 0.4200
2025-04-04 15:58:32,938 - INFO - Epoch 3, Batch 60: Training loss = 0.4100
2025-04-04 15:58:33,038 - INFO - Epoch 3, Batch 61: Training loss = 0.4000
2025-04-04 15:58:33,139 - INFO - Epoch 3, Batch 62: Training loss = 0.3900
2025-04-04 15:58:33,240 - INFO - Epoch 3, Batch 63: Training loss = 0.3800
2025-04-04 15:58:33,342 - INFO - Epoch 3, Batch 64: Training loss = 0.3700
2025-04-04 15:58:33,442 - INFO - Epoch 3, Batch 65: Training loss = 0.3600
2025-04-04 15:58:33,543 - INFO - Epoch 3, Batch 66: Training loss = 0.3500
2025-04-04 15:58:33,644 - INFO - Epoch 3, Batch 67: Training loss = 0.3400
2025-04-04 15:58:33,745 - INFO - Epoch 3, Batch 68: Training loss = 0.3300
2025-04-04 15:58:33,845 - INFO - Epoch 3, Batch 69: Training loss = 0.3200
2025-04-04 15:58:33,946 - INFO - Epoch 3, Batch 70: Training loss = 0.3100
2025-04-04 15:58:34,047 - INFO - Epoch 3, Batch 71: Training loss = 0.3000
2025-04-04 15:58:34,148 - INFO - Epoch 3, Batch 72: Training loss = 0.2900
2025-04-04 15:58:34,248 - INFO - Epoch 3, Batch 73: Training loss = 0.2800
2025-04-04 15:58:34,350 - INFO - Epoch 3, Batch 74: Training loss = 0.2700
2025-04-04 15:58:34,451 - INFO - Epoch 3, Batch 75: Training loss = 0.2600
2025-04-04 15:58:34,552 - INFO - Epoch 3, Batch 76: Training loss = 0.2500
2025-04-04 15:58:34,652 - INFO - Epoch 3, Batch 77: Training loss = 0.2400
2025-04-04 15:58:34,753 - INFO - Epoch 3, Batch 78: Training loss = 0.2300
2025-04-04 15:58:34,854 - INFO - Epoch 3, Batch 79: Training loss = 0.2200
2025-04-04 15:58:34,954 - INFO - Epoch 3, Batch 80: Training loss = 0.2100
2025-04-04 15:58:35,055 - INFO - Epoch 3, Batch 81: Training loss = 0.2000
2025-04-04 15:58:35,155 - INFO - Epoch 3, Batch 82: Training loss = 0.1900
2025-04-04 15:58:35,257 - INFO - Epoch 3, Batch 83: Training loss = 0.1800
2025-04-04 15:58:35,358 - INFO - Epoch 3, Batch 84: Training loss = 0.1700
2025-04-04 15:58:35,459 - INFO - Epoch 3, Batch 85: Training loss = 0.1600
2025-04-04 15:58:35,560 - INFO - Epoch 3, Batch 86: Training loss = 0.1500
2025-04-04 15:58:35,661 - INFO - Epoch 3, Batch 87: Training loss = 0.1400
2025-04-04 15:58:35,761 - INFO - Epoch 3, Batch 88: Training loss = 0.1300
2025-04-04 15:58:35,862 - INFO - Epoch 3, Batch 89: Training loss = 0.1200
2025-04-04 15:58:35,963 - INFO - Epoch 3, Batch 90: Training loss = 0.1100
2025-04-04 15:58:36,063 - INFO - Epoch 3, Batch 91: Training loss = 0.1000
2025-04-04 15:58:36,164 - INFO - Epoch 3, Batch 92: Training loss = 0.0900
2025-04-04 15:58:36,264 - INFO - Epoch 3, Batch 93: Training loss = 0.0800
2025-04-04 15:58:36,365 - INFO - Epoch 3, Batch 94: Training loss = 0.0700
2025-04-04 15:58:36,465 - INFO - Epoch 3, Batch 95: Training loss = 0.0600
2025-04-04 15:58:36,566 - INFO - Epoch 3, Batch 96: Training loss = 0.0500
2025-04-04 15:58:36,667 - INFO - Epoch 3, Batch 97: Training loss = 0.0400
2025-04-04 15:58:36,767 - INFO - Epoch 3, Batch 98: Training loss = 0.0300
2025-04-04 15:58:36,868 - INFO - Epoch 3, Batch 99: Training loss = 0.0200
2025-04-04 15:58:36,969 - INFO - Epoch 3, Batch 100: Training loss = 0.0100
2025-04-04 15:58:37,069 - INFO - Epoch 3: Validation loss = 0.0800, Validation accuracy = 0.8400
2025-04-04 15:58:37,070 - INFO - Epoch 3: MSE = 0.0800, RMSE = 0.2828, MAE = 0.0640, RÂ² = 0.9200
2025-04-04 15:58:37,071 - INFO - Epoch 4 started.
2025-04-04 15:58:37,071 - INFO - Epoch 4, Batch 1: Training loss = 1.0000
2025-04-04 15:58:37,171 - INFO - Epoch 4, Batch 2: Training loss = 0.9900
2025-04-04 15:58:37,272 - INFO - Epoch 4, Batch 3: Training loss = 0.9800
2025-04-04 15:58:37,373 - INFO - Epoch 4, Batch 4: Training loss = 0.9700
2025-04-04 15:58:37,474 - INFO - Epoch 4, Batch 5: Training loss = 0.9600
2025-04-04 15:58:37,574 - INFO - Epoch 4, Batch 6: Training loss = 0.9500
2025-04-04 15:58:37,675 - INFO - Epoch 4, Batch 7: Training loss = 0.9400
2025-04-04 15:58:37,775 - INFO - Epoch 4, Batch 8: Training loss = 0.9300
2025-04-04 15:58:37,876 - INFO - Epoch 4, Batch 9: Training loss = 0.9200
2025-04-04 15:58:37,977 - INFO - Epoch 4, Batch 10: Training loss = 0.9100
2025-04-04 15:58:38,077 - INFO - Epoch 4, Batch 11: Training loss = 0.9000
2025-04-04 15:58:38,178 - INFO - Epoch 4, Batch 12: Training loss = 0.8900
2025-04-04 15:58:38,279 - INFO - Epoch 4, Batch 13: Training loss = 0.8800
2025-04-04 15:58:38,379 - INFO - Epoch 4, Batch 14: Training loss = 0.8700
2025-04-04 15:58:38,480 - INFO - Epoch 4, Batch 15: Training loss = 0.8600
2025-04-04 15:58:38,581 - INFO - Epoch 4, Batch 16: Training loss = 0.8500
2025-04-04 15:58:38,682 - INFO - Epoch 4, Batch 17: Training loss = 0.8400
2025-04-04 15:58:38,783 - INFO - Epoch 4, Batch 18: Training loss = 0.8300
2025-04-04 15:58:38,883 - INFO - Epoch 4, Batch 19: Training loss = 0.8200
2025-04-04 15:58:38,984 - INFO - Epoch 4, Batch 20: Training loss = 0.8100
2025-04-04 15:58:39,084 - INFO - Epoch 4, Batch 21: Training loss = 0.8000
2025-04-04 15:58:39,185 - INFO - Epoch 4, Batch 22: Training loss = 0.7900
2025-04-04 15:58:39,286 - INFO - Epoch 4, Batch 23: Training loss = 0.7800
2025-04-04 15:58:39,387 - INFO - Epoch 4, Batch 24: Training loss = 0.7700
2025-04-04 15:58:39,487 - INFO - Epoch 4, Batch 25: Training loss = 0.7600
2025-04-04 15:58:39,588 - INFO - Epoch 4, Batch 26: Training loss = 0.7500
2025-04-04 15:58:39,689 - INFO - Epoch 4, Batch 27: Training loss = 0.7400
2025-04-04 15:58:39,789 - INFO - Epoch 4, Batch 28: Training loss = 0.7300
2025-04-04 15:58:39,890 - INFO - Epoch 4, Batch 29: Training loss = 0.7200
2025-04-04 15:58:39,991 - INFO - Epoch 4, Batch 30: Training loss = 0.7100
2025-04-04 15:58:40,092 - INFO - Epoch 4, Batch 31: Training loss = 0.7000
2025-04-04 15:58:40,192 - INFO - Epoch 4, Batch 32: Training loss = 0.6900
2025-04-04 15:58:40,293 - INFO - Epoch 4, Batch 33: Training loss = 0.6800
2025-04-04 15:58:40,393 - INFO - Epoch 4, Batch 34: Training loss = 0.6700
2025-04-04 15:58:40,494 - INFO - Epoch 4, Batch 35: Training loss = 0.6600
2025-04-04 15:58:40,595 - INFO - Epoch 4, Batch 36: Training loss = 0.6500
2025-04-04 15:58:40,696 - INFO - Epoch 4, Batch 37: Training loss = 0.6400
2025-04-04 15:58:40,797 - INFO - Epoch 4, Batch 38: Training loss = 0.6300
2025-04-04 15:58:40,898 - INFO - Epoch 4, Batch 39: Training loss = 0.6200
2025-04-04 15:58:40,999 - INFO - Epoch 4, Batch 40: Training loss = 0.6100
2025-04-04 15:58:41,099 - INFO - Epoch 4, Batch 41: Training loss = 0.6000
2025-04-04 15:58:41,200 - INFO - Epoch 4, Batch 42: Training loss = 0.5900
2025-04-04 15:58:41,301 - INFO - Epoch 4, Batch 43: Training loss = 0.5800
2025-04-04 15:58:41,402 - INFO - Epoch 4, Batch 44: Training loss = 0.5700
2025-04-04 15:58:41,502 - INFO - Epoch 4, Batch 45: Training loss = 0.5600
2025-04-04 15:58:41,603 - INFO - Epoch 4, Batch 46: Training loss = 0.5500
2025-04-04 15:58:41,703 - INFO - Epoch 4, Batch 47: Training loss = 0.5400
2025-04-04 15:58:41,804 - INFO - Epoch 4, Batch 48: Training loss = 0.5300
2025-04-04 15:58:41,905 - INFO - Epoch 4, Batch 49: Training loss = 0.5200
2025-04-04 15:58:42,005 - INFO - Epoch 4, Batch 50: Training loss = 0.5100
2025-04-04 15:58:42,106 - INFO - Epoch 4, Batch 51: Training loss = 0.5000
2025-04-04 15:58:42,206 - INFO - Epoch 4, Batch 52: Training loss = 0.4900
2025-04-04 15:58:42,307 - INFO - Epoch 4, Batch 53: Training loss = 0.4800
2025-04-04 15:58:42,407 - INFO - Epoch 4, Batch 54: Training loss = 0.4700
2025-04-04 15:58:42,508 - INFO - Epoch 4, Batch 55: Training loss = 0.4600
2025-04-04 15:58:42,608 - INFO - Epoch 4, Batch 56: Training loss = 0.4500
2025-04-04 15:58:42,709 - INFO - Epoch 4, Batch 57: Training loss = 0.4400
2025-04-04 15:58:42,809 - INFO - Epoch 4, Batch 58: Training loss = 0.4300
2025-04-04 15:58:42,909 - INFO - Epoch 4, Batch 59: Training loss = 0.4200
2025-04-04 15:58:43,010 - INFO - Epoch 4, Batch 60: Training loss = 0.4100
2025-04-04 15:58:43,110 - INFO - Epoch 4, Batch 61: Training loss = 0.4000
2025-04-04 15:58:43,211 - INFO - Epoch 4, Batch 62: Training loss = 0.3900
2025-04-04 15:58:43,312 - INFO - Epoch 4, Batch 63: Training loss = 0.3800
2025-04-04 15:58:43,413 - INFO - Epoch 4, Batch 64: Training loss = 0.3700
2025-04-04 15:58:43,514 - INFO - Epoch 4, Batch 65: Training loss = 0.3600
2025-04-04 15:58:43,615 - INFO - Epoch 4, Batch 66: Training loss = 0.3500
2025-04-04 15:58:43,715 - INFO - Epoch 4, Batch 67: Training loss = 0.3400
2025-04-04 15:58:43,816 - INFO - Epoch 4, Batch 68: Training loss = 0.3300
2025-04-04 15:58:43,916 - INFO - Epoch 4, Batch 69: Training loss = 0.3200
2025-04-04 15:58:44,017 - INFO - Epoch 4, Batch 70: Training loss = 0.3100
2025-04-04 15:58:44,117 - INFO - Epoch 4, Batch 71: Training loss = 0.3000
2025-04-04 15:58:44,218 - INFO - Epoch 4, Batch 72: Training loss = 0.2900
2025-04-04 15:58:44,319 - INFO - Epoch 4, Batch 73: Training loss = 0.2800
2025-04-04 15:58:44,419 - INFO - Epoch 4, Batch 74: Training loss = 0.2700
2025-04-04 15:58:44,520 - INFO - Epoch 4, Batch 75: Training loss = 0.2600
2025-04-04 15:58:44,620 - INFO - Epoch 4, Batch 76: Training loss = 0.2500
2025-04-04 15:58:44,721 - INFO - Epoch 4, Batch 77: Training loss = 0.2400
2025-04-04 15:58:44,821 - INFO - Epoch 4, Batch 78: Training loss = 0.2300
2025-04-04 15:58:44,922 - INFO - Epoch 4, Batch 79: Training loss = 0.2200
2025-04-04 15:58:45,022 - INFO - Epoch 4, Batch 80: Training loss = 0.2100
2025-04-04 15:58:45,122 - INFO - Epoch 4, Batch 81: Training loss = 0.2000
2025-04-04 15:58:45,223 - INFO - Epoch 4, Batch 82: Training loss = 0.1900
2025-04-04 15:59:08,275 - INFO - Starting regression training process...
2025-04-04 15:59:08,275 - INFO - Epoch 1 started.
2025-04-04 15:59:08,275 - INFO - Epoch 1, Batch 1: Training loss = 1.0000
2025-04-04 15:59:08,376 - INFO - Epoch 1, Batch 2: Training loss = 0.9900
2025-04-04 15:59:08,476 - INFO - Epoch 1, Batch 3: Training loss = 0.9800
2025-04-04 15:59:08,577 - INFO - Epoch 1, Batch 4: Training loss = 0.9700
2025-04-04 15:59:08,677 - INFO - Epoch 1, Batch 5: Training loss = 0.9600
2025-04-04 15:59:08,778 - INFO - Epoch 1, Batch 6: Training loss = 0.9500
2025-04-04 15:59:08,878 - INFO - Epoch 1, Batch 7: Training loss = 0.9400
2025-04-04 15:59:08,979 - INFO - Epoch 1, Batch 8: Training loss = 0.9300
2025-04-04 15:59:09,079 - INFO - Epoch 1, Batch 9: Training loss = 0.9200
2025-04-04 15:59:09,180 - INFO - Epoch 1, Batch 10: Training loss = 0.9100
2025-04-04 15:59:09,281 - INFO - Epoch 1, Batch 11: Training loss = 0.9000
2025-04-04 15:59:09,382 - INFO - Epoch 1, Batch 12: Training loss = 0.8900
2025-04-04 15:59:09,483 - INFO - Epoch 1, Batch 13: Training loss = 0.8800
2025-04-04 15:59:09,584 - INFO - Epoch 1, Batch 14: Training loss = 0.8700
2025-04-04 15:59:09,685 - INFO - Epoch 1, Batch 15: Training loss = 0.8600
2025-04-04 15:59:09,785 - INFO - Epoch 1, Batch 16: Training loss = 0.8500
2025-04-04 15:59:09,886 - INFO - Epoch 1, Batch 17: Training loss = 0.8400
2025-04-04 15:59:09,986 - INFO - Epoch 1, Batch 18: Training loss = 0.8300
2025-04-04 15:59:10,087 - INFO - Epoch 1, Batch 19: Training loss = 0.8200
2025-04-04 15:59:10,187 - INFO - Epoch 1, Batch 20: Training loss = 0.8100
2025-04-04 15:59:10,288 - INFO - Epoch 1, Batch 21: Training loss = 0.8000
2025-04-04 15:59:10,389 - INFO - Epoch 1, Batch 22: Training loss = 0.7900
2025-04-04 15:59:10,490 - INFO - Epoch 1, Batch 23: Training loss = 0.7800
2025-04-04 15:59:10,590 - INFO - Epoch 1, Batch 24: Training loss = 0.7700
2025-04-04 15:59:10,691 - INFO - Epoch 1, Batch 25: Training loss = 0.7600
2025-04-04 15:59:10,792 - INFO - Epoch 1, Batch 26: Training loss = 0.7500
2025-04-04 15:59:10,892 - INFO - Epoch 1, Batch 27: Training loss = 0.7400
2025-04-04 15:59:10,993 - INFO - Epoch 1, Batch 28: Training loss = 0.7300
2025-04-04 15:59:11,093 - INFO - Epoch 1, Batch 29: Training loss = 0.7200
2025-04-04 15:59:11,194 - INFO - Epoch 1, Batch 30: Training loss = 0.7100
2025-04-04 15:59:11,295 - INFO - Epoch 1, Batch 31: Training loss = 0.7000
2025-04-04 15:59:11,396 - INFO - Epoch 1, Batch 32: Training loss = 0.6900
2025-04-04 15:59:11,497 - INFO - Epoch 1, Batch 33: Training loss = 0.6800
2025-04-04 15:59:11,597 - INFO - Epoch 1, Batch 34: Training loss = 0.6700
2025-04-04 15:59:11,698 - INFO - Epoch 1, Batch 35: Training loss = 0.6600
2025-04-04 15:59:11,799 - INFO - Epoch 1, Batch 36: Training loss = 0.6500
2025-04-04 15:59:11,900 - INFO - Epoch 1, Batch 37: Training loss = 0.6400
2025-04-04 15:59:12,001 - INFO - Epoch 1, Batch 38: Training loss = 0.6300
2025-04-04 15:59:12,101 - INFO - Epoch 1, Batch 39: Training loss = 0.6200
2025-04-04 15:59:12,202 - INFO - Epoch 1, Batch 40: Training loss = 0.6100
2025-04-04 15:59:12,303 - INFO - Epoch 1, Batch 41: Training loss = 0.6000
2025-04-04 15:59:12,403 - INFO - Epoch 1, Batch 42: Training loss = 0.5900
2025-04-04 15:59:12,504 - INFO - Epoch 1, Batch 43: Training loss = 0.5800
2025-04-04 15:59:12,604 - INFO - Epoch 1, Batch 44: Training loss = 0.5700
2025-04-04 15:59:12,705 - INFO - Epoch 1, Batch 45: Training loss = 0.5600
2025-04-04 15:59:12,806 - INFO - Epoch 1, Batch 46: Training loss = 0.5500
2025-04-04 15:59:12,907 - INFO - Epoch 1, Batch 47: Training loss = 0.5400
2025-04-04 15:59:13,007 - INFO - Epoch 1, Batch 48: Training loss = 0.5300
2025-04-04 15:59:13,107 - INFO - Epoch 1, Batch 49: Training loss = 0.5200
2025-04-04 15:59:13,208 - INFO - Epoch 1, Batch 50: Training loss = 0.5100
2025-04-04 15:59:13,309 - INFO - Epoch 1, Batch 51: Training loss = 0.5000
2025-04-04 15:59:13,410 - INFO - Epoch 1, Batch 52: Training loss = 0.4900
2025-04-04 15:59:13,511 - INFO - Epoch 1, Batch 53: Training loss = 0.4800
2025-04-04 15:59:13,612 - INFO - Epoch 1, Batch 54: Training loss = 0.4700
2025-04-04 15:59:13,712 - INFO - Epoch 1, Batch 55: Training loss = 0.4600
2025-04-04 15:59:13,813 - INFO - Epoch 1, Batch 56: Training loss = 0.4500
2025-04-04 15:59:13,914 - INFO - Epoch 1, Batch 57: Training loss = 0.4400
2025-04-04 15:59:14,015 - INFO - Epoch 1, Batch 58: Training loss = 0.4300
2025-04-04 15:59:14,115 - INFO - Epoch 1, Batch 59: Training loss = 0.4200
2025-04-04 15:59:14,215 - INFO - Epoch 1, Batch 60: Training loss = 0.4100
2025-04-04 15:59:14,316 - INFO - Epoch 1, Batch 61: Training loss = 0.4000
2025-04-04 15:59:14,417 - INFO - Epoch 1, Batch 62: Training loss = 0.3900
2025-04-04 15:59:14,518 - INFO - Epoch 1, Batch 63: Training loss = 0.3800
2025-04-04 15:59:14,618 - INFO - Epoch 1, Batch 64: Training loss = 0.3700
2025-04-04 15:59:14,719 - INFO - Epoch 1, Batch 65: Training loss = 0.3600
2025-04-04 15:59:14,819 - INFO - Epoch 1, Batch 66: Training loss = 0.3500
2025-04-04 15:59:14,920 - INFO - Epoch 1, Batch 67: Training loss = 0.3400
2025-04-04 15:59:15,020 - INFO - Epoch 1, Batch 68: Training loss = 0.3300
2025-04-04 15:59:15,121 - INFO - Epoch 1, Batch 69: Training loss = 0.3200
2025-04-04 15:59:15,221 - INFO - Epoch 1, Batch 70: Training loss = 0.3100
2025-04-04 15:59:15,322 - INFO - Epoch 1, Batch 71: Training loss = 0.3000
2025-04-04 15:59:15,424 - INFO - Epoch 1, Batch 72: Training loss = 0.2900
2025-04-04 15:59:15,525 - INFO - Epoch 1, Batch 73: Training loss = 0.2800
2025-04-04 15:59:15,626 - INFO - Epoch 1, Batch 74: Training loss = 0.2700
2025-04-04 15:59:15,726 - INFO - Epoch 1, Batch 75: Training loss = 0.2600
2025-04-04 15:59:15,827 - INFO - Epoch 1, Batch 76: Training loss = 0.2500
2025-04-04 15:59:15,928 - INFO - Epoch 1, Batch 77: Training loss = 0.2400
2025-04-04 15:59:16,029 - INFO - Epoch 1, Batch 78: Training loss = 0.2300
2025-04-04 15:59:16,130 - INFO - Epoch 1, Batch 79: Training loss = 0.2200
2025-04-04 15:59:16,231 - INFO - Epoch 1, Batch 80: Training loss = 0.2100
2025-04-04 15:59:16,332 - INFO - Epoch 1, Batch 81: Training loss = 0.2000
2025-04-04 15:59:16,433 - INFO - Epoch 1, Batch 82: Training loss = 0.1900
2025-04-04 15:59:16,534 - INFO - Epoch 1, Batch 83: Training loss = 0.1800
2025-04-04 15:59:16,635 - INFO - Epoch 1, Batch 84: Training loss = 0.1700
2025-04-04 15:59:16,735 - INFO - Epoch 1, Batch 85: Training loss = 0.1600
2025-04-04 15:59:16,836 - INFO - Epoch 1, Batch 86: Training loss = 0.1500
2025-04-04 15:59:16,937 - INFO - Epoch 1, Batch 87: Training loss = 0.1400
2025-04-04 15:59:17,038 - INFO - Epoch 1, Batch 88: Training loss = 0.1300
2025-04-04 15:59:17,138 - INFO - Epoch 1, Batch 89: Training loss = 0.1200
2025-04-04 15:59:17,239 - INFO - Epoch 1, Batch 90: Training loss = 0.1100
2025-04-04 15:59:17,340 - INFO - Epoch 1, Batch 91: Training loss = 0.1000
2025-04-04 15:59:17,440 - INFO - Epoch 1, Batch 92: Training loss = 0.0900
2025-04-04 15:59:17,541 - INFO - Epoch 1, Batch 93: Training loss = 0.0800
2025-04-04 15:59:17,642 - INFO - Epoch 1, Batch 94: Training loss = 0.0700
2025-04-04 15:59:17,742 - INFO - Epoch 1, Batch 95: Training loss = 0.0600
2025-04-04 15:59:17,843 - INFO - Epoch 1, Batch 96: Training loss = 0.0500
2025-04-04 15:59:17,943 - INFO - Epoch 1, Batch 97: Training loss = 0.0400
2025-04-04 15:59:18,043 - INFO - Epoch 1, Batch 98: Training loss = 0.0300
2025-04-04 15:59:18,144 - INFO - Epoch 1, Batch 99: Training loss = 0.0200
2025-04-04 15:59:18,245 - INFO - Epoch 1, Batch 100: Training loss = 0.0100
2025-04-04 15:59:18,346 - INFO - Epoch 1: Validation loss = 0.1000, Validation accuracy = 0.8000
2025-04-04 15:59:18,346 - INFO - Epoch 1: MSE = 0.1000, RMSE = 0.3162, MAE = 0.0800, RÂ² = 0.9000
2025-04-04 15:59:18,346 - INFO - Epoch 2 started.
2025-04-04 15:59:18,347 - INFO - Epoch 2, Batch 1: Training loss = 1.0000
2025-04-04 15:59:18,447 - INFO - Epoch 2, Batch 2: Training loss = 0.9900
2025-04-04 15:59:18,548 - INFO - Epoch 2, Batch 3: Training loss = 0.9800
2025-04-04 15:59:18,649 - INFO - Epoch 2, Batch 4: Training loss = 0.9700
2025-04-04 15:59:18,750 - INFO - Epoch 2, Batch 5: Training loss = 0.9600
2025-04-04 15:59:18,850 - INFO - Epoch 2, Batch 6: Training loss = 0.9500
2025-04-04 15:59:18,951 - INFO - Epoch 2, Batch 7: Training loss = 0.9400
2025-04-04 15:59:19,051 - INFO - Epoch 2, Batch 8: Training loss = 0.9300
2025-04-04 15:59:19,152 - INFO - Epoch 2, Batch 9: Training loss = 0.9200
2025-04-04 15:59:19,253 - INFO - Epoch 2, Batch 10: Training loss = 0.9100
2025-04-04 15:59:19,354 - INFO - Epoch 2, Batch 11: Training loss = 0.9000
2025-04-04 15:59:19,455 - INFO - Epoch 2, Batch 12: Training loss = 0.8900
2025-04-04 15:59:19,555 - INFO - Epoch 2, Batch 13: Training loss = 0.8800
2025-04-04 15:59:19,656 - INFO - Epoch 2, Batch 14: Training loss = 0.8700
2025-04-04 15:59:19,757 - INFO - Epoch 2, Batch 15: Training loss = 0.8600
2025-04-04 15:59:19,858 - INFO - Epoch 2, Batch 16: Training loss = 0.8500
2025-04-04 15:59:19,958 - INFO - Epoch 2, Batch 17: Training loss = 0.8400
2025-04-04 15:59:20,059 - INFO - Epoch 2, Batch 18: Training loss = 0.8300
2025-04-04 15:59:20,160 - INFO - Epoch 2, Batch 19: Training loss = 0.8200
2025-04-04 15:59:20,260 - INFO - Epoch 2, Batch 20: Training loss = 0.8100
2025-04-04 15:59:20,361 - INFO - Epoch 2, Batch 21: Training loss = 0.8000
2025-04-04 15:59:20,462 - INFO - Epoch 2, Batch 22: Training loss = 0.7900
2025-04-04 15:59:20,563 - INFO - Epoch 2, Batch 23: Training loss = 0.7800
2025-04-04 15:59:20,664 - INFO - Epoch 2, Batch 24: Training loss = 0.7700
2025-04-04 15:59:20,764 - INFO - Epoch 2, Batch 25: Training loss = 0.7600
2025-04-04 15:59:20,865 - INFO - Epoch 2, Batch 26: Training loss = 0.7500
2025-04-04 15:59:20,965 - INFO - Epoch 2, Batch 27: Training loss = 0.7400
2025-04-04 15:59:21,066 - INFO - Epoch 2, Batch 28: Training loss = 0.7300
2025-04-04 15:59:21,166 - INFO - Epoch 2, Batch 29: Training loss = 0.7200
2025-04-04 15:59:21,267 - INFO - Epoch 2, Batch 30: Training loss = 0.7100
2025-04-04 15:59:21,368 - INFO - Epoch 2, Batch 31: Training loss = 0.7000
2025-04-04 15:59:21,469 - INFO - Epoch 2, Batch 32: Training loss = 0.6900
2025-04-04 15:59:21,570 - INFO - Epoch 2, Batch 33: Training loss = 0.6800
2025-04-04 15:59:21,671 - INFO - Epoch 2, Batch 34: Training loss = 0.6700
2025-04-04 15:59:21,772 - INFO - Epoch 2, Batch 35: Training loss = 0.6600
2025-04-04 15:59:21,872 - INFO - Epoch 2, Batch 36: Training loss = 0.6500
2025-04-04 15:59:21,973 - INFO - Epoch 2, Batch 37: Training loss = 0.6400
2025-04-04 15:59:22,074 - INFO - Epoch 2, Batch 38: Training loss = 0.6300
2025-04-04 15:59:22,174 - INFO - Epoch 2, Batch 39: Training loss = 0.6200
2025-04-04 15:59:22,275 - INFO - Epoch 2, Batch 40: Training loss = 0.6100
2025-04-04 15:59:22,376 - INFO - Epoch 2, Batch 41: Training loss = 0.6000
2025-04-04 15:59:22,477 - INFO - Epoch 2, Batch 42: Training loss = 0.5900
2025-04-04 15:59:22,578 - INFO - Epoch 2, Batch 43: Training loss = 0.5800
2025-04-04 15:59:22,678 - INFO - Epoch 2, Batch 44: Training loss = 0.5700
2025-04-04 15:59:22,779 - INFO - Epoch 2, Batch 45: Training loss = 0.5600
2025-04-04 15:59:22,880 - INFO - Epoch 2, Batch 46: Training loss = 0.5500
2025-04-04 15:59:22,981 - INFO - Epoch 2, Batch 47: Training loss = 0.5400
2025-04-04 15:59:23,081 - INFO - Epoch 2, Batch 48: Training loss = 0.5300
2025-04-04 15:59:23,182 - INFO - Epoch 2, Batch 49: Training loss = 0.5200
2025-04-04 15:59:23,282 - INFO - Epoch 2, Batch 50: Training loss = 0.5100
2025-04-04 15:59:23,383 - INFO - Epoch 2, Batch 51: Training loss = 0.5000
2025-04-04 15:59:23,483 - INFO - Epoch 2, Batch 52: Training loss = 0.4900
2025-04-04 15:59:23,584 - INFO - Epoch 2, Batch 53: Training loss = 0.4800
2025-04-04 15:59:23,684 - INFO - Epoch 2, Batch 54: Training loss = 0.4700
2025-04-04 15:59:23,785 - INFO - Epoch 2, Batch 55: Training loss = 0.4600
2025-04-04 15:59:23,885 - INFO - Epoch 2, Batch 56: Training loss = 0.4500
2025-04-04 15:59:23,986 - INFO - Epoch 2, Batch 57: Training loss = 0.4400
2025-04-04 15:59:24,087 - INFO - Epoch 2, Batch 58: Training loss = 0.4300
2025-04-04 15:59:24,188 - INFO - Epoch 2, Batch 59: Training loss = 0.4200
2025-04-04 15:59:24,289 - INFO - Epoch 2, Batch 60: Training loss = 0.4100
2025-04-04 15:59:24,390 - INFO - Epoch 2, Batch 61: Training loss = 0.4000
2025-04-04 15:59:24,491 - INFO - Epoch 2, Batch 62: Training loss = 0.3900
2025-04-04 15:59:24,591 - INFO - Epoch 2, Batch 63: Training loss = 0.3800
2025-04-04 15:59:24,692 - INFO - Epoch 2, Batch 64: Training loss = 0.3700
2025-04-04 15:59:24,793 - INFO - Epoch 2, Batch 65: Training loss = 0.3600
2025-04-04 15:59:24,893 - INFO - Epoch 2, Batch 66: Training loss = 0.3500
2025-04-04 15:59:24,994 - INFO - Epoch 2, Batch 67: Training loss = 0.3400
2025-04-04 15:59:25,094 - INFO - Epoch 2, Batch 68: Training loss = 0.3300
2025-04-04 15:59:25,195 - INFO - Epoch 2, Batch 69: Training loss = 0.3200
2025-04-04 15:59:25,296 - INFO - Epoch 2, Batch 70: Training loss = 0.3100
2025-04-04 15:59:25,397 - INFO - Epoch 2, Batch 71: Training loss = 0.3000
2025-04-04 15:59:25,497 - INFO - Epoch 2, Batch 72: Training loss = 0.2900
2025-04-04 15:59:25,598 - INFO - Epoch 2, Batch 73: Training loss = 0.2800
2025-04-04 15:59:25,699 - INFO - Epoch 2, Batch 74: Training loss = 0.2700
2025-04-04 15:59:25,800 - INFO - Epoch 2, Batch 75: Training loss = 0.2600
2025-04-04 15:59:25,900 - INFO - Epoch 2, Batch 76: Training loss = 0.2500
2025-04-04 15:59:26,001 - INFO - Epoch 2, Batch 77: Training loss = 0.2400
2025-04-04 15:59:26,102 - INFO - Epoch 2, Batch 78: Training loss = 0.2300
2025-04-04 15:59:26,203 - INFO - Epoch 2, Batch 79: Training loss = 0.2200
2025-04-04 15:59:26,304 - INFO - Epoch 2, Batch 80: Training loss = 0.2100
2025-04-04 15:59:26,404 - INFO - Epoch 2, Batch 81: Training loss = 0.2000
2025-04-04 15:59:26,505 - INFO - Epoch 2, Batch 82: Training loss = 0.1900
2025-04-04 15:59:26,606 - INFO - Epoch 2, Batch 83: Training loss = 0.1800
2025-04-04 15:59:26,707 - INFO - Epoch 2, Batch 84: Training loss = 0.1700
2025-04-04 15:59:26,808 - INFO - Epoch 2, Batch 85: Training loss = 0.1600
2025-04-04 15:59:26,908 - INFO - Epoch 2, Batch 86: Training loss = 0.1500
2025-04-04 15:59:27,009 - INFO - Epoch 2, Batch 87: Training loss = 0.1400
2025-04-04 15:59:27,110 - INFO - Epoch 2, Batch 88: Training loss = 0.1300
2025-04-04 15:59:27,211 - INFO - Epoch 2, Batch 89: Training loss = 0.1200
2025-04-04 15:59:27,311 - INFO - Epoch 2, Batch 90: Training loss = 0.1100
2025-04-04 15:59:27,412 - INFO - Epoch 2, Batch 91: Training loss = 0.1000
2025-04-04 15:59:27,512 - INFO - Epoch 2, Batch 92: Training loss = 0.0900
2025-04-04 15:59:27,613 - INFO - Epoch 2, Batch 93: Training loss = 0.0800
2025-04-04 15:59:27,713 - INFO - Epoch 2, Batch 94: Training loss = 0.0700
2025-04-04 15:59:27,814 - INFO - Epoch 2, Batch 95: Training loss = 0.0600
2025-04-04 15:59:27,915 - INFO - Epoch 2, Batch 96: Training loss = 0.0500
2025-04-04 15:59:28,015 - INFO - Epoch 2, Batch 97: Training loss = 0.0400
2025-04-04 15:59:28,116 - INFO - Epoch 2, Batch 98: Training loss = 0.0300
2025-04-04 15:59:28,217 - INFO - Epoch 2, Batch 99: Training loss = 0.0200
2025-04-04 15:59:28,318 - INFO - Epoch 2, Batch 100: Training loss = 0.0100
2025-04-04 15:59:28,418 - INFO - Epoch 2: Validation loss = 0.0900, Validation accuracy = 0.8200
2025-04-04 15:59:28,419 - INFO - Epoch 2: MSE = 0.0900, RMSE = 0.3000, MAE = 0.0720, RÂ² = 0.9100
2025-04-04 15:59:28,419 - INFO - Epoch 3 started.
2025-04-04 15:59:28,419 - INFO - Epoch 3, Batch 1: Training loss = 1.0000
2025-04-04 15:59:28,520 - INFO - Epoch 3, Batch 2: Training loss = 0.9900
2025-04-04 15:59:28,621 - INFO - Epoch 3, Batch 3: Training loss = 0.9800
2025-04-04 15:59:28,721 - INFO - Epoch 3, Batch 4: Training loss = 0.9700
2025-04-04 15:59:28,822 - INFO - Epoch 3, Batch 5: Training loss = 0.9600
2025-04-04 15:59:28,923 - INFO - Epoch 3, Batch 6: Training loss = 0.9500
2025-04-04 15:59:29,023 - INFO - Epoch 3, Batch 7: Training loss = 0.9400
2025-04-04 15:59:29,124 - INFO - Epoch 3, Batch 8: Training loss = 0.9300
2025-04-04 15:59:29,224 - INFO - Epoch 3, Batch 9: Training loss = 0.9200
2025-04-04 15:59:29,325 - INFO - Epoch 3, Batch 10: Training loss = 0.9100
2025-04-04 15:59:29,426 - INFO - Epoch 3, Batch 11: Training loss = 0.9000
2025-04-04 15:59:29,527 - INFO - Epoch 3, Batch 12: Training loss = 0.8900
2025-04-04 15:59:29,628 - INFO - Epoch 3, Batch 13: Training loss = 0.8800
2025-04-04 15:59:29,728 - INFO - Epoch 3, Batch 14: Training loss = 0.8700
2025-04-04 15:59:29,829 - INFO - Epoch 3, Batch 15: Training loss = 0.8600
2025-04-04 15:59:29,929 - INFO - Epoch 3, Batch 16: Training loss = 0.8500
2025-04-04 15:59:30,030 - INFO - Epoch 3, Batch 17: Training loss = 0.8400
2025-04-04 15:59:30,130 - INFO - Epoch 3, Batch 18: Training loss = 0.8300
2025-04-04 15:59:30,231 - INFO - Epoch 3, Batch 19: Training loss = 0.8200
2025-04-04 15:59:30,331 - INFO - Epoch 3, Batch 20: Training loss = 0.8100
2025-04-04 15:59:30,432 - INFO - Epoch 3, Batch 21: Training loss = 0.8000
2025-04-04 15:59:30,533 - INFO - Epoch 3, Batch 22: Training loss = 0.7900
2025-04-04 15:59:30,633 - INFO - Epoch 3, Batch 23: Training loss = 0.7800
2025-04-04 15:59:30,734 - INFO - Epoch 3, Batch 24: Training loss = 0.7700
2025-04-04 15:59:30,835 - INFO - Epoch 3, Batch 25: Training loss = 0.7600
2025-04-04 15:59:30,936 - INFO - Epoch 3, Batch 26: Training loss = 0.7500
2025-04-04 15:59:31,037 - INFO - Epoch 3, Batch 27: Training loss = 0.7400
2025-04-04 15:59:31,137 - INFO - Epoch 3, Batch 28: Training loss = 0.7300
2025-04-04 15:59:31,238 - INFO - Epoch 3, Batch 29: Training loss = 0.7200
2025-04-04 15:59:31,339 - INFO - Epoch 3, Batch 30: Training loss = 0.7100
2025-04-04 15:59:31,439 - INFO - Epoch 3, Batch 31: Training loss = 0.7000
2025-04-04 15:59:31,540 - INFO - Epoch 3, Batch 32: Training loss = 0.6900
2025-04-04 15:59:31,641 - INFO - Epoch 3, Batch 33: Training loss = 0.6800
2025-04-04 15:59:31,742 - INFO - Epoch 3, Batch 34: Training loss = 0.6700
2025-04-04 15:59:31,842 - INFO - Epoch 3, Batch 35: Training loss = 0.6600
2025-04-04 15:59:31,942 - INFO - Epoch 3, Batch 36: Training loss = 0.6500
2025-04-04 15:59:32,043 - INFO - Epoch 3, Batch 37: Training loss = 0.6400
2025-04-04 15:59:32,144 - INFO - Epoch 3, Batch 38: Training loss = 0.6300
2025-04-04 15:59:32,244 - INFO - Epoch 3, Batch 39: Training loss = 0.6200
2025-04-04 15:59:32,345 - INFO - Epoch 3, Batch 40: Training loss = 0.6100
2025-04-04 15:59:32,446 - INFO - Epoch 3, Batch 41: Training loss = 0.6000
2025-04-04 15:59:32,546 - INFO - Epoch 3, Batch 42: Training loss = 0.5900
2025-04-04 15:59:32,647 - INFO - Epoch 3, Batch 43: Training loss = 0.5800
2025-04-04 15:59:32,747 - INFO - Epoch 3, Batch 44: Training loss = 0.5700
2025-04-04 15:59:32,848 - INFO - Epoch 3, Batch 45: Training loss = 0.5600
2025-04-04 15:59:32,948 - INFO - Epoch 3, Batch 46: Training loss = 0.5500
2025-04-04 15:59:33,049 - INFO - Epoch 3, Batch 47: Training loss = 0.5400
2025-04-04 15:59:33,150 - INFO - Epoch 3, Batch 48: Training loss = 0.5300
2025-04-04 15:59:33,251 - INFO - Epoch 3, Batch 49: Training loss = 0.5200
2025-04-04 15:59:33,351 - INFO - Epoch 3, Batch 50: Training loss = 0.5100
2025-04-04 15:59:33,452 - INFO - Epoch 3, Batch 51: Training loss = 0.5000
2025-04-04 15:59:33,553 - INFO - Epoch 3, Batch 52: Training loss = 0.4900
2025-04-04 15:59:33,653 - INFO - Epoch 3, Batch 53: Training loss = 0.4800
2025-04-04 15:59:33,754 - INFO - Epoch 3, Batch 54: Training loss = 0.4700
2025-04-04 15:59:33,854 - INFO - Epoch 3, Batch 55: Training loss = 0.4600
2025-04-04 15:59:33,955 - INFO - Epoch 3, Batch 56: Training loss = 0.4500
2025-04-04 15:59:34,056 - INFO - Epoch 3, Batch 57: Training loss = 0.4400
2025-04-04 15:59:34,156 - INFO - Epoch 3, Batch 58: Training loss = 0.4300
2025-04-04 15:59:34,257 - INFO - Epoch 3, Batch 59: Training loss = 0.4200
2025-04-04 15:59:34,357 - INFO - Epoch 3, Batch 60: Training loss = 0.4100
2025-04-04 15:59:34,458 - INFO - Epoch 3, Batch 61: Training loss = 0.4000
2025-04-04 15:59:34,559 - INFO - Epoch 3, Batch 62: Training loss = 0.3900
2025-04-04 15:59:34,660 - INFO - Epoch 3, Batch 63: Training loss = 0.3800
2025-04-04 15:59:34,761 - INFO - Epoch 3, Batch 64: Training loss = 0.3700
2025-04-04 15:59:34,861 - INFO - Epoch 3, Batch 65: Training loss = 0.3600
2025-04-04 15:59:34,962 - INFO - Epoch 3, Batch 66: Training loss = 0.3500
2025-04-04 15:59:35,063 - INFO - Epoch 3, Batch 67: Training loss = 0.3400
2025-04-04 15:59:35,164 - INFO - Epoch 3, Batch 68: Training loss = 0.3300
2025-04-04 15:59:35,265 - INFO - Epoch 3, Batch 69: Training loss = 0.3200
2025-04-04 15:59:35,365 - INFO - Epoch 3, Batch 70: Training loss = 0.3100
2025-04-04 15:59:35,466 - INFO - Epoch 3, Batch 71: Training loss = 0.3000
2025-04-04 15:59:35,566 - INFO - Epoch 3, Batch 72: Training loss = 0.2900
2025-04-04 15:59:35,667 - INFO - Epoch 3, Batch 73: Training loss = 0.2800
2025-04-04 15:59:35,768 - INFO - Epoch 3, Batch 74: Training loss = 0.2700
2025-04-04 15:59:35,868 - INFO - Epoch 3, Batch 75: Training loss = 0.2600
2025-04-04 15:59:35,969 - INFO - Epoch 3, Batch 76: Training loss = 0.2500
2025-04-04 15:59:36,069 - INFO - Epoch 3, Batch 77: Training loss = 0.2400
2025-04-04 15:59:36,170 - INFO - Epoch 3, Batch 78: Training loss = 0.2300
2025-04-04 15:59:36,271 - INFO - Epoch 3, Batch 79: Training loss = 0.2200
2025-04-04 15:59:36,371 - INFO - Epoch 3, Batch 80: Training loss = 0.2100
2025-04-04 15:59:36,472 - INFO - Epoch 3, Batch 81: Training loss = 0.2000
2025-04-04 15:59:36,573 - INFO - Epoch 3, Batch 82: Training loss = 0.1900
2025-04-04 15:59:36,674 - INFO - Epoch 3, Batch 83: Training loss = 0.1800
2025-04-04 15:59:36,774 - INFO - Epoch 3, Batch 84: Training loss = 0.1700
2025-04-04 15:59:36,875 - INFO - Epoch 3, Batch 85: Training loss = 0.1600
2025-04-04 15:59:36,976 - INFO - Epoch 3, Batch 86: Training loss = 0.1500
2025-04-04 15:59:37,077 - INFO - Epoch 3, Batch 87: Training loss = 0.1400
2025-04-04 15:59:37,178 - INFO - Epoch 3, Batch 88: Training loss = 0.1300
2025-04-04 15:59:37,279 - INFO - Epoch 3, Batch 89: Training loss = 0.1200
2025-04-04 15:59:37,379 - INFO - Epoch 3, Batch 90: Training loss = 0.1100
2025-04-04 15:59:37,480 - INFO - Epoch 3, Batch 91: Training loss = 0.1000
2025-04-04 15:59:37,581 - INFO - Epoch 3, Batch 92: Training loss = 0.0900
2025-04-04 15:59:37,682 - INFO - Epoch 3, Batch 93: Training loss = 0.0800
2025-04-04 15:59:37,783 - INFO - Epoch 3, Batch 94: Training loss = 0.0700
2025-04-04 15:59:37,884 - INFO - Epoch 3, Batch 95: Training loss = 0.0600
2025-04-04 15:59:37,984 - INFO - Epoch 3, Batch 96: Training loss = 0.0500
2025-04-04 15:59:38,085 - INFO - Epoch 3, Batch 97: Training loss = 0.0400
2025-04-04 15:59:38,186 - INFO - Epoch 3, Batch 98: Training loss = 0.0300
2025-04-04 15:59:38,287 - INFO - Epoch 3, Batch 99: Training loss = 0.0200
2025-04-04 15:59:38,387 - INFO - Epoch 3, Batch 100: Training loss = 0.0100
2025-04-04 15:59:38,488 - INFO - Epoch 3: Validation loss = 0.0800, Validation accuracy = 0.8400
2025-04-04 15:59:38,488 - INFO - Epoch 3: MSE = 0.0800, RMSE = 0.2828, MAE = 0.0640, RÂ² = 0.9200
2025-04-04 15:59:38,488 - INFO - Epoch 4 started.
2025-04-04 15:59:38,488 - INFO - Epoch 4, Batch 1: Training loss = 1.0000
2025-04-04 15:59:38,589 - INFO - Epoch 4, Batch 2: Training loss = 0.9900
2025-04-04 15:59:38,689 - INFO - Epoch 4, Batch 3: Training loss = 0.9800
2025-04-04 15:59:38,790 - INFO - Epoch 4, Batch 4: Training loss = 0.9700
2025-04-04 15:59:38,890 - INFO - Epoch 4, Batch 5: Training loss = 0.9600
2025-04-04 15:59:38,991 - INFO - Epoch 4, Batch 6: Training loss = 0.9500
2025-04-04 15:59:39,092 - INFO - Epoch 4, Batch 7: Training loss = 0.9400
2025-04-04 15:59:39,193 - INFO - Epoch 4, Batch 8: Training loss = 0.9300
2025-04-04 15:59:39,294 - INFO - Epoch 4, Batch 9: Training loss = 0.9200
2025-04-04 15:59:39,395 - INFO - Epoch 4, Batch 10: Training loss = 0.9100
2025-04-04 15:59:39,495 - INFO - Epoch 4, Batch 11: Training loss = 0.9000
2025-04-04 15:59:39,596 - INFO - Epoch 4, Batch 12: Training loss = 0.8900
2025-04-04 15:59:39,696 - INFO - Epoch 4, Batch 13: Training loss = 0.8800
2025-04-04 15:59:39,797 - INFO - Epoch 4, Batch 14: Training loss = 0.8700
2025-04-04 15:59:39,898 - INFO - Epoch 4, Batch 15: Training loss = 0.8600
2025-04-04 15:59:39,998 - INFO - Epoch 4, Batch 16: Training loss = 0.8500
2025-04-04 15:59:40,099 - INFO - Epoch 4, Batch 17: Training loss = 0.8400
2025-04-04 15:59:40,200 - INFO - Epoch 4, Batch 18: Training loss = 0.8300
2025-04-04 15:59:40,300 - INFO - Epoch 4, Batch 19: Training loss = 0.8200
2025-04-04 15:59:40,401 - INFO - Epoch 4, Batch 20: Training loss = 0.8100
2025-04-04 15:59:40,501 - INFO - Epoch 4, Batch 21: Training loss = 0.8000
2025-04-04 15:59:40,602 - INFO - Epoch 4, Batch 22: Training loss = 0.7900
2025-04-04 15:59:40,703 - INFO - Epoch 4, Batch 23: Training loss = 0.7800
2025-04-04 15:59:40,804 - INFO - Epoch 4, Batch 24: Training loss = 0.7700
2025-04-04 15:59:40,904 - INFO - Epoch 4, Batch 25: Training loss = 0.7600
2025-04-04 15:59:41,005 - INFO - Epoch 4, Batch 26: Training loss = 0.7500
2025-04-04 15:59:41,106 - INFO - Epoch 4, Batch 27: Training loss = 0.7400
2025-04-04 15:59:41,207 - INFO - Epoch 4, Batch 28: Training loss = 0.7300
2025-04-04 15:59:41,308 - INFO - Epoch 4, Batch 29: Training loss = 0.7200
2025-04-04 15:59:41,409 - INFO - Epoch 4, Batch 30: Training loss = 0.7100
2025-04-04 15:59:41,509 - INFO - Epoch 4, Batch 31: Training loss = 0.7000
2025-04-04 15:59:41,610 - INFO - Epoch 4, Batch 32: Training loss = 0.6900
2025-04-04 15:59:41,711 - INFO - Epoch 4, Batch 33: Training loss = 0.6800
2025-04-04 15:59:41,812 - INFO - Epoch 4, Batch 34: Training loss = 0.6700
2025-04-04 15:59:41,913 - INFO - Epoch 4, Batch 35: Training loss = 0.6600
2025-04-04 15:59:42,014 - INFO - Epoch 4, Batch 36: Training loss = 0.6500
2025-04-04 15:59:42,115 - INFO - Epoch 4, Batch 37: Training loss = 0.6400
2025-04-04 15:59:42,216 - INFO - Epoch 4, Batch 38: Training loss = 0.6300
2025-04-04 15:59:42,317 - INFO - Epoch 4, Batch 39: Training loss = 0.6200
2025-04-04 15:59:42,418 - INFO - Epoch 4, Batch 40: Training loss = 0.6100
2025-04-04 15:59:42,518 - INFO - Epoch 4, Batch 41: Training loss = 0.6000
2025-04-04 15:59:42,619 - INFO - Epoch 4, Batch 42: Training loss = 0.5900
2025-04-04 15:59:42,720 - INFO - Epoch 4, Batch 43: Training loss = 0.5800
2025-04-04 15:59:42,821 - INFO - Epoch 4, Batch 44: Training loss = 0.5700
2025-04-04 15:59:42,921 - INFO - Epoch 4, Batch 45: Training loss = 0.5600
2025-04-04 15:59:43,022 - INFO - Epoch 4, Batch 46: Training loss = 0.5500
2025-04-04 15:59:43,122 - INFO - Epoch 4, Batch 47: Training loss = 0.5400
2025-04-04 15:59:43,223 - INFO - Epoch 4, Batch 48: Training loss = 0.5300
2025-04-04 15:59:43,323 - INFO - Epoch 4, Batch 49: Training loss = 0.5200
2025-04-04 15:59:43,424 - INFO - Epoch 4, Batch 50: Training loss = 0.5100
2025-04-04 15:59:43,524 - INFO - Epoch 4, Batch 51: Training loss = 0.5000
2025-04-04 15:59:43,625 - INFO - Epoch 4, Batch 52: Training loss = 0.4900
2025-04-04 15:59:43,725 - INFO - Epoch 4, Batch 53: Training loss = 0.4800
2025-04-04 15:59:43,826 - INFO - Epoch 4, Batch 54: Training loss = 0.4700
2025-04-04 15:59:43,926 - INFO - Epoch 4, Batch 55: Training loss = 0.4600
2025-04-04 15:59:44,027 - INFO - Epoch 4, Batch 56: Training loss = 0.4500
2025-04-04 15:59:44,128 - INFO - Epoch 4, Batch 57: Training loss = 0.4400
2025-04-04 15:59:44,228 - INFO - Epoch 4, Batch 58: Training loss = 0.4300
2025-04-04 15:59:44,329 - INFO - Epoch 4, Batch 59: Training loss = 0.4200
2025-04-04 15:59:44,430 - INFO - Epoch 4, Batch 60: Training loss = 0.4100
2025-04-04 15:59:44,530 - INFO - Epoch 4, Batch 61: Training loss = 0.4000
2025-04-04 15:59:44,631 - INFO - Epoch 4, Batch 62: Training loss = 0.3900
2025-04-04 15:59:44,732 - INFO - Epoch 4, Batch 63: Training loss = 0.3800
2025-04-04 15:59:44,833 - INFO - Epoch 4, Batch 64: Training loss = 0.3700
2025-04-04 15:59:44,933 - INFO - Epoch 4, Batch 65: Training loss = 0.3600
2025-04-04 15:59:45,034 - INFO - Epoch 4, Batch 66: Training loss = 0.3500
2025-04-04 15:59:45,134 - INFO - Epoch 4, Batch 67: Training loss = 0.3400
2025-04-04 15:59:45,235 - INFO - Epoch 4, Batch 68: Training loss = 0.3300
2025-04-04 15:59:45,336 - INFO - Epoch 4, Batch 69: Training loss = 0.3200
2025-04-04 15:59:45,437 - INFO - Epoch 4, Batch 70: Training loss = 0.3100
2025-04-04 15:59:45,537 - INFO - Epoch 4, Batch 71: Training loss = 0.3000
2025-04-04 15:59:45,638 - INFO - Epoch 4, Batch 72: Training loss = 0.2900
2025-04-04 15:59:45,738 - INFO - Epoch 4, Batch 73: Training loss = 0.2800
2025-04-04 15:59:45,839 - INFO - Epoch 4, Batch 74: Training loss = 0.2700
2025-04-04 15:59:45,940 - INFO - Epoch 4, Batch 75: Training loss = 0.2600
2025-04-04 15:59:46,041 - INFO - Epoch 4, Batch 76: Training loss = 0.2500
2025-04-04 15:59:46,142 - INFO - Epoch 4, Batch 77: Training loss = 0.2400
2025-04-04 15:59:46,243 - INFO - Epoch 4, Batch 78: Training loss = 0.2300
2025-04-04 15:59:46,343 - INFO - Epoch 4, Batch 79: Training loss = 0.2200
2025-04-04 15:59:46,444 - INFO - Epoch 4, Batch 80: Training loss = 0.2100
2025-04-04 15:59:46,545 - INFO - Epoch 4, Batch 81: Training loss = 0.2000
2025-04-04 15:59:46,646 - INFO - Epoch 4, Batch 82: Training loss = 0.1900
2025-04-04 15:59:46,746 - INFO - Epoch 4, Batch 83: Training loss = 0.1800
2025-04-04 15:59:46,847 - INFO - Epoch 4, Batch 84: Training loss = 0.1700
2025-04-04 15:59:46,947 - INFO - Epoch 4, Batch 85: Training loss = 0.1600
2025-04-04 15:59:47,048 - INFO - Epoch 4, Batch 86: Training loss = 0.1500
2025-04-04 15:59:47,149 - INFO - Epoch 4, Batch 87: Training loss = 0.1400
2025-04-04 15:59:47,250 - INFO - Epoch 4, Batch 88: Training loss = 0.1300
2025-04-04 15:59:47,351 - INFO - Epoch 4, Batch 89: Training loss = 0.1200
2025-04-04 15:59:47,452 - INFO - Epoch 4, Batch 90: Training loss = 0.1100
2025-04-04 15:59:47,552 - INFO - Epoch 4, Batch 91: Training loss = 0.1000
2025-04-04 15:59:47,653 - INFO - Epoch 4, Batch 92: Training loss = 0.0900
2025-04-04 15:59:47,754 - INFO - Epoch 4, Batch 93: Training loss = 0.0800
2025-04-04 15:59:47,854 - INFO - Epoch 4, Batch 94: Training loss = 0.0700
2025-04-04 15:59:47,955 - INFO - Epoch 4, Batch 95: Training loss = 0.0600
2025-04-04 15:59:48,056 - INFO - Epoch 4, Batch 96: Training loss = 0.0500
2025-04-04 15:59:48,157 - INFO - Epoch 4, Batch 97: Training loss = 0.0400
2025-04-04 15:59:48,258 - INFO - Epoch 4, Batch 98: Training loss = 0.0300
2025-04-04 15:59:48,359 - INFO - Epoch 4, Batch 99: Training loss = 0.0200
2025-04-04 15:59:48,459 - INFO - Epoch 4, Batch 100: Training loss = 0.0100
2025-04-04 15:59:48,560 - INFO - Epoch 4: Validation loss = 0.0700, Validation accuracy = 0.8600
2025-04-04 15:59:48,560 - INFO - Epoch 4: MSE = 0.0700, RMSE = 0.2646, MAE = 0.0560, RÂ² = 0.9300
2025-04-04 15:59:48,560 - INFO - Epoch 5 started.
2025-04-04 15:59:48,560 - INFO - Epoch 5, Batch 1: Training loss = 1.0000
2025-04-04 15:59:48,661 - INFO - Epoch 5, Batch 2: Training loss = 0.9900
2025-04-04 15:59:48,762 - INFO - Epoch 5, Batch 3: Training loss = 0.9800
2025-04-04 15:59:48,862 - INFO - Epoch 5, Batch 4: Training loss = 0.9700
2025-04-04 15:59:48,963 - INFO - Epoch 5, Batch 5: Training loss = 0.9600
2025-04-04 15:59:49,064 - INFO - Epoch 5, Batch 6: Training loss = 0.9500
2025-04-04 15:59:49,165 - INFO - Epoch 5, Batch 7: Training loss = 0.9400
2025-04-04 15:59:49,265 - INFO - Epoch 5, Batch 8: Training loss = 0.9300
2025-04-04 15:59:49,366 - INFO - Epoch 5, Batch 9: Training loss = 0.9200
2025-04-04 15:59:49,467 - INFO - Epoch 5, Batch 10: Training loss = 0.9100
2025-04-04 15:59:49,567 - INFO - Epoch 5, Batch 11: Training loss = 0.9000
2025-04-04 15:59:49,668 - INFO - Epoch 5, Batch 12: Training loss = 0.8900
2025-04-04 15:59:49,769 - INFO - Epoch 5, Batch 13: Training loss = 0.8800
2025-04-04 15:59:49,870 - INFO - Epoch 5, Batch 14: Training loss = 0.8700
2025-04-04 15:59:49,970 - INFO - Epoch 5, Batch 15: Training loss = 0.8600
2025-04-04 15:59:50,071 - INFO - Epoch 5, Batch 16: Training loss = 0.8500
2025-04-04 15:59:50,172 - INFO - Epoch 5, Batch 17: Training loss = 0.8400
2025-04-04 15:59:50,273 - INFO - Epoch 5, Batch 18: Training loss = 0.8300
2025-04-04 15:59:50,374 - INFO - Epoch 5, Batch 19: Training loss = 0.8200
2025-04-04 15:59:50,475 - INFO - Epoch 5, Batch 20: Training loss = 0.8100
2025-04-04 15:59:50,575 - INFO - Epoch 5, Batch 21: Training loss = 0.8000
2025-04-04 15:59:50,676 - INFO - Epoch 5, Batch 22: Training loss = 0.7900
2025-04-04 15:59:50,777 - INFO - Epoch 5, Batch 23: Training loss = 0.7800
2025-04-04 15:59:50,877 - INFO - Epoch 5, Batch 24: Training loss = 0.7700
2025-04-04 15:59:50,978 - INFO - Epoch 5, Batch 25: Training loss = 0.7600
2025-04-04 15:59:51,079 - INFO - Epoch 5, Batch 26: Training loss = 0.7500
2025-04-04 15:59:51,179 - INFO - Epoch 5, Batch 27: Training loss = 0.7400
2025-04-04 15:59:51,280 - INFO - Epoch 5, Batch 28: Training loss = 0.7300
2025-04-04 15:59:51,381 - INFO - Epoch 5, Batch 29: Training loss = 0.7200
2025-04-04 15:59:51,482 - INFO - Epoch 5, Batch 30: Training loss = 0.7100
2025-04-04 15:59:51,583 - INFO - Epoch 5, Batch 31: Training loss = 0.7000
2025-04-04 15:59:51,683 - INFO - Epoch 5, Batch 32: Training loss = 0.6900
2025-04-04 15:59:51,784 - INFO - Epoch 5, Batch 33: Training loss = 0.6800
2025-04-04 15:59:51,884 - INFO - Epoch 5, Batch 34: Training loss = 0.6700
2025-04-04 15:59:51,985 - INFO - Epoch 5, Batch 35: Training loss = 0.6600
2025-04-04 15:59:52,085 - INFO - Epoch 5, Batch 36: Training loss = 0.6500
2025-04-04 15:59:52,186 - INFO - Epoch 5, Batch 37: Training loss = 0.6400
2025-04-04 15:59:52,286 - INFO - Epoch 5, Batch 38: Training loss = 0.6300
2025-04-04 15:59:52,387 - INFO - Epoch 5, Batch 39: Training loss = 0.6200
2025-04-04 15:59:52,488 - INFO - Epoch 5, Batch 40: Training loss = 0.6100
2025-04-04 15:59:52,589 - INFO - Epoch 5, Batch 41: Training loss = 0.6000
2025-04-04 15:59:52,690 - INFO - Epoch 5, Batch 42: Training loss = 0.5900
2025-04-04 15:59:52,791 - INFO - Epoch 5, Batch 43: Training loss = 0.5800
2025-04-04 15:59:52,892 - INFO - Epoch 5, Batch 44: Training loss = 0.5700
2025-04-04 15:59:52,993 - INFO - Epoch 5, Batch 45: Training loss = 0.5600
2025-04-04 15:59:53,093 - INFO - Epoch 5, Batch 46: Training loss = 0.5500
2025-04-04 15:59:53,194 - INFO - Epoch 5, Batch 47: Training loss = 0.5400
2025-04-04 15:59:53,295 - INFO - Epoch 5, Batch 48: Training loss = 0.5300
2025-04-04 15:59:53,396 - INFO - Epoch 5, Batch 49: Training loss = 0.5200
2025-04-04 15:59:53,497 - INFO - Epoch 5, Batch 50: Training loss = 0.5100
2025-04-04 15:59:53,598 - INFO - Epoch 5, Batch 51: Training loss = 0.5000
2025-04-04 15:59:53,699 - INFO - Epoch 5, Batch 52: Training loss = 0.4900
2025-04-04 15:59:53,799 - INFO - Epoch 5, Batch 53: Training loss = 0.4800
2025-04-04 15:59:53,900 - INFO - Epoch 5, Batch 54: Training loss = 0.4700
2025-04-04 15:59:54,001 - INFO - Epoch 5, Batch 55: Training loss = 0.4600
2025-04-04 15:59:54,101 - INFO - Epoch 5, Batch 56: Training loss = 0.4500
2025-04-04 15:59:54,202 - INFO - Epoch 5, Batch 57: Training loss = 0.4400
2025-04-04 15:59:54,303 - INFO - Epoch 5, Batch 58: Training loss = 0.4300
2025-04-04 15:59:54,404 - INFO - Epoch 5, Batch 59: Training loss = 0.4200
2025-04-04 15:59:54,505 - INFO - Epoch 5, Batch 60: Training loss = 0.4100
2025-04-04 15:59:54,606 - INFO - Epoch 5, Batch 61: Training loss = 0.4000
2025-04-04 15:59:54,706 - INFO - Epoch 5, Batch 62: Training loss = 0.3900
2025-04-04 15:59:54,807 - INFO - Epoch 5, Batch 63: Training loss = 0.3800
2025-04-04 15:59:54,908 - INFO - Epoch 5, Batch 64: Training loss = 0.3700
2025-04-04 15:59:55,008 - INFO - Epoch 5, Batch 65: Training loss = 0.3600
2025-04-04 15:59:55,109 - INFO - Epoch 5, Batch 66: Training loss = 0.3500
2025-04-04 15:59:55,210 - INFO - Epoch 5, Batch 67: Training loss = 0.3400
2025-04-04 15:59:55,311 - INFO - Epoch 5, Batch 68: Training loss = 0.3300
2025-04-04 15:59:55,412 - INFO - Epoch 5, Batch 69: Training loss = 0.3200
2025-04-04 15:59:55,512 - INFO - Epoch 5, Batch 70: Training loss = 0.3100
2025-04-04 15:59:55,613 - INFO - Epoch 5, Batch 71: Training loss = 0.3000
2025-04-04 15:59:55,713 - INFO - Epoch 5, Batch 72: Training loss = 0.2900
2025-04-04 15:59:55,813 - INFO - Epoch 5, Batch 73: Training loss = 0.2800
2025-04-04 15:59:55,914 - INFO - Epoch 5, Batch 74: Training loss = 0.2700
2025-04-04 15:59:56,015 - INFO - Epoch 5, Batch 75: Training loss = 0.2600
2025-04-04 15:59:56,115 - INFO - Epoch 5, Batch 76: Training loss = 0.2500
2025-04-04 15:59:56,217 - INFO - Epoch 5, Batch 77: Training loss = 0.2400
2025-04-04 15:59:56,318 - INFO - Epoch 5, Batch 78: Training loss = 0.2300
2025-04-04 15:59:56,418 - INFO - Epoch 5, Batch 79: Training loss = 0.2200
2025-04-04 15:59:56,519 - INFO - Epoch 5, Batch 80: Training loss = 0.2100
2025-04-04 15:59:56,620 - INFO - Epoch 5, Batch 81: Training loss = 0.2000
2025-04-04 15:59:56,720 - INFO - Epoch 5, Batch 82: Training loss = 0.1900
2025-04-04 15:59:56,821 - INFO - Epoch 5, Batch 83: Training loss = 0.1800
2025-04-04 15:59:56,922 - INFO - Epoch 5, Batch 84: Training loss = 0.1700
2025-04-04 15:59:57,023 - INFO - Epoch 5, Batch 85: Training loss = 0.1600
2025-04-04 15:59:57,123 - INFO - Epoch 5, Batch 86: Training loss = 0.1500
2025-04-04 15:59:57,225 - INFO - Epoch 5, Batch 87: Training loss = 0.1400
2025-04-04 15:59:57,325 - INFO - Epoch 5, Batch 88: Training loss = 0.1300
2025-04-04 15:59:57,426 - INFO - Epoch 5, Batch 89: Training loss = 0.1200
2025-04-04 15:59:57,527 - INFO - Epoch 5, Batch 90: Training loss = 0.1100
2025-04-04 15:59:57,628 - INFO - Epoch 5, Batch 91: Training loss = 0.1000
2025-04-04 15:59:57,728 - INFO - Epoch 5, Batch 92: Training loss = 0.0900
2025-04-04 15:59:57,829 - INFO - Epoch 5, Batch 93: Training loss = 0.0800
2025-04-04 15:59:57,930 - INFO - Epoch 5, Batch 94: Training loss = 0.0700
2025-04-04 15:59:58,031 - INFO - Epoch 5, Batch 95: Training loss = 0.0600
2025-04-04 15:59:58,131 - INFO - Epoch 5, Batch 96: Training loss = 0.0500
2025-04-04 15:59:58,231 - INFO - Epoch 5, Batch 97: Training loss = 0.0400
2025-04-04 15:59:58,332 - INFO - Epoch 5, Batch 98: Training loss = 0.0300
2025-04-04 15:59:58,433 - INFO - Epoch 5, Batch 99: Training loss = 0.0200
2025-04-04 15:59:58,534 - INFO - Epoch 5, Batch 100: Training loss = 0.0100
2025-04-04 15:59:58,635 - INFO - Epoch 5: Validation loss = 0.0600, Validation accuracy = 0.8800
2025-04-04 15:59:58,635 - INFO - Epoch 5: MSE = 0.0600, RMSE = 0.2449, MAE = 0.0480, RÂ² = 0.9400
2025-04-04 15:59:58,635 - INFO - Epoch 6 started.
2025-04-04 15:59:58,636 - INFO - Epoch 6, Batch 1: Training loss = 1.0000
2025-04-04 15:59:58,736 - INFO - Epoch 6, Batch 2: Training loss = 0.9900
2025-04-04 15:59:58,837 - INFO - Epoch 6, Batch 3: Training loss = 0.9800
2025-04-04 15:59:58,937 - INFO - Epoch 6, Batch 4: Training loss = 0.9700
2025-04-04 15:59:59,038 - INFO - Epoch 6, Batch 5: Training loss = 0.9600
2025-04-04 15:59:59,139 - INFO - Epoch 6, Batch 6: Training loss = 0.9500
2025-04-04 15:59:59,239 - INFO - Epoch 6, Batch 7: Training loss = 0.9400
2025-04-04 15:59:59,341 - INFO - Epoch 6, Batch 8: Training loss = 0.9300
2025-04-04 15:59:59,442 - INFO - Epoch 6, Batch 9: Training loss = 0.9200
2025-04-04 15:59:59,542 - INFO - Epoch 6, Batch 10: Training loss = 0.9100
2025-04-04 15:59:59,643 - INFO - Epoch 6, Batch 11: Training loss = 0.9000
2025-04-04 15:59:59,743 - INFO - Epoch 6, Batch 12: Training loss = 0.8900
2025-04-04 15:59:59,844 - INFO - Epoch 6, Batch 13: Training loss = 0.8800
2025-04-04 15:59:59,944 - INFO - Epoch 6, Batch 14: Training loss = 0.8700
2025-04-04 16:00:00,045 - INFO - Epoch 6, Batch 15: Training loss = 0.8600
2025-04-04 16:00:00,145 - INFO - Epoch 6, Batch 16: Training loss = 0.8500
2025-04-04 16:00:00,246 - INFO - Epoch 6, Batch 17: Training loss = 0.8400
2025-04-04 16:00:00,346 - INFO - Epoch 6, Batch 18: Training loss = 0.8300
2025-04-04 16:00:00,447 - INFO - Epoch 6, Batch 19: Training loss = 0.8200
2025-04-04 16:00:00,547 - INFO - Epoch 6, Batch 20: Training loss = 0.8100
2025-04-04 16:00:00,648 - INFO - Epoch 6, Batch 21: Training loss = 0.8000
2025-04-04 16:00:00,749 - INFO - Epoch 6, Batch 22: Training loss = 0.7900
2025-04-04 16:00:00,849 - INFO - Epoch 6, Batch 23: Training loss = 0.7800
2025-04-04 16:00:00,950 - INFO - Epoch 6, Batch 24: Training loss = 0.7700
2025-04-04 16:00:01,051 - INFO - Epoch 6, Batch 25: Training loss = 0.7600
2025-04-04 16:00:01,152 - INFO - Epoch 6, Batch 26: Training loss = 0.7500
2025-04-04 16:00:01,253 - INFO - Epoch 6, Batch 27: Training loss = 0.7400
2025-04-04 16:00:01,354 - INFO - Epoch 6, Batch 28: Training loss = 0.7300
2025-04-04 16:00:01,454 - INFO - Epoch 6, Batch 29: Training loss = 0.7200
2025-04-04 16:00:01,555 - INFO - Epoch 6, Batch 30: Training loss = 0.7100
2025-04-04 16:00:01,656 - INFO - Epoch 6, Batch 31: Training loss = 0.7000
2025-04-04 16:00:01,757 - INFO - Epoch 6, Batch 32: Training loss = 0.6900
2025-04-04 16:00:01,858 - INFO - Epoch 6, Batch 33: Training loss = 0.6800
2025-04-04 16:00:01,958 - INFO - Epoch 6, Batch 34: Training loss = 0.6700
2025-04-04 16:00:02,058 - INFO - Epoch 6, Batch 35: Training loss = 0.6600
2025-04-04 16:00:02,159 - INFO - Epoch 6, Batch 36: Training loss = 0.6500
2025-04-04 16:00:02,260 - INFO - Epoch 6, Batch 37: Training loss = 0.6400
2025-04-04 16:00:02,360 - INFO - Epoch 6, Batch 38: Training loss = 0.6300
2025-04-04 16:00:02,461 - INFO - Epoch 6, Batch 39: Training loss = 0.6200
2025-04-04 16:00:02,561 - INFO - Epoch 6, Batch 40: Training loss = 0.6100
2025-04-04 16:00:02,662 - INFO - Epoch 6, Batch 41: Training loss = 0.6000
2025-04-04 16:00:02,762 - INFO - Epoch 6, Batch 42: Training loss = 0.5900
2025-04-04 16:00:02,863 - INFO - Epoch 6, Batch 43: Training loss = 0.5800
2025-04-04 16:00:02,963 - INFO - Epoch 6, Batch 44: Training loss = 0.5700
2025-04-04 16:00:03,064 - INFO - Epoch 6, Batch 45: Training loss = 0.5600
2025-04-04 16:00:03,165 - INFO - Epoch 6, Batch 46: Training loss = 0.5500
2025-04-04 16:00:03,265 - INFO - Epoch 6, Batch 47: Training loss = 0.5400
2025-04-04 16:00:03,366 - INFO - Epoch 6, Batch 48: Training loss = 0.5300
2025-04-04 16:00:03,467 - INFO - Epoch 6, Batch 49: Training loss = 0.5200
2025-04-04 16:00:03,568 - INFO - Epoch 6, Batch 50: Training loss = 0.5100
2025-04-04 16:00:03,669 - INFO - Epoch 6, Batch 51: Training loss = 0.5000
2025-04-04 16:00:03,770 - INFO - Epoch 6, Batch 52: Training loss = 0.4900
2025-04-04 16:00:03,871 - INFO - Epoch 6, Batch 53: Training loss = 0.4800
2025-04-04 16:00:03,972 - INFO - Epoch 6, Batch 54: Training loss = 0.4700
2025-04-04 16:00:04,072 - INFO - Epoch 6, Batch 55: Training loss = 0.4600
2025-04-04 16:00:04,173 - INFO - Epoch 6, Batch 56: Training loss = 0.4500
2025-04-04 16:00:04,274 - INFO - Epoch 6, Batch 57: Training loss = 0.4400
2025-04-04 16:00:04,375 - INFO - Epoch 6, Batch 58: Training loss = 0.4300
2025-04-04 16:00:04,476 - INFO - Epoch 6, Batch 59: Training loss = 0.4200
2025-04-04 16:00:04,577 - INFO - Epoch 6, Batch 60: Training loss = 0.4100
2025-04-04 16:00:04,678 - INFO - Epoch 6, Batch 61: Training loss = 0.4000
2025-04-04 16:00:04,780 - INFO - Epoch 6, Batch 62: Training loss = 0.3900
2025-04-04 16:00:04,880 - INFO - Epoch 6, Batch 63: Training loss = 0.3800
2025-04-04 16:00:04,981 - INFO - Epoch 6, Batch 64: Training loss = 0.3700
2025-04-04 16:00:05,082 - INFO - Epoch 6, Batch 65: Training loss = 0.3600
2025-04-04 16:00:05,182 - INFO - Epoch 6, Batch 66: Training loss = 0.3500
2025-04-04 16:00:05,283 - INFO - Epoch 6, Batch 67: Training loss = 0.3400
2025-04-04 16:00:05,384 - INFO - Epoch 6, Batch 68: Training loss = 0.3300
2025-04-04 16:00:05,485 - INFO - Epoch 6, Batch 69: Training loss = 0.3200
2025-04-04 16:00:05,586 - INFO - Epoch 6, Batch 70: Training loss = 0.3100
2025-04-04 16:00:05,687 - INFO - Epoch 6, Batch 71: Training loss = 0.3000
2025-04-04 16:00:05,788 - INFO - Epoch 6, Batch 72: Training loss = 0.2900
2025-04-04 16:00:05,889 - INFO - Epoch 6, Batch 73: Training loss = 0.2800
2025-04-04 16:00:05,989 - INFO - Epoch 6, Batch 74: Training loss = 0.2700
2025-04-04 16:00:06,090 - INFO - Epoch 6, Batch 75: Training loss = 0.2600
2025-04-04 16:00:06,192 - INFO - Epoch 6, Batch 76: Training loss = 0.2500
2025-04-04 16:00:06,293 - INFO - Epoch 6, Batch 77: Training loss = 0.2400
2025-04-04 16:00:06,394 - INFO - Epoch 6, Batch 78: Training loss = 0.2300
2025-04-04 16:00:06,494 - INFO - Epoch 6, Batch 79: Training loss = 0.2200
2025-04-04 16:00:06,595 - INFO - Epoch 6, Batch 80: Training loss = 0.2100
2025-04-04 16:00:06,696 - INFO - Epoch 6, Batch 81: Training loss = 0.2000
2025-04-04 16:00:06,797 - INFO - Epoch 6, Batch 82: Training loss = 0.1900
2025-04-04 16:00:06,897 - INFO - Epoch 6, Batch 83: Training loss = 0.1800
2025-04-04 16:00:06,998 - INFO - Epoch 6, Batch 84: Training loss = 0.1700
2025-04-04 16:00:07,099 - INFO - Epoch 6, Batch 85: Training loss = 0.1600
2025-04-04 16:00:07,200 - INFO - Epoch 6, Batch 86: Training loss = 0.1500
2025-04-04 16:00:07,301 - INFO - Epoch 6, Batch 87: Training loss = 0.1400
2025-04-04 16:00:07,402 - INFO - Epoch 6, Batch 88: Training loss = 0.1300
2025-04-04 16:00:07,503 - INFO - Epoch 6, Batch 89: Training loss = 0.1200
2025-04-04 16:00:07,604 - INFO - Epoch 6, Batch 90: Training loss = 0.1100
2025-04-04 16:00:07,705 - INFO - Epoch 6, Batch 91: Training loss = 0.1000
2025-04-04 16:00:07,805 - INFO - Epoch 6, Batch 92: Training loss = 0.0900
2025-04-04 16:00:07,906 - INFO - Epoch 6, Batch 93: Training loss = 0.0800
2025-04-04 16:00:08,007 - INFO - Epoch 6, Batch 94: Training loss = 0.0700
2025-04-04 16:00:08,107 - INFO - Epoch 6, Batch 95: Training loss = 0.0600
2025-04-04 16:00:08,208 - INFO - Epoch 6, Batch 96: Training loss = 0.0500
2025-04-04 16:00:08,309 - INFO - Epoch 6, Batch 97: Training loss = 0.0400
2025-04-04 16:00:08,409 - INFO - Epoch 6, Batch 98: Training loss = 0.0300
2025-04-04 16:00:08,510 - INFO - Epoch 6, Batch 99: Training loss = 0.0200
2025-04-04 16:00:08,611 - INFO - Epoch 6, Batch 100: Training loss = 0.0100
2025-04-04 16:00:08,712 - INFO - Epoch 6: Validation loss = 0.0500, Validation accuracy = 0.9000
2025-04-04 16:00:08,712 - INFO - Epoch 6: MSE = 0.0500, RMSE = 0.2236, MAE = 0.0400, RÂ² = 0.9500
2025-04-04 16:00:08,713 - INFO - Epoch 7 started.
2025-04-04 16:00:08,713 - INFO - Epoch 7, Batch 1: Training loss = 1.0000
2025-04-04 16:00:08,814 - INFO - Epoch 7, Batch 2: Training loss = 0.9900
2025-04-04 16:00:08,914 - INFO - Epoch 7, Batch 3: Training loss = 0.9800
2025-04-04 16:00:09,015 - INFO - Epoch 7, Batch 4: Training loss = 0.9700
2025-04-04 16:00:09,116 - INFO - Epoch 7, Batch 5: Training loss = 0.9600
2025-04-04 16:00:09,216 - INFO - Epoch 7, Batch 6: Training loss = 0.9500
2025-04-04 16:00:09,317 - INFO - Epoch 7, Batch 7: Training loss = 0.9400
2025-04-04 16:00:09,417 - INFO - Epoch 7, Batch 8: Training loss = 0.9300
2025-04-04 16:00:09,519 - INFO - Epoch 7, Batch 9: Training loss = 0.9200
2025-04-04 16:00:09,619 - INFO - Epoch 7, Batch 10: Training loss = 0.9100
2025-04-04 16:00:09,720 - INFO - Epoch 7, Batch 11: Training loss = 0.9000
2025-04-04 16:00:09,821 - INFO - Epoch 7, Batch 12: Training loss = 0.8900
2025-04-04 16:00:09,922 - INFO - Epoch 7, Batch 13: Training loss = 0.8800
2025-04-04 16:00:10,022 - INFO - Epoch 7, Batch 14: Training loss = 0.8700
2025-04-04 16:00:10,123 - INFO - Epoch 7, Batch 15: Training loss = 0.8600
2025-04-04 16:00:10,224 - INFO - Epoch 7, Batch 16: Training loss = 0.8500
2025-04-04 16:00:10,324 - INFO - Epoch 7, Batch 17: Training loss = 0.8400
2025-04-04 16:00:10,425 - INFO - Epoch 7, Batch 18: Training loss = 0.8300
2025-04-04 16:00:10,525 - INFO - Epoch 7, Batch 19: Training loss = 0.8200
2025-04-04 16:00:10,626 - INFO - Epoch 7, Batch 20: Training loss = 0.8100
2025-04-04 16:00:10,726 - INFO - Epoch 7, Batch 21: Training loss = 0.8000
2025-04-04 16:00:10,827 - INFO - Epoch 7, Batch 22: Training loss = 0.7900
2025-04-04 16:00:10,928 - INFO - Epoch 7, Batch 23: Training loss = 0.7800
2025-04-04 16:00:11,029 - INFO - Epoch 7, Batch 24: Training loss = 0.7700
2025-04-04 16:00:11,129 - INFO - Epoch 7, Batch 25: Training loss = 0.7600
2025-04-04 16:00:11,230 - INFO - Epoch 7, Batch 26: Training loss = 0.7500
2025-04-04 16:00:11,331 - INFO - Epoch 7, Batch 27: Training loss = 0.7400
2025-04-04 16:00:11,432 - INFO - Epoch 7, Batch 28: Training loss = 0.7300
2025-04-04 16:00:11,532 - INFO - Epoch 7, Batch 29: Training loss = 0.7200
2025-04-04 16:00:11,633 - INFO - Epoch 7, Batch 30: Training loss = 0.7100
2025-04-04 16:00:11,734 - INFO - Epoch 7, Batch 31: Training loss = 0.7000
2025-04-04 16:00:11,835 - INFO - Epoch 7, Batch 32: Training loss = 0.6900
2025-04-04 16:00:11,936 - INFO - Epoch 7, Batch 33: Training loss = 0.6800
2025-04-04 16:00:12,037 - INFO - Epoch 7, Batch 34: Training loss = 0.6700
2025-04-04 16:00:12,137 - INFO - Epoch 7, Batch 35: Training loss = 0.6600
2025-04-04 16:00:12,238 - INFO - Epoch 7, Batch 36: Training loss = 0.6500
2025-04-04 16:00:12,338 - INFO - Epoch 7, Batch 37: Training loss = 0.6400
2025-04-04 16:00:12,439 - INFO - Epoch 7, Batch 38: Training loss = 0.6300
2025-04-04 16:00:12,541 - INFO - Epoch 7, Batch 39: Training loss = 0.6200
2025-04-04 16:00:12,642 - INFO - Epoch 7, Batch 40: Training loss = 0.6100
2025-04-04 16:00:12,743 - INFO - Epoch 7, Batch 41: Training loss = 0.6000
2025-04-04 16:00:12,844 - INFO - Epoch 7, Batch 42: Training loss = 0.5900
2025-04-04 16:00:12,944 - INFO - Epoch 7, Batch 43: Training loss = 0.5800
2025-04-04 16:00:13,045 - INFO - Epoch 7, Batch 44: Training loss = 0.5700
2025-04-04 16:00:13,146 - INFO - Epoch 7, Batch 45: Training loss = 0.5600
2025-04-04 16:00:13,246 - INFO - Epoch 7, Batch 46: Training loss = 0.5500
2025-04-04 16:00:13,347 - INFO - Epoch 7, Batch 47: Training loss = 0.5400
2025-04-04 16:00:13,448 - INFO - Epoch 7, Batch 48: Training loss = 0.5300
2025-04-04 16:00:13,549 - INFO - Epoch 7, Batch 49: Training loss = 0.5200
2025-04-04 16:00:13,649 - INFO - Epoch 7, Batch 50: Training loss = 0.5100
2025-04-04 16:00:13,750 - INFO - Epoch 7, Batch 51: Training loss = 0.5000
2025-04-04 16:00:13,851 - INFO - Epoch 7, Batch 52: Training loss = 0.4900
2025-04-04 16:00:13,951 - INFO - Epoch 7, Batch 53: Training loss = 0.4800
2025-04-04 16:00:14,052 - INFO - Epoch 7, Batch 54: Training loss = 0.4700
2025-04-04 16:00:14,153 - INFO - Epoch 7, Batch 55: Training loss = 0.4600
2025-04-04 16:00:14,254 - INFO - Epoch 7, Batch 56: Training loss = 0.4500
2025-04-04 16:00:14,354 - INFO - Epoch 7, Batch 57: Training loss = 0.4400
2025-04-04 16:00:14,455 - INFO - Epoch 7, Batch 58: Training loss = 0.4300
2025-04-04 16:00:14,556 - INFO - Epoch 7, Batch 59: Training loss = 0.4200
2025-04-04 16:00:14,657 - INFO - Epoch 7, Batch 60: Training loss = 0.4100
2025-04-04 16:00:14,757 - INFO - Epoch 7, Batch 61: Training loss = 0.4000
2025-04-04 16:00:14,858 - INFO - Epoch 7, Batch 62: Training loss = 0.3900
2025-04-04 16:00:14,958 - INFO - Epoch 7, Batch 63: Training loss = 0.3800
2025-04-04 16:00:15,058 - INFO - Epoch 7, Batch 64: Training loss = 0.3700
2025-04-04 16:00:15,159 - INFO - Epoch 7, Batch 65: Training loss = 0.3600
2025-04-04 16:00:15,259 - INFO - Epoch 7, Batch 66: Training loss = 0.3500
2025-04-04 16:00:15,360 - INFO - Epoch 7, Batch 67: Training loss = 0.3400
2025-04-04 16:00:15,460 - INFO - Epoch 7, Batch 68: Training loss = 0.3300
2025-04-04 16:00:15,561 - INFO - Epoch 7, Batch 69: Training loss = 0.3200
2025-04-04 16:00:15,661 - INFO - Epoch 7, Batch 70: Training loss = 0.3100
2025-04-04 16:00:15,762 - INFO - Epoch 7, Batch 71: Training loss = 0.3000
2025-04-04 16:00:15,862 - INFO - Epoch 7, Batch 72: Training loss = 0.2900
2025-04-04 16:00:15,963 - INFO - Epoch 7, Batch 73: Training loss = 0.2800
2025-04-04 16:00:16,063 - INFO - Epoch 7, Batch 74: Training loss = 0.2700
2025-04-04 16:00:16,164 - INFO - Epoch 7, Batch 75: Training loss = 0.2600
2025-04-04 16:00:16,265 - INFO - Epoch 7, Batch 76: Training loss = 0.2500
2025-04-04 16:00:16,366 - INFO - Epoch 7, Batch 77: Training loss = 0.2400
2025-04-04 16:00:16,466 - INFO - Epoch 7, Batch 78: Training loss = 0.2300
2025-04-04 16:00:16,567 - INFO - Epoch 7, Batch 79: Training loss = 0.2200
2025-04-04 16:00:16,668 - INFO - Epoch 7, Batch 80: Training loss = 0.2100
2025-04-04 16:00:16,769 - INFO - Epoch 7, Batch 81: Training loss = 0.2000
2025-04-04 16:00:16,869 - INFO - Epoch 7, Batch 82: Training loss = 0.1900
2025-04-04 16:00:16,970 - INFO - Epoch 7, Batch 83: Training loss = 0.1800
2025-04-04 16:00:17,071 - INFO - Epoch 7, Batch 84: Training loss = 0.1700
2025-04-04 16:00:17,172 - INFO - Epoch 7, Batch 85: Training loss = 0.1600
2025-04-04 16:00:17,273 - INFO - Epoch 7, Batch 86: Training loss = 0.1500
2025-04-04 16:00:17,374 - INFO - Epoch 7, Batch 87: Training loss = 0.1400
2025-04-04 16:00:17,475 - INFO - Epoch 7, Batch 88: Training loss = 0.1300
2025-04-04 16:00:17,576 - INFO - Epoch 7, Batch 89: Training loss = 0.1200
2025-04-04 16:00:17,677 - INFO - Epoch 7, Batch 90: Training loss = 0.1100
2025-04-04 16:00:17,778 - INFO - Epoch 7, Batch 91: Training loss = 0.1000
2025-04-04 16:00:17,878 - INFO - Epoch 7, Batch 92: Training loss = 0.0900
2025-04-04 16:00:17,979 - INFO - Epoch 7, Batch 93: Training loss = 0.0800
2025-04-04 16:00:18,079 - INFO - Epoch 7, Batch 94: Training loss = 0.0700
2025-04-04 16:00:18,180 - INFO - Epoch 7, Batch 95: Training loss = 0.0600
2025-04-04 16:00:18,280 - INFO - Epoch 7, Batch 96: Training loss = 0.0500
2025-04-04 16:00:18,380 - INFO - Epoch 7, Batch 97: Training loss = 0.0400
2025-04-04 16:00:18,481 - INFO - Epoch 7, Batch 98: Training loss = 0.0300
2025-04-04 16:00:18,582 - INFO - Epoch 7, Batch 99: Training loss = 0.0200
2025-04-04 16:00:18,683 - INFO - Epoch 7, Batch 100: Training loss = 0.0100
2025-04-04 16:00:18,784 - INFO - Epoch 7: Validation loss = 0.0400, Validation accuracy = 0.9200
2025-04-04 16:00:18,785 - INFO - Epoch 7: MSE = 0.0400, RMSE = 0.2000, MAE = 0.0320, RÂ² = 0.9600
2025-04-04 16:00:18,785 - INFO - Epoch 8 started.
2025-04-04 16:00:18,785 - INFO - Epoch 8, Batch 1: Training loss = 1.0000
2025-04-04 16:00:18,885 - INFO - Epoch 8, Batch 2: Training loss = 0.9900
2025-04-04 16:00:18,986 - INFO - Epoch 8, Batch 3: Training loss = 0.9800
2025-04-04 16:00:19,087 - INFO - Epoch 8, Batch 4: Training loss = 0.9700
2025-04-04 16:00:19,188 - INFO - Epoch 8, Batch 5: Training loss = 0.9600
2025-04-04 16:00:19,290 - INFO - Epoch 8, Batch 6: Training loss = 0.9500
2025-04-04 16:00:19,390 - INFO - Epoch 8, Batch 7: Training loss = 0.9400
2025-04-04 16:00:19,491 - INFO - Epoch 8, Batch 8: Training loss = 0.9300
2025-04-04 16:00:19,592 - INFO - Epoch 8, Batch 9: Training loss = 0.9200
2025-04-04 16:00:19,693 - INFO - Epoch 8, Batch 10: Training loss = 0.9100
2025-04-04 16:00:19,794 - INFO - Epoch 8, Batch 11: Training loss = 0.9000
2025-04-04 16:00:19,895 - INFO - Epoch 8, Batch 12: Training loss = 0.8900
2025-04-04 16:00:19,996 - INFO - Epoch 8, Batch 13: Training loss = 0.8800
2025-04-04 16:00:20,097 - INFO - Epoch 8, Batch 14: Training loss = 0.8700
2025-04-04 16:00:20,197 - INFO - Epoch 8, Batch 15: Training loss = 0.8600
2025-04-04 16:00:20,298 - INFO - Epoch 8, Batch 16: Training loss = 0.8500
2025-04-04 16:00:20,399 - INFO - Epoch 8, Batch 17: Training loss = 0.8400
2025-04-04 16:00:20,499 - INFO - Epoch 8, Batch 18: Training loss = 0.8300
2025-04-04 16:00:20,600 - INFO - Epoch 8, Batch 19: Training loss = 0.8200
2025-04-04 16:00:20,701 - INFO - Epoch 8, Batch 20: Training loss = 0.8100
2025-04-04 16:00:20,801 - INFO - Epoch 8, Batch 21: Training loss = 0.8000
2025-04-04 16:00:20,902 - INFO - Epoch 8, Batch 22: Training loss = 0.7900
2025-04-04 16:00:21,003 - INFO - Epoch 8, Batch 23: Training loss = 0.7800
2025-04-04 16:00:21,103 - INFO - Epoch 8, Batch 24: Training loss = 0.7700
2025-04-04 16:00:21,205 - INFO - Epoch 8, Batch 25: Training loss = 0.7600
2025-04-04 16:00:21,306 - INFO - Epoch 8, Batch 26: Training loss = 0.7500
2025-04-04 16:00:21,407 - INFO - Epoch 8, Batch 27: Training loss = 0.7400
2025-04-04 16:00:21,508 - INFO - Epoch 8, Batch 28: Training loss = 0.7300
2025-04-04 16:00:21,609 - INFO - Epoch 8, Batch 29: Training loss = 0.7200
2025-04-04 16:00:21,710 - INFO - Epoch 8, Batch 30: Training loss = 0.7100
2025-04-04 16:00:21,811 - INFO - Epoch 8, Batch 31: Training loss = 0.7000
2025-04-04 16:00:21,911 - INFO - Epoch 8, Batch 32: Training loss = 0.6900
2025-04-04 16:00:22,012 - INFO - Epoch 8, Batch 33: Training loss = 0.6800
2025-04-04 16:00:22,113 - INFO - Epoch 8, Batch 34: Training loss = 0.6700
2025-04-04 16:00:22,213 - INFO - Epoch 8, Batch 35: Training loss = 0.6600
2025-04-04 16:00:22,314 - INFO - Epoch 8, Batch 36: Training loss = 0.6500
2025-04-04 16:00:22,415 - INFO - Epoch 8, Batch 37: Training loss = 0.6400
2025-04-04 16:00:22,516 - INFO - Epoch 8, Batch 38: Training loss = 0.6300
2025-04-04 16:00:22,616 - INFO - Epoch 8, Batch 39: Training loss = 0.6200
2025-04-04 16:00:22,717 - INFO - Epoch 8, Batch 40: Training loss = 0.6100
2025-04-04 16:00:22,818 - INFO - Epoch 8, Batch 41: Training loss = 0.6000
2025-04-04 16:00:22,919 - INFO - Epoch 8, Batch 42: Training loss = 0.5900
2025-04-04 16:00:23,020 - INFO - Epoch 8, Batch 43: Training loss = 0.5800
2025-04-04 16:00:23,121 - INFO - Epoch 8, Batch 44: Training loss = 0.5700
2025-04-04 16:00:23,222 - INFO - Epoch 8, Batch 45: Training loss = 0.5600
2025-04-04 16:00:23,322 - INFO - Epoch 8, Batch 46: Training loss = 0.5500
2025-04-04 16:00:23,423 - INFO - Epoch 8, Batch 47: Training loss = 0.5400
2025-04-04 16:00:23,524 - INFO - Epoch 8, Batch 48: Training loss = 0.5300
2025-04-04 16:00:23,625 - INFO - Epoch 8, Batch 49: Training loss = 0.5200
2025-04-04 16:00:23,726 - INFO - Epoch 8, Batch 50: Training loss = 0.5100
2025-04-04 16:00:23,826 - INFO - Epoch 8, Batch 51: Training loss = 0.5000
2025-04-04 16:00:23,927 - INFO - Epoch 8, Batch 52: Training loss = 0.4900
2025-04-04 16:00:24,028 - INFO - Epoch 8, Batch 53: Training loss = 0.4800
2025-04-04 16:00:24,129 - INFO - Epoch 8, Batch 54: Training loss = 0.4700
2025-04-04 16:00:24,230 - INFO - Epoch 8, Batch 55: Training loss = 0.4600
2025-04-04 16:00:24,330 - INFO - Epoch 8, Batch 56: Training loss = 0.4500
2025-04-04 16:00:24,431 - INFO - Epoch 8, Batch 57: Training loss = 0.4400
2025-04-04 16:00:24,532 - INFO - Epoch 8, Batch 58: Training loss = 0.4300
2025-04-04 16:00:24,632 - INFO - Epoch 8, Batch 59: Training loss = 0.4200
2025-04-04 16:00:24,733 - INFO - Epoch 8, Batch 60: Training loss = 0.4100
2025-04-04 16:00:24,834 - INFO - Epoch 8, Batch 61: Training loss = 0.4000
2025-04-04 16:00:24,935 - INFO - Epoch 8, Batch 62: Training loss = 0.3900
2025-04-04 16:00:25,036 - INFO - Epoch 8, Batch 63: Training loss = 0.3800
2025-04-04 16:00:25,136 - INFO - Epoch 8, Batch 64: Training loss = 0.3700
2025-04-04 16:00:25,237 - INFO - Epoch 8, Batch 65: Training loss = 0.3600
2025-04-04 16:00:25,338 - INFO - Epoch 8, Batch 66: Training loss = 0.3500
2025-04-04 16:00:25,439 - INFO - Epoch 8, Batch 67: Training loss = 0.3400
2025-04-04 16:00:25,540 - INFO - Epoch 8, Batch 68: Training loss = 0.3300
2025-04-04 16:00:25,640 - INFO - Epoch 8, Batch 69: Training loss = 0.3200
2025-04-04 16:00:25,741 - INFO - Epoch 8, Batch 70: Training loss = 0.3100
2025-04-04 16:00:25,842 - INFO - Epoch 8, Batch 71: Training loss = 0.3000
2025-04-04 16:00:25,942 - INFO - Epoch 8, Batch 72: Training loss = 0.2900
2025-04-04 16:00:26,043 - INFO - Epoch 8, Batch 73: Training loss = 0.2800
2025-04-04 16:00:26,143 - INFO - Epoch 8, Batch 74: Training loss = 0.2700
2025-04-04 16:00:26,244 - INFO - Epoch 8, Batch 75: Training loss = 0.2600
2025-04-04 16:00:26,345 - INFO - Epoch 8, Batch 76: Training loss = 0.2500
2025-04-04 16:00:26,446 - INFO - Epoch 8, Batch 77: Training loss = 0.2400
2025-04-04 16:00:26,546 - INFO - Epoch 8, Batch 78: Training loss = 0.2300
2025-04-04 16:00:26,647 - INFO - Epoch 8, Batch 79: Training loss = 0.2200
2025-04-04 16:00:26,748 - INFO - Epoch 8, Batch 80: Training loss = 0.2100
2025-04-04 16:00:26,848 - INFO - Epoch 8, Batch 81: Training loss = 0.2000
2025-04-04 16:00:26,949 - INFO - Epoch 8, Batch 82: Training loss = 0.1900
2025-04-04 16:00:27,049 - INFO - Epoch 8, Batch 83: Training loss = 0.1800
2025-04-04 16:00:27,150 - INFO - Epoch 8, Batch 84: Training loss = 0.1700
2025-04-04 16:00:27,251 - INFO - Epoch 8, Batch 85: Training loss = 0.1600
2025-04-04 16:00:27,352 - INFO - Epoch 8, Batch 86: Training loss = 0.1500
2025-04-04 16:00:27,453 - INFO - Epoch 8, Batch 87: Training loss = 0.1400
2025-04-04 16:00:27,554 - INFO - Epoch 8, Batch 88: Training loss = 0.1300
2025-04-04 16:00:27,655 - INFO - Epoch 8, Batch 89: Training loss = 0.1200
2025-04-04 16:00:27,756 - INFO - Epoch 8, Batch 90: Training loss = 0.1100
2025-04-04 16:00:27,857 - INFO - Epoch 8, Batch 91: Training loss = 0.1000
2025-04-04 16:00:27,958 - INFO - Epoch 8, Batch 92: Training loss = 0.0900
2025-04-04 16:00:28,059 - INFO - Epoch 8, Batch 93: Training loss = 0.0800
2025-04-04 16:00:28,160 - INFO - Epoch 8, Batch 94: Training loss = 0.0700
2025-04-04 16:00:28,260 - INFO - Epoch 8, Batch 95: Training loss = 0.0600
2025-04-04 16:00:28,361 - INFO - Epoch 8, Batch 96: Training loss = 0.0500
2025-04-04 16:00:28,462 - INFO - Epoch 8, Batch 97: Training loss = 0.0400
2025-04-04 16:00:28,562 - INFO - Epoch 8, Batch 98: Training loss = 0.0300
2025-04-04 16:00:28,663 - INFO - Epoch 8, Batch 99: Training loss = 0.0200
2025-04-04 16:00:28,764 - INFO - Epoch 8, Batch 100: Training loss = 0.0100
2025-04-04 16:00:28,864 - INFO - Epoch 8: Validation loss = 0.0300, Validation accuracy = 0.9400
2025-04-04 16:00:28,864 - INFO - Epoch 8: MSE = 0.0300, RMSE = 0.1732, MAE = 0.0240, RÂ² = 0.9700
2025-04-04 16:00:28,864 - INFO - Epoch 9 started.
2025-04-04 16:00:28,864 - INFO - Epoch 9, Batch 1: Training loss = 1.0000
2025-04-04 16:00:28,965 - INFO - Epoch 9, Batch 2: Training loss = 0.9900
2025-04-04 16:00:29,065 - INFO - Epoch 9, Batch 3: Training loss = 0.9800
2025-04-04 16:00:29,166 - INFO - Epoch 9, Batch 4: Training loss = 0.9700
2025-04-04 16:00:29,267 - INFO - Epoch 9, Batch 5: Training loss = 0.9600
2025-04-04 16:00:29,368 - INFO - Epoch 9, Batch 6: Training loss = 0.9500
2025-04-04 16:00:29,469 - INFO - Epoch 9, Batch 7: Training loss = 0.9400
2025-04-04 16:00:29,570 - INFO - Epoch 9, Batch 8: Training loss = 0.9300
2025-04-04 16:00:29,670 - INFO - Epoch 9, Batch 9: Training loss = 0.9200
2025-04-04 16:00:29,771 - INFO - Epoch 9, Batch 10: Training loss = 0.9100
2025-04-04 16:00:29,872 - INFO - Epoch 9, Batch 11: Training loss = 0.9000
2025-04-04 16:00:29,973 - INFO - Epoch 9, Batch 12: Training loss = 0.8900
2025-04-04 16:00:30,074 - INFO - Epoch 9, Batch 13: Training loss = 0.8800
2025-04-04 16:00:30,175 - INFO - Epoch 9, Batch 14: Training loss = 0.8700
2025-04-04 16:00:30,276 - INFO - Epoch 9, Batch 15: Training loss = 0.8600
2025-04-04 16:00:30,377 - INFO - Epoch 9, Batch 16: Training loss = 0.8500
2025-04-04 16:00:30,478 - INFO - Epoch 9, Batch 17: Training loss = 0.8400
2025-04-04 16:00:30,578 - INFO - Epoch 9, Batch 18: Training loss = 0.8300
2025-04-04 16:00:30,679 - INFO - Epoch 9, Batch 19: Training loss = 0.8200
2025-04-04 16:00:30,780 - INFO - Epoch 9, Batch 20: Training loss = 0.8100
2025-04-04 16:00:30,880 - INFO - Epoch 9, Batch 21: Training loss = 0.8000
2025-04-04 16:00:30,981 - INFO - Epoch 9, Batch 22: Training loss = 0.7900
2025-04-04 16:00:31,082 - INFO - Epoch 9, Batch 23: Training loss = 0.7800
2025-04-04 16:00:31,183 - INFO - Epoch 9, Batch 24: Training loss = 0.7700
2025-04-04 16:00:31,284 - INFO - Epoch 9, Batch 25: Training loss = 0.7600
2025-04-04 16:00:31,385 - INFO - Epoch 9, Batch 26: Training loss = 0.7500
2025-04-04 16:00:31,486 - INFO - Epoch 9, Batch 27: Training loss = 0.7400
2025-04-04 16:00:31,587 - INFO - Epoch 9, Batch 28: Training loss = 0.7300
2025-04-04 16:00:31,688 - INFO - Epoch 9, Batch 29: Training loss = 0.7200
2025-04-04 16:00:31,789 - INFO - Epoch 9, Batch 30: Training loss = 0.7100
2025-04-04 16:00:31,890 - INFO - Epoch 9, Batch 31: Training loss = 0.7000
2025-04-04 16:00:31,991 - INFO - Epoch 9, Batch 32: Training loss = 0.6900
2025-04-04 16:00:32,091 - INFO - Epoch 9, Batch 33: Training loss = 0.6800
2025-04-04 16:00:32,192 - INFO - Epoch 9, Batch 34: Training loss = 0.6700
2025-04-04 16:00:32,293 - INFO - Epoch 9, Batch 35: Training loss = 0.6600
2025-04-04 16:00:32,394 - INFO - Epoch 9, Batch 36: Training loss = 0.6500
2025-04-04 16:00:32,494 - INFO - Epoch 9, Batch 37: Training loss = 0.6400
2025-04-04 16:00:32,595 - INFO - Epoch 9, Batch 38: Training loss = 0.6300
2025-04-04 16:00:32,696 - INFO - Epoch 9, Batch 39: Training loss = 0.6200
2025-04-04 16:00:32,797 - INFO - Epoch 9, Batch 40: Training loss = 0.6100
2025-04-04 16:00:32,897 - INFO - Epoch 9, Batch 41: Training loss = 0.6000
2025-04-04 16:00:32,998 - INFO - Epoch 9, Batch 42: Training loss = 0.5900
2025-04-04 16:00:33,099 - INFO - Epoch 9, Batch 43: Training loss = 0.5800
2025-04-04 16:00:33,200 - INFO - Epoch 9, Batch 44: Training loss = 0.5700
2025-04-04 16:00:33,301 - INFO - Epoch 9, Batch 45: Training loss = 0.5600
2025-04-04 16:00:33,401 - INFO - Epoch 9, Batch 46: Training loss = 0.5500
2025-04-04 16:00:33,502 - INFO - Epoch 9, Batch 47: Training loss = 0.5400
2025-04-04 16:00:33,603 - INFO - Epoch 9, Batch 48: Training loss = 0.5300
2025-04-04 16:00:33,704 - INFO - Epoch 9, Batch 49: Training loss = 0.5200
2025-04-04 16:00:33,804 - INFO - Epoch 9, Batch 50: Training loss = 0.5100
2025-04-04 16:00:33,905 - INFO - Epoch 9, Batch 51: Training loss = 0.5000
2025-04-04 16:00:34,006 - INFO - Epoch 9, Batch 52: Training loss = 0.4900
2025-04-04 16:00:34,107 - INFO - Epoch 9, Batch 53: Training loss = 0.4800
2025-04-04 16:00:34,207 - INFO - Epoch 9, Batch 54: Training loss = 0.4700
2025-04-04 16:00:34,309 - INFO - Epoch 9, Batch 55: Training loss = 0.4600
2025-04-04 16:00:34,410 - INFO - Epoch 9, Batch 56: Training loss = 0.4500
2025-04-04 16:00:34,510 - INFO - Epoch 9, Batch 57: Training loss = 0.4400
2025-04-04 16:00:34,611 - INFO - Epoch 9, Batch 58: Training loss = 0.4300
2025-04-04 16:00:34,712 - INFO - Epoch 9, Batch 59: Training loss = 0.4200
2025-04-04 16:00:34,813 - INFO - Epoch 9, Batch 60: Training loss = 0.4100
2025-04-04 16:00:34,913 - INFO - Epoch 9, Batch 61: Training loss = 0.4000
2025-04-04 16:00:35,014 - INFO - Epoch 9, Batch 62: Training loss = 0.3900
2025-04-04 16:00:35,115 - INFO - Epoch 9, Batch 63: Training loss = 0.3800
2025-04-04 16:00:35,216 - INFO - Epoch 9, Batch 64: Training loss = 0.3700
2025-04-04 16:00:35,317 - INFO - Epoch 9, Batch 65: Training loss = 0.3600
2025-04-04 16:00:35,418 - INFO - Epoch 9, Batch 66: Training loss = 0.3500
2025-04-04 16:00:35,518 - INFO - Epoch 9, Batch 67: Training loss = 0.3400
2025-04-04 16:00:35,619 - INFO - Epoch 9, Batch 68: Training loss = 0.3300
2025-04-04 16:00:35,720 - INFO - Epoch 9, Batch 69: Training loss = 0.3200
2025-04-04 16:00:35,820 - INFO - Epoch 9, Batch 70: Training loss = 0.3100
2025-04-04 16:00:35,921 - INFO - Epoch 9, Batch 71: Training loss = 0.3000
2025-04-04 16:00:36,022 - INFO - Epoch 9, Batch 72: Training loss = 0.2900
2025-04-04 16:00:36,123 - INFO - Epoch 9, Batch 73: Training loss = 0.2800
2025-04-04 16:00:36,224 - INFO - Epoch 9, Batch 74: Training loss = 0.2700
2025-04-04 16:00:36,325 - INFO - Epoch 9, Batch 75: Training loss = 0.2600
2025-04-04 16:00:36,426 - INFO - Epoch 9, Batch 76: Training loss = 0.2500
2025-04-04 16:00:36,526 - INFO - Epoch 9, Batch 77: Training loss = 0.2400
2025-04-04 16:00:36,627 - INFO - Epoch 9, Batch 78: Training loss = 0.2300
2025-04-04 16:00:36,728 - INFO - Epoch 9, Batch 79: Training loss = 0.2200
2025-04-04 16:00:36,828 - INFO - Epoch 9, Batch 80: Training loss = 0.2100
2025-04-04 16:00:36,929 - INFO - Epoch 9, Batch 81: Training loss = 0.2000
2025-04-04 16:00:37,030 - INFO - Epoch 9, Batch 82: Training loss = 0.1900
2025-04-04 16:00:37,130 - INFO - Epoch 9, Batch 83: Training loss = 0.1800
2025-04-04 16:00:37,231 - INFO - Epoch 9, Batch 84: Training loss = 0.1700
2025-04-04 16:00:37,333 - INFO - Epoch 9, Batch 85: Training loss = 0.1600
2025-04-04 16:00:37,434 - INFO - Epoch 9, Batch 86: Training loss = 0.1500
2025-04-04 16:00:37,534 - INFO - Epoch 9, Batch 87: Training loss = 0.1400
2025-04-04 16:00:37,636 - INFO - Epoch 9, Batch 88: Training loss = 0.1300
2025-04-04 16:00:37,736 - INFO - Epoch 9, Batch 89: Training loss = 0.1200
2025-04-04 16:00:37,837 - INFO - Epoch 9, Batch 90: Training loss = 0.1100
2025-04-04 16:00:37,938 - INFO - Epoch 9, Batch 91: Training loss = 0.1000
2025-04-04 16:00:38,039 - INFO - Epoch 9, Batch 92: Training loss = 0.0900
2025-04-04 16:00:38,140 - INFO - Epoch 9, Batch 93: Training loss = 0.0800
2025-04-04 16:00:38,241 - INFO - Epoch 9, Batch 94: Training loss = 0.0700
2025-04-04 16:00:38,343 - INFO - Epoch 9, Batch 95: Training loss = 0.0600
2025-04-04 16:00:38,444 - INFO - Epoch 9, Batch 96: Training loss = 0.0500
2025-04-04 16:00:38,544 - INFO - Epoch 9, Batch 97: Training loss = 0.0400
2025-04-04 16:00:38,645 - INFO - Epoch 9, Batch 98: Training loss = 0.0300
2025-04-04 16:00:38,745 - INFO - Epoch 9, Batch 99: Training loss = 0.0200
2025-04-04 16:00:38,846 - INFO - Epoch 9, Batch 100: Training loss = 0.0100
2025-04-04 16:00:38,947 - INFO - Epoch 9: Validation loss = 0.0200, Validation accuracy = 0.9600
2025-04-04 16:00:38,947 - INFO - Epoch 9: MSE = 0.0200, RMSE = 0.1414, MAE = 0.0160, RÂ² = 0.9800
2025-04-04 16:00:38,947 - INFO - Epoch 10 started.
2025-04-04 16:00:38,947 - INFO - Epoch 10, Batch 1: Training loss = 1.0000
2025-04-04 16:00:39,048 - INFO - Epoch 10, Batch 2: Training loss = 0.9900
2025-04-04 16:00:39,149 - INFO - Epoch 10, Batch 3: Training loss = 0.9800
2025-04-04 16:00:39,250 - INFO - Epoch 10, Batch 4: Training loss = 0.9700
2025-04-04 16:00:39,351 - INFO - Epoch 10, Batch 5: Training loss = 0.9600
2025-04-04 16:00:39,452 - INFO - Epoch 10, Batch 6: Training loss = 0.9500
2025-04-04 16:00:39,553 - INFO - Epoch 10, Batch 7: Training loss = 0.9400
2025-04-04 16:00:39,655 - INFO - Epoch 10, Batch 8: Training loss = 0.9300
2025-04-04 16:00:39,756 - INFO - Epoch 10, Batch 9: Training loss = 0.9200
2025-04-04 16:00:39,856 - INFO - Epoch 10, Batch 10: Training loss = 0.9100
2025-04-04 16:00:39,957 - INFO - Epoch 10, Batch 11: Training loss = 0.9000
2025-04-04 16:00:40,058 - INFO - Epoch 10, Batch 12: Training loss = 0.8900
2025-04-04 16:00:40,159 - INFO - Epoch 10, Batch 13: Training loss = 0.8800
2025-04-04 16:00:40,260 - INFO - Epoch 10, Batch 14: Training loss = 0.8700
2025-04-04 16:00:40,361 - INFO - Epoch 10, Batch 15: Training loss = 0.8600
2025-04-04 16:00:40,462 - INFO - Epoch 10, Batch 16: Training loss = 0.8500
2025-04-04 16:00:40,562 - INFO - Epoch 10, Batch 17: Training loss = 0.8400
2025-04-04 16:00:40,663 - INFO - Epoch 10, Batch 18: Training loss = 0.8300
2025-04-04 16:00:40,764 - INFO - Epoch 10, Batch 19: Training loss = 0.8200
2025-04-04 16:00:40,865 - INFO - Epoch 10, Batch 20: Training loss = 0.8100
2025-04-04 16:00:40,966 - INFO - Epoch 10, Batch 21: Training loss = 0.8000
2025-04-04 16:00:41,066 - INFO - Epoch 10, Batch 22: Training loss = 0.7900
2025-04-04 16:00:41,167 - INFO - Epoch 10, Batch 23: Training loss = 0.7800
2025-04-04 16:00:41,268 - INFO - Epoch 10, Batch 24: Training loss = 0.7700
2025-04-04 16:00:41,369 - INFO - Epoch 10, Batch 25: Training loss = 0.7600
2025-04-04 16:00:41,471 - INFO - Epoch 10, Batch 26: Training loss = 0.7500
2025-04-04 16:00:41,572 - INFO - Epoch 10, Batch 27: Training loss = 0.7400
2025-04-04 16:00:41,673 - INFO - Epoch 10, Batch 28: Training loss = 0.7300
2025-04-04 16:00:41,774 - INFO - Epoch 10, Batch 29: Training loss = 0.7200
2025-04-04 16:00:41,874 - INFO - Epoch 10, Batch 30: Training loss = 0.7100
2025-04-04 16:00:41,975 - INFO - Epoch 10, Batch 31: Training loss = 0.7000
2025-04-04 16:00:42,075 - INFO - Epoch 10, Batch 32: Training loss = 0.6900
2025-04-04 16:00:42,176 - INFO - Epoch 10, Batch 33: Training loss = 0.6800
2025-04-04 16:00:42,276 - INFO - Epoch 10, Batch 34: Training loss = 0.6700
2025-04-04 16:00:42,377 - INFO - Epoch 10, Batch 35: Training loss = 0.6600
2025-04-04 16:00:42,477 - INFO - Epoch 10, Batch 36: Training loss = 0.6500
2025-04-04 16:00:42,578 - INFO - Epoch 10, Batch 37: Training loss = 0.6400
2025-04-04 16:00:42,679 - INFO - Epoch 10, Batch 38: Training loss = 0.6300
2025-04-04 16:00:42,780 - INFO - Epoch 10, Batch 39: Training loss = 0.6200
2025-04-04 16:00:42,880 - INFO - Epoch 10, Batch 40: Training loss = 0.6100
2025-04-04 16:00:42,981 - INFO - Epoch 10, Batch 41: Training loss = 0.6000
2025-04-04 16:00:43,082 - INFO - Epoch 10, Batch 42: Training loss = 0.5900
2025-04-04 16:00:43,183 - INFO - Epoch 10, Batch 43: Training loss = 0.5800
2025-04-04 16:00:43,283 - INFO - Epoch 10, Batch 44: Training loss = 0.5700
2025-04-04 16:00:43,384 - INFO - Epoch 10, Batch 45: Training loss = 0.5600
2025-04-04 16:00:43,485 - INFO - Epoch 10, Batch 46: Training loss = 0.5500
2025-04-04 16:00:43,585 - INFO - Epoch 10, Batch 47: Training loss = 0.5400
2025-04-04 16:00:43,687 - INFO - Epoch 10, Batch 48: Training loss = 0.5300
2025-04-04 16:00:43,787 - INFO - Epoch 10, Batch 49: Training loss = 0.5200
2025-04-04 16:00:43,888 - INFO - Epoch 10, Batch 50: Training loss = 0.5100
2025-04-04 16:00:43,989 - INFO - Epoch 10, Batch 51: Training loss = 0.5000
2025-04-04 16:00:44,089 - INFO - Epoch 10, Batch 52: Training loss = 0.4900
2025-04-04 16:00:44,190 - INFO - Epoch 10, Batch 53: Training loss = 0.4800
2025-04-04 16:00:44,291 - INFO - Epoch 10, Batch 54: Training loss = 0.4700
2025-04-04 16:00:44,391 - INFO - Epoch 10, Batch 55: Training loss = 0.4600
2025-04-04 16:00:44,492 - INFO - Epoch 10, Batch 56: Training loss = 0.4500
2025-04-04 16:00:44,593 - INFO - Epoch 10, Batch 57: Training loss = 0.4400
2025-04-04 16:00:44,694 - INFO - Epoch 10, Batch 58: Training loss = 0.4300
2025-04-04 16:00:44,795 - INFO - Epoch 10, Batch 59: Training loss = 0.4200
2025-04-04 16:00:44,896 - INFO - Epoch 10, Batch 60: Training loss = 0.4100
2025-04-04 16:00:44,997 - INFO - Epoch 10, Batch 61: Training loss = 0.4000
2025-04-04 16:00:45,097 - INFO - Epoch 10, Batch 62: Training loss = 0.3900
2025-04-04 16:00:45,198 - INFO - Epoch 10, Batch 63: Training loss = 0.3800
2025-04-04 16:00:45,299 - INFO - Epoch 10, Batch 64: Training loss = 0.3700
2025-04-04 16:00:45,401 - INFO - Epoch 10, Batch 65: Training loss = 0.3600
2025-04-04 16:00:45,502 - INFO - Epoch 10, Batch 66: Training loss = 0.3500
2025-04-04 16:00:45,603 - INFO - Epoch 10, Batch 67: Training loss = 0.3400
2025-04-04 16:00:45,704 - INFO - Epoch 10, Batch 68: Training loss = 0.3300
2025-04-04 16:00:45,804 - INFO - Epoch 10, Batch 69: Training loss = 0.3200
2025-04-04 16:00:45,905 - INFO - Epoch 10, Batch 70: Training loss = 0.3100
2025-04-04 16:00:46,005 - INFO - Epoch 10, Batch 71: Training loss = 0.3000
2025-04-04 16:00:46,106 - INFO - Epoch 10, Batch 72: Training loss = 0.2900
2025-04-04 16:00:46,207 - INFO - Epoch 10, Batch 73: Training loss = 0.2800
2025-04-04 16:00:46,308 - INFO - Epoch 10, Batch 74: Training loss = 0.2700
2025-04-04 16:00:46,408 - INFO - Epoch 10, Batch 75: Training loss = 0.2600
2025-04-04 16:00:46,509 - INFO - Epoch 10, Batch 76: Training loss = 0.2500
2025-04-04 16:00:46,610 - INFO - Epoch 10, Batch 77: Training loss = 0.2400
2025-04-04 16:00:46,710 - INFO - Epoch 10, Batch 78: Training loss = 0.2300
2025-04-04 16:00:46,811 - INFO - Epoch 10, Batch 79: Training loss = 0.2200
2025-04-04 16:00:46,912 - INFO - Epoch 10, Batch 80: Training loss = 0.2100
2025-04-04 16:00:47,013 - INFO - Epoch 10, Batch 81: Training loss = 0.2000
2025-04-04 16:00:47,114 - INFO - Epoch 10, Batch 82: Training loss = 0.1900
2025-04-04 16:00:47,215 - INFO - Epoch 10, Batch 83: Training loss = 0.1800
2025-04-04 16:00:47,316 - INFO - Epoch 10, Batch 84: Training loss = 0.1700
2025-04-04 16:00:47,417 - INFO - Epoch 10, Batch 85: Training loss = 0.1600
2025-04-04 16:00:47,518 - INFO - Epoch 10, Batch 86: Training loss = 0.1500
2025-04-04 16:00:47,619 - INFO - Epoch 10, Batch 87: Training loss = 0.1400
2025-04-04 16:00:47,720 - INFO - Epoch 10, Batch 88: Training loss = 0.1300
2025-04-04 16:00:47,820 - INFO - Epoch 10, Batch 89: Training loss = 0.1200
2025-04-04 16:00:47,921 - INFO - Epoch 10, Batch 90: Training loss = 0.1100
2025-04-04 16:00:48,021 - INFO - Epoch 10, Batch 91: Training loss = 0.1000
2025-04-04 16:00:48,122 - INFO - Epoch 10, Batch 92: Training loss = 0.0900
2025-04-04 16:00:48,222 - INFO - Epoch 10, Batch 93: Training loss = 0.0800
2025-04-04 16:00:48,323 - INFO - Epoch 10, Batch 94: Training loss = 0.0700
2025-04-04 16:00:48,424 - INFO - Epoch 10, Batch 95: Training loss = 0.0600
2025-04-04 16:00:48,524 - INFO - Epoch 10, Batch 96: Training loss = 0.0500
2025-04-04 16:00:48,625 - INFO - Epoch 10, Batch 97: Training loss = 0.0400
2025-04-04 16:00:48,726 - INFO - Epoch 10, Batch 98: Training loss = 0.0300
2025-04-04 16:00:48,826 - INFO - Epoch 10, Batch 99: Training loss = 0.0200
2025-04-04 16:00:48,927 - INFO - Epoch 10, Batch 100: Training loss = 0.0100
2025-04-04 16:00:49,028 - INFO - Epoch 10: Validation loss = 0.0100, Validation accuracy = 0.9800
2025-04-04 16:00:49,028 - INFO - Epoch 10: MSE = 0.0100, RMSE = 0.1000, MAE = 0.0080, RÂ² = 0.9900
2025-04-04 16:00:49,029 - INFO - Regression training process completed.
2025-04-04 16:13:42,021 - INFO - Starting regression training process...
2025-04-04 16:13:42,021 - INFO - Epoch 1 started.
2025-04-04 16:13:42,021 - INFO - Epoch 1, Batch 1: Training loss = 1.0000
2025-04-04 16:13:42,121 - INFO - Epoch 1, Batch 2: Training loss = 0.9900
2025-04-04 16:13:42,222 - INFO - Epoch 1, Batch 3: Training loss = 0.9800
2025-04-04 16:13:42,323 - INFO - Epoch 1, Batch 4: Training loss = 0.9700
2025-04-04 16:13:42,423 - INFO - Epoch 1, Batch 5: Training loss = 0.9600
2025-04-04 16:13:42,524 - INFO - Epoch 1, Batch 6: Training loss = 0.9500
2025-04-04 16:13:42,625 - INFO - Epoch 1, Batch 7: Training loss = 0.9400
2025-04-04 16:13:42,725 - INFO - Epoch 1, Batch 8: Training loss = 0.9300
2025-04-04 16:13:42,825 - INFO - Epoch 1, Batch 9: Training loss = 0.9200
2025-04-04 16:13:42,926 - INFO - Epoch 1, Batch 10: Training loss = 0.9100
2025-04-04 16:13:43,027 - INFO - Epoch 1, Batch 11: Training loss = 0.9000
2025-04-04 16:13:43,128 - INFO - Epoch 1, Batch 12: Training loss = 0.8900
2025-04-04 16:13:43,228 - INFO - Epoch 1, Batch 13: Training loss = 0.8800
2025-04-04 16:13:43,329 - INFO - Epoch 1, Batch 14: Training loss = 0.8700
2025-04-04 16:13:43,430 - INFO - Epoch 1, Batch 15: Training loss = 0.8600
2025-04-04 16:13:43,531 - INFO - Epoch 1, Batch 16: Training loss = 0.8500
2025-04-04 16:13:43,631 - INFO - Epoch 1, Batch 17: Training loss = 0.8400
2025-04-04 16:13:43,732 - INFO - Epoch 1, Batch 18: Training loss = 0.8300
2025-04-04 16:13:43,832 - INFO - Epoch 1, Batch 19: Training loss = 0.8200
2025-04-04 16:13:43,933 - INFO - Epoch 1, Batch 20: Training loss = 0.8100
2025-04-04 16:13:44,033 - INFO - Epoch 1, Batch 21: Training loss = 0.8000
2025-04-04 16:13:44,134 - INFO - Epoch 1, Batch 22: Training loss = 0.7900
2025-04-04 16:13:44,235 - INFO - Epoch 1, Batch 23: Training loss = 0.7800
2025-04-04 16:13:44,336 - INFO - Epoch 1, Batch 24: Training loss = 0.7700
2025-04-04 16:13:44,437 - INFO - Epoch 1, Batch 25: Training loss = 0.7600
2025-04-04 16:13:44,538 - INFO - Epoch 1, Batch 26: Training loss = 0.7500
2025-04-04 16:13:44,639 - INFO - Epoch 1, Batch 27: Training loss = 0.7400
2025-04-04 16:13:44,740 - INFO - Epoch 1, Batch 28: Training loss = 0.7300
2025-04-04 16:13:44,841 - INFO - Epoch 1, Batch 29: Training loss = 0.7200
2025-04-04 16:13:44,941 - INFO - Epoch 1, Batch 30: Training loss = 0.7100
2025-04-04 16:13:45,042 - INFO - Epoch 1, Batch 31: Training loss = 0.7000
2025-04-04 16:13:45,143 - INFO - Epoch 1, Batch 32: Training loss = 0.6900
2025-04-04 16:13:45,244 - INFO - Epoch 1, Batch 33: Training loss = 0.6800
2025-04-04 16:13:45,345 - INFO - Epoch 1, Batch 34: Training loss = 0.6700
2025-04-04 16:13:45,445 - INFO - Epoch 1, Batch 35: Training loss = 0.6600
2025-04-04 16:13:45,546 - INFO - Epoch 1, Batch 36: Training loss = 0.6500
2025-04-04 16:13:45,648 - INFO - Epoch 1, Batch 37: Training loss = 0.6400
2025-04-04 16:13:45,749 - INFO - Epoch 1, Batch 38: Training loss = 0.6300
2025-04-04 16:13:45,849 - INFO - Epoch 1, Batch 39: Training loss = 0.6200
2025-04-04 16:13:45,950 - INFO - Epoch 1, Batch 40: Training loss = 0.6100
2025-04-04 16:13:46,051 - INFO - Epoch 1, Batch 41: Training loss = 0.6000
2025-04-04 16:13:46,152 - INFO - Epoch 1, Batch 42: Training loss = 0.5900
2025-04-04 16:13:46,252 - INFO - Epoch 1, Batch 43: Training loss = 0.5800
2025-04-04 16:13:46,353 - INFO - Epoch 1, Batch 44: Training loss = 0.5700
2025-04-04 16:13:46,454 - INFO - Epoch 1, Batch 45: Training loss = 0.5600
2025-04-04 16:13:46,555 - INFO - Epoch 1, Batch 46: Training loss = 0.5500
2025-04-04 16:13:46,655 - INFO - Epoch 1, Batch 47: Training loss = 0.5400
2025-04-04 16:13:46,756 - INFO - Epoch 1, Batch 48: Training loss = 0.5300
2025-04-04 16:13:46,856 - INFO - Epoch 1, Batch 49: Training loss = 0.5200
2025-04-04 16:13:46,957 - INFO - Epoch 1, Batch 50: Training loss = 0.5100
2025-04-04 16:13:47,057 - INFO - Epoch 1, Batch 51: Training loss = 0.5000
2025-04-04 16:13:47,158 - INFO - Epoch 1, Batch 52: Training loss = 0.4900
2025-04-04 16:13:47,259 - INFO - Epoch 1, Batch 53: Training loss = 0.4800
2025-04-04 16:13:47,359 - INFO - Epoch 1, Batch 54: Training loss = 0.4700
2025-04-04 16:13:47,460 - INFO - Epoch 1, Batch 55: Training loss = 0.4600
2025-04-04 16:13:47,560 - INFO - Epoch 1, Batch 56: Training loss = 0.4500
2025-04-04 16:13:47,661 - INFO - Epoch 1, Batch 57: Training loss = 0.4400
2025-04-04 16:13:47,761 - INFO - Epoch 1, Batch 58: Training loss = 0.4300
2025-04-04 16:13:47,862 - INFO - Epoch 1, Batch 59: Training loss = 0.4200
2025-04-04 16:13:47,963 - INFO - Epoch 1, Batch 60: Training loss = 0.4100
2025-04-04 16:13:48,063 - INFO - Epoch 1, Batch 61: Training loss = 0.4000
2025-04-04 16:13:48,164 - INFO - Epoch 1, Batch 62: Training loss = 0.3900
2025-04-04 16:13:48,265 - INFO - Epoch 1, Batch 63: Training loss = 0.3800
2025-04-04 16:13:48,366 - INFO - Epoch 1, Batch 64: Training loss = 0.3700
2025-04-04 16:13:48,466 - INFO - Epoch 1, Batch 65: Training loss = 0.3600
2025-04-04 16:13:48,567 - INFO - Epoch 1, Batch 66: Training loss = 0.3500
2025-04-04 16:13:48,668 - INFO - Epoch 1, Batch 67: Training loss = 0.3400
2025-04-04 16:13:48,769 - INFO - Epoch 1, Batch 68: Training loss = 0.3300
2025-04-04 16:13:48,869 - INFO - Epoch 1, Batch 69: Training loss = 0.3200
2025-04-04 16:13:48,970 - INFO - Epoch 1, Batch 70: Training loss = 0.3100
2025-04-04 16:13:49,070 - INFO - Epoch 1, Batch 71: Training loss = 0.3000
2025-04-04 16:13:49,172 - INFO - Epoch 1, Batch 72: Training loss = 0.2900
2025-04-04 16:13:49,273 - INFO - Epoch 1, Batch 73: Training loss = 0.2800
2025-04-04 16:13:49,374 - INFO - Epoch 1, Batch 74: Training loss = 0.2700
2025-04-04 16:13:49,474 - INFO - Epoch 1, Batch 75: Training loss = 0.2600
2025-04-04 16:13:49,575 - INFO - Epoch 1, Batch 76: Training loss = 0.2500
2025-04-04 16:13:49,676 - INFO - Epoch 1, Batch 77: Training loss = 0.2400
2025-04-04 16:13:49,777 - INFO - Epoch 1, Batch 78: Training loss = 0.2300
2025-04-04 16:13:49,878 - INFO - Epoch 1, Batch 79: Training loss = 0.2200
2025-04-04 16:13:49,978 - INFO - Epoch 1, Batch 80: Training loss = 0.2100
2025-04-04 16:13:50,079 - INFO - Epoch 1, Batch 81: Training loss = 0.2000
2025-04-04 16:13:50,179 - INFO - Epoch 1, Batch 82: Training loss = 0.1900
2025-04-04 16:13:50,280 - INFO - Epoch 1, Batch 83: Training loss = 0.1800
2025-04-04 16:13:50,381 - INFO - Epoch 1, Batch 84: Training loss = 0.1700
2025-04-04 16:13:50,482 - INFO - Epoch 1, Batch 85: Training loss = 0.1600
2025-04-04 16:13:50,583 - INFO - Epoch 1, Batch 86: Training loss = 0.1500
2025-04-04 16:13:50,683 - INFO - Epoch 1, Batch 87: Training loss = 0.1400
2025-04-04 16:13:50,784 - INFO - Epoch 1, Batch 88: Training loss = 0.1300
2025-04-04 16:13:50,885 - INFO - Epoch 1, Batch 89: Training loss = 0.1200
2025-04-04 16:13:50,986 - INFO - Epoch 1, Batch 90: Training loss = 0.1100
2025-04-04 16:13:51,086 - INFO - Epoch 1, Batch 91: Training loss = 0.1000
2025-04-04 16:13:51,187 - INFO - Epoch 1, Batch 92: Training loss = 0.0900
2025-04-04 16:13:51,288 - INFO - Epoch 1, Batch 93: Training loss = 0.0800
2025-04-04 16:13:51,389 - INFO - Epoch 1, Batch 94: Training loss = 0.0700
2025-04-04 16:13:51,490 - INFO - Epoch 1, Batch 95: Training loss = 0.0600
2025-04-04 16:13:51,590 - INFO - Epoch 1, Batch 96: Training loss = 0.0500
2025-04-04 16:13:51,691 - INFO - Epoch 1, Batch 97: Training loss = 0.0400
2025-04-04 16:13:51,792 - INFO - Epoch 1, Batch 98: Training loss = 0.0300
2025-04-04 16:13:51,893 - INFO - Epoch 1, Batch 99: Training loss = 0.0200
2025-04-04 16:13:51,993 - INFO - Epoch 1, Batch 100: Training loss = 0.0100
2025-04-04 16:13:52,094 - INFO - Epoch 1: Validation loss = 0.1000, Validation accuracy = 0.8000
2025-04-04 16:13:52,095 - INFO - Epoch 1: MSE = 0.1000, RMSE = 0.3162, MAE = 0.0800, RÂ² = 0.9000
2025-04-04 16:13:52,095 - INFO - Epoch 2 started.
2025-04-04 16:13:52,095 - INFO - Epoch 2, Batch 1: Training loss = 1.0000
2025-04-04 16:13:52,195 - INFO - Epoch 2, Batch 2: Training loss = 0.9900
2025-04-04 16:13:52,296 - INFO - Epoch 2, Batch 3: Training loss = 0.9800
2025-04-04 16:13:52,397 - INFO - Epoch 2, Batch 4: Training loss = 0.9700
2025-04-04 16:13:52,497 - INFO - Epoch 2, Batch 5: Training loss = 0.9600
2025-04-04 16:13:52,598 - INFO - Epoch 2, Batch 6: Training loss = 0.9500
2025-04-04 16:13:52,699 - INFO - Epoch 2, Batch 7: Training loss = 0.9400
2025-04-04 16:13:52,799 - INFO - Epoch 2, Batch 8: Training loss = 0.9300
2025-04-04 16:13:52,900 - INFO - Epoch 2, Batch 9: Training loss = 0.9200
2025-04-04 16:13:53,000 - INFO - Epoch 2, Batch 10: Training loss = 0.9100
2025-04-04 16:13:53,101 - INFO - Epoch 2, Batch 11: Training loss = 0.9000
2025-04-04 16:13:53,201 - INFO - Epoch 2, Batch 12: Training loss = 0.8900
2025-04-04 16:13:53,302 - INFO - Epoch 2, Batch 13: Training loss = 0.8800
2025-04-04 16:13:53,403 - INFO - Epoch 2, Batch 14: Training loss = 0.8700
2025-04-04 16:13:53,504 - INFO - Epoch 2, Batch 15: Training loss = 0.8600
2025-04-04 16:13:53,604 - INFO - Epoch 2, Batch 16: Training loss = 0.8500
2025-04-04 16:13:53,705 - INFO - Epoch 2, Batch 17: Training loss = 0.8400
2025-04-04 16:13:53,806 - INFO - Epoch 2, Batch 18: Training loss = 0.8300
2025-04-04 16:13:53,906 - INFO - Epoch 2, Batch 19: Training loss = 0.8200
2025-04-04 16:13:54,007 - INFO - Epoch 2, Batch 20: Training loss = 0.8100
2025-04-04 16:13:54,107 - INFO - Epoch 2, Batch 21: Training loss = 0.8000
2025-04-04 16:13:54,208 - INFO - Epoch 2, Batch 22: Training loss = 0.7900
2025-04-04 16:13:54,309 - INFO - Epoch 2, Batch 23: Training loss = 0.7800
2025-04-04 16:13:54,410 - INFO - Epoch 2, Batch 24: Training loss = 0.7700
2025-04-04 16:13:54,510 - INFO - Epoch 2, Batch 25: Training loss = 0.7600
2025-04-04 16:13:54,611 - INFO - Epoch 2, Batch 26: Training loss = 0.7500
2025-04-04 16:13:54,711 - INFO - Epoch 2, Batch 27: Training loss = 0.7400
2025-04-04 16:13:54,812 - INFO - Epoch 2, Batch 28: Training loss = 0.7300
2025-04-04 16:13:54,912 - INFO - Epoch 2, Batch 29: Training loss = 0.7200
2025-04-04 16:13:55,012 - INFO - Epoch 2, Batch 30: Training loss = 0.7100
2025-04-04 16:13:55,113 - INFO - Epoch 2, Batch 31: Training loss = 0.7000
2025-04-04 16:13:55,213 - INFO - Epoch 2, Batch 32: Training loss = 0.6900
2025-04-04 16:13:55,314 - INFO - Epoch 2, Batch 33: Training loss = 0.6800
2025-04-04 16:13:55,414 - INFO - Epoch 2, Batch 34: Training loss = 0.6700
2025-04-04 16:13:55,515 - INFO - Epoch 2, Batch 35: Training loss = 0.6600
2025-04-04 16:13:55,615 - INFO - Epoch 2, Batch 36: Training loss = 0.6500
2025-04-04 16:13:55,716 - INFO - Epoch 2, Batch 37: Training loss = 0.6400
2025-04-04 16:13:55,816 - INFO - Epoch 2, Batch 38: Training loss = 0.6300
2025-04-04 16:13:55,917 - INFO - Epoch 2, Batch 39: Training loss = 0.6200
2025-04-04 16:13:56,018 - INFO - Epoch 2, Batch 40: Training loss = 0.6100
2025-04-04 16:13:56,119 - INFO - Epoch 2, Batch 41: Training loss = 0.6000
2025-04-04 16:13:56,220 - INFO - Epoch 2, Batch 42: Training loss = 0.5900
2025-04-04 16:13:56,321 - INFO - Epoch 2, Batch 43: Training loss = 0.5800
2025-04-04 16:13:56,421 - INFO - Epoch 2, Batch 44: Training loss = 0.5700
2025-04-04 16:13:56,522 - INFO - Epoch 2, Batch 45: Training loss = 0.5600
2025-04-04 16:13:56,622 - INFO - Epoch 2, Batch 46: Training loss = 0.5500
2025-04-04 16:13:56,722 - INFO - Epoch 2, Batch 47: Training loss = 0.5400
2025-04-04 16:13:56,823 - INFO - Epoch 2, Batch 48: Training loss = 0.5300
2025-04-04 16:13:56,923 - INFO - Epoch 2, Batch 49: Training loss = 0.5200
2025-04-04 16:13:57,024 - INFO - Epoch 2, Batch 50: Training loss = 0.5100
2025-04-04 16:13:57,124 - INFO - Epoch 2, Batch 51: Training loss = 0.5000
2025-04-04 16:13:57,225 - INFO - Epoch 2, Batch 52: Training loss = 0.4900
2025-04-04 16:13:57,325 - INFO - Epoch 2, Batch 53: Training loss = 0.4800
2025-04-04 16:13:57,426 - INFO - Epoch 2, Batch 54: Training loss = 0.4700
2025-04-04 16:13:57,526 - INFO - Epoch 2, Batch 55: Training loss = 0.4600
2025-04-04 16:13:57,627 - INFO - Epoch 2, Batch 56: Training loss = 0.4500
2025-04-04 16:13:57,727 - INFO - Epoch 2, Batch 57: Training loss = 0.4400
2025-04-04 16:13:57,828 - INFO - Epoch 2, Batch 58: Training loss = 0.4300
2025-04-04 16:13:57,928 - INFO - Epoch 2, Batch 59: Training loss = 0.4200
2025-04-04 16:13:58,028 - INFO - Epoch 2, Batch 60: Training loss = 0.4100
2025-04-04 16:13:58,129 - INFO - Epoch 2, Batch 61: Training loss = 0.4000
2025-04-04 16:13:58,230 - INFO - Epoch 2, Batch 62: Training loss = 0.3900
2025-04-04 16:13:58,331 - INFO - Epoch 2, Batch 63: Training loss = 0.3800
2025-04-04 16:13:58,431 - INFO - Epoch 2, Batch 64: Training loss = 0.3700
2025-04-04 16:13:58,532 - INFO - Epoch 2, Batch 65: Training loss = 0.3600
2025-04-04 16:13:58,632 - INFO - Epoch 2, Batch 66: Training loss = 0.3500
2025-04-04 16:13:58,732 - INFO - Epoch 2, Batch 67: Training loss = 0.3400
2025-04-04 16:13:58,833 - INFO - Epoch 2, Batch 68: Training loss = 0.3300
2025-04-04 16:13:58,934 - INFO - Epoch 2, Batch 69: Training loss = 0.3200
2025-04-04 16:13:59,034 - INFO - Epoch 2, Batch 70: Training loss = 0.3100
2025-04-04 16:13:59,135 - INFO - Epoch 2, Batch 71: Training loss = 0.3000
2025-04-04 16:13:59,235 - INFO - Epoch 2, Batch 72: Training loss = 0.2900
2025-04-04 16:13:59,336 - INFO - Epoch 2, Batch 73: Training loss = 0.2800
2025-04-04 16:13:59,437 - INFO - Epoch 2, Batch 74: Training loss = 0.2700
2025-04-04 16:13:59,538 - INFO - Epoch 2, Batch 75: Training loss = 0.2600
2025-04-04 16:13:59,638 - INFO - Epoch 2, Batch 76: Training loss = 0.2500
2025-04-04 16:13:59,739 - INFO - Epoch 2, Batch 77: Training loss = 0.2400
2025-04-04 16:13:59,839 - INFO - Epoch 2, Batch 78: Training loss = 0.2300
2025-04-04 16:13:59,940 - INFO - Epoch 2, Batch 79: Training loss = 0.2200
2025-04-04 16:14:00,040 - INFO - Epoch 2, Batch 80: Training loss = 0.2100
2025-04-04 16:14:00,141 - INFO - Epoch 2, Batch 81: Training loss = 0.2000
2025-04-04 16:14:00,242 - INFO - Epoch 2, Batch 82: Training loss = 0.1900
2025-04-04 16:14:00,342 - INFO - Epoch 2, Batch 83: Training loss = 0.1800
2025-04-04 16:14:00,442 - INFO - Epoch 2, Batch 84: Training loss = 0.1700
2025-04-04 16:14:00,543 - INFO - Epoch 2, Batch 85: Training loss = 0.1600
2025-04-04 16:14:00,644 - INFO - Epoch 2, Batch 86: Training loss = 0.1500
2025-04-04 16:14:00,744 - INFO - Epoch 2, Batch 87: Training loss = 0.1400
2025-04-04 16:14:00,845 - INFO - Epoch 2, Batch 88: Training loss = 0.1300
2025-04-04 16:14:00,945 - INFO - Epoch 2, Batch 89: Training loss = 0.1200
2025-04-04 16:14:01,046 - INFO - Epoch 2, Batch 90: Training loss = 0.1100
2025-04-04 16:14:01,146 - INFO - Epoch 2, Batch 91: Training loss = 0.1000
2025-04-04 16:14:01,247 - INFO - Epoch 2, Batch 92: Training loss = 0.0900
2025-04-04 16:14:01,348 - INFO - Epoch 2, Batch 93: Training loss = 0.0800
2025-04-04 16:14:01,448 - INFO - Epoch 2, Batch 94: Training loss = 0.0700
2025-04-04 16:14:01,549 - INFO - Epoch 2, Batch 95: Training loss = 0.0600
2025-04-04 16:14:01,650 - INFO - Epoch 2, Batch 96: Training loss = 0.0500
2025-04-04 16:14:01,751 - INFO - Epoch 2, Batch 97: Training loss = 0.0400
2025-04-04 16:14:01,851 - INFO - Epoch 2, Batch 98: Training loss = 0.0300
2025-04-04 16:14:01,952 - INFO - Epoch 2, Batch 99: Training loss = 0.0200
2025-04-04 16:14:02,053 - INFO - Epoch 2, Batch 100: Training loss = 0.0100
2025-04-04 16:14:02,154 - INFO - Epoch 2: Validation loss = 0.0900, Validation accuracy = 0.8200
2025-04-04 16:14:02,155 - INFO - Epoch 2: MSE = 0.0900, RMSE = 0.3000, MAE = 0.0720, RÂ² = 0.9100
2025-04-04 16:14:02,155 - INFO - Epoch 3 started.
2025-04-04 16:14:02,155 - INFO - Epoch 3, Batch 1: Training loss = 1.0000
2025-04-04 16:14:02,256 - INFO - Epoch 3, Batch 2: Training loss = 0.9900
2025-04-04 16:14:02,356 - INFO - Epoch 3, Batch 3: Training loss = 0.9800
2025-04-04 16:14:02,457 - INFO - Epoch 3, Batch 4: Training loss = 0.9700
2025-04-04 16:14:02,557 - INFO - Epoch 3, Batch 5: Training loss = 0.9600
2025-04-04 16:14:02,658 - INFO - Epoch 3, Batch 6: Training loss = 0.9500
2025-04-04 16:14:02,758 - INFO - Epoch 3, Batch 7: Training loss = 0.9400
2025-04-04 16:14:02,859 - INFO - Epoch 3, Batch 8: Training loss = 0.9300
2025-04-04 16:14:02,959 - INFO - Epoch 3, Batch 9: Training loss = 0.9200
2025-04-04 16:14:03,060 - INFO - Epoch 3, Batch 10: Training loss = 0.9100
2025-04-04 16:14:03,160 - INFO - Epoch 3, Batch 11: Training loss = 0.9000
2025-04-04 16:14:03,260 - INFO - Epoch 3, Batch 12: Training loss = 0.8900
2025-04-04 16:14:03,361 - INFO - Epoch 3, Batch 13: Training loss = 0.8800
2025-04-04 16:14:03,461 - INFO - Epoch 3, Batch 14: Training loss = 0.8700
2025-04-04 16:14:03,562 - INFO - Epoch 3, Batch 15: Training loss = 0.8600
2025-04-04 16:14:03,662 - INFO - Epoch 3, Batch 16: Training loss = 0.8500
2025-04-04 16:14:03,762 - INFO - Epoch 3, Batch 17: Training loss = 0.8400
2025-04-04 16:14:03,863 - INFO - Epoch 3, Batch 18: Training loss = 0.8300
2025-04-04 16:14:03,963 - INFO - Epoch 3, Batch 19: Training loss = 0.8200
2025-04-04 16:14:04,064 - INFO - Epoch 3, Batch 20: Training loss = 0.8100
2025-04-04 16:14:04,165 - INFO - Epoch 3, Batch 21: Training loss = 0.8000
2025-04-04 16:14:04,266 - INFO - Epoch 3, Batch 22: Training loss = 0.7900
2025-04-04 16:14:04,367 - INFO - Epoch 3, Batch 23: Training loss = 0.7800
2025-04-04 16:14:04,468 - INFO - Epoch 3, Batch 24: Training loss = 0.7700
2025-04-04 16:14:04,569 - INFO - Epoch 3, Batch 25: Training loss = 0.7600
2025-04-04 16:14:04,670 - INFO - Epoch 3, Batch 26: Training loss = 0.7500
2025-04-04 16:14:04,770 - INFO - Epoch 3, Batch 27: Training loss = 0.7400
2025-04-04 16:14:04,871 - INFO - Epoch 3, Batch 28: Training loss = 0.7300
2025-04-04 16:14:04,972 - INFO - Epoch 3, Batch 29: Training loss = 0.7200
2025-04-04 16:14:05,072 - INFO - Epoch 3, Batch 30: Training loss = 0.7100
2025-04-04 16:14:05,173 - INFO - Epoch 3, Batch 31: Training loss = 0.7000
2025-04-04 16:14:05,274 - INFO - Epoch 3, Batch 32: Training loss = 0.6900
2025-04-04 16:14:05,375 - INFO - Epoch 3, Batch 33: Training loss = 0.6800
2025-04-04 16:14:05,475 - INFO - Epoch 3, Batch 34: Training loss = 0.6700
2025-04-04 16:14:05,576 - INFO - Epoch 3, Batch 35: Training loss = 0.6600
2025-04-04 16:14:05,676 - INFO - Epoch 3, Batch 36: Training loss = 0.6500
2025-04-04 16:14:05,777 - INFO - Epoch 3, Batch 37: Training loss = 0.6400
2025-04-04 16:14:05,878 - INFO - Epoch 3, Batch 38: Training loss = 0.6300
2025-04-04 16:14:05,978 - INFO - Epoch 3, Batch 39: Training loss = 0.6200
2025-04-04 16:14:06,079 - INFO - Epoch 3, Batch 40: Training loss = 0.6100
2025-04-04 16:14:06,180 - INFO - Epoch 3, Batch 41: Training loss = 0.6000
2025-04-04 16:14:06,281 - INFO - Epoch 3, Batch 42: Training loss = 0.5900
2025-04-04 16:14:06,381 - INFO - Epoch 3, Batch 43: Training loss = 0.5800
2025-04-04 16:14:06,482 - INFO - Epoch 3, Batch 44: Training loss = 0.5700
2025-04-04 16:14:06,582 - INFO - Epoch 3, Batch 45: Training loss = 0.5600
2025-04-04 16:14:06,683 - INFO - Epoch 3, Batch 46: Training loss = 0.5500
2025-04-04 16:14:06,784 - INFO - Epoch 3, Batch 47: Training loss = 0.5400
2025-04-04 16:14:06,884 - INFO - Epoch 3, Batch 48: Training loss = 0.5300
2025-04-04 16:14:06,984 - INFO - Epoch 3, Batch 49: Training loss = 0.5200
2025-04-04 16:14:07,085 - INFO - Epoch 3, Batch 50: Training loss = 0.5100
2025-04-04 16:14:07,186 - INFO - Epoch 3, Batch 51: Training loss = 0.5000
2025-04-04 16:14:07,286 - INFO - Epoch 3, Batch 52: Training loss = 0.4900
2025-04-04 16:14:07,387 - INFO - Epoch 3, Batch 53: Training loss = 0.4800
2025-04-04 16:14:07,488 - INFO - Epoch 3, Batch 54: Training loss = 0.4700
2025-04-04 16:14:07,588 - INFO - Epoch 3, Batch 55: Training loss = 0.4600
2025-04-04 16:14:07,689 - INFO - Epoch 3, Batch 56: Training loss = 0.4500
2025-04-04 16:14:07,789 - INFO - Epoch 3, Batch 57: Training loss = 0.4400
2025-04-04 16:14:07,890 - INFO - Epoch 3, Batch 58: Training loss = 0.4300
2025-04-04 16:14:07,990 - INFO - Epoch 3, Batch 59: Training loss = 0.4200
2025-04-04 16:14:08,091 - INFO - Epoch 3, Batch 60: Training loss = 0.4100
2025-04-04 16:14:08,191 - INFO - Epoch 3, Batch 61: Training loss = 0.4000
2025-04-04 16:14:08,292 - INFO - Epoch 3, Batch 62: Training loss = 0.3900
2025-04-04 16:14:08,392 - INFO - Epoch 3, Batch 63: Training loss = 0.3800
2025-04-04 16:14:08,493 - INFO - Epoch 3, Batch 64: Training loss = 0.3700
2025-04-04 16:14:08,593 - INFO - Epoch 3, Batch 65: Training loss = 0.3600
2025-04-04 16:14:08,694 - INFO - Epoch 3, Batch 66: Training loss = 0.3500
2025-04-04 16:14:08,795 - INFO - Epoch 3, Batch 67: Training loss = 0.3400
2025-04-04 16:14:08,895 - INFO - Epoch 3, Batch 68: Training loss = 0.3300
2025-04-04 16:14:08,996 - INFO - Epoch 3, Batch 69: Training loss = 0.3200
2025-04-04 16:14:09,096 - INFO - Epoch 3, Batch 70: Training loss = 0.3100
2025-04-04 16:14:09,197 - INFO - Epoch 3, Batch 71: Training loss = 0.3000
2025-04-04 16:14:09,298 - INFO - Epoch 3, Batch 72: Training loss = 0.2900
2025-04-04 16:14:09,398 - INFO - Epoch 3, Batch 73: Training loss = 0.2800
2025-04-04 16:14:09,499 - INFO - Epoch 3, Batch 74: Training loss = 0.2700
2025-04-04 16:14:09,600 - INFO - Epoch 3, Batch 75: Training loss = 0.2600
2025-04-04 16:14:09,700 - INFO - Epoch 3, Batch 76: Training loss = 0.2500
2025-04-04 16:14:09,801 - INFO - Epoch 3, Batch 77: Training loss = 0.2400
2025-04-04 16:14:09,901 - INFO - Epoch 3, Batch 78: Training loss = 0.2300
2025-04-04 16:14:10,002 - INFO - Epoch 3, Batch 79: Training loss = 0.2200
2025-04-04 16:14:10,103 - INFO - Epoch 3, Batch 80: Training loss = 0.2100
2025-04-04 16:14:10,204 - INFO - Epoch 3, Batch 81: Training loss = 0.2000
2025-04-04 16:14:10,304 - INFO - Epoch 3, Batch 82: Training loss = 0.1900
2025-04-04 16:14:10,405 - INFO - Epoch 3, Batch 83: Training loss = 0.1800
2025-04-04 16:14:10,506 - INFO - Epoch 3, Batch 84: Training loss = 0.1700
2025-04-04 16:14:10,606 - INFO - Epoch 3, Batch 85: Training loss = 0.1600
2025-04-04 16:14:10,707 - INFO - Epoch 3, Batch 86: Training loss = 0.1500
2025-04-04 16:14:10,808 - INFO - Epoch 3, Batch 87: Training loss = 0.1400
2025-04-04 16:14:10,909 - INFO - Epoch 3, Batch 88: Training loss = 0.1300
2025-04-04 16:14:11,010 - INFO - Epoch 3, Batch 89: Training loss = 0.1200
2025-04-04 16:14:11,110 - INFO - Epoch 3, Batch 90: Training loss = 0.1100
2025-04-04 16:14:11,211 - INFO - Epoch 3, Batch 91: Training loss = 0.1000
2025-04-04 16:14:11,312 - INFO - Epoch 3, Batch 92: Training loss = 0.0900
2025-04-04 16:14:11,413 - INFO - Epoch 3, Batch 93: Training loss = 0.0800
2025-04-04 16:14:11,514 - INFO - Epoch 3, Batch 94: Training loss = 0.0700
2025-04-04 16:14:11,614 - INFO - Epoch 3, Batch 95: Training loss = 0.0600
2025-04-04 16:14:11,715 - INFO - Epoch 3, Batch 96: Training loss = 0.0500
2025-04-04 16:14:11,816 - INFO - Epoch 3, Batch 97: Training loss = 0.0400
2025-04-04 16:14:11,916 - INFO - Epoch 3, Batch 98: Training loss = 0.0300
2025-04-04 16:14:12,017 - INFO - Epoch 3, Batch 99: Training loss = 0.0200
2025-04-04 16:14:12,117 - INFO - Epoch 3, Batch 100: Training loss = 0.0100
2025-04-04 16:14:12,218 - INFO - Epoch 3: Validation loss = 0.0800, Validation accuracy = 0.8400
2025-04-04 16:14:12,219 - INFO - Epoch 3: MSE = 0.0800, RMSE = 0.2828, MAE = 0.0640, RÂ² = 0.9200
2025-04-04 16:14:12,219 - INFO - Epoch 4 started.
2025-04-04 16:14:12,220 - INFO - Epoch 4, Batch 1: Training loss = 1.0000
2025-04-04 16:14:12,320 - INFO - Epoch 4, Batch 2: Training loss = 0.9900
2025-04-04 16:14:12,421 - INFO - Epoch 4, Batch 3: Training loss = 0.9800
2025-04-04 16:14:12,522 - INFO - Epoch 4, Batch 4: Training loss = 0.9700
2025-04-04 16:14:12,622 - INFO - Epoch 4, Batch 5: Training loss = 0.9600
2025-04-04 16:14:12,723 - INFO - Epoch 4, Batch 6: Training loss = 0.9500
2025-04-04 16:14:12,824 - INFO - Epoch 4, Batch 7: Training loss = 0.9400
2025-04-04 16:14:12,924 - INFO - Epoch 4, Batch 8: Training loss = 0.9300
2025-04-04 16:14:13,025 - INFO - Epoch 4, Batch 9: Training loss = 0.9200
2025-04-04 16:14:13,125 - INFO - Epoch 4, Batch 10: Training loss = 0.9100
2025-04-04 16:14:13,226 - INFO - Epoch 4, Batch 11: Training loss = 0.9000
2025-04-04 16:14:13,326 - INFO - Epoch 4, Batch 12: Training loss = 0.8900
2025-04-04 16:14:13,427 - INFO - Epoch 4, Batch 13: Training loss = 0.8800
2025-04-04 16:14:13,527 - INFO - Epoch 4, Batch 14: Training loss = 0.8700
2025-04-04 16:14:13,628 - INFO - Epoch 4, Batch 15: Training loss = 0.8600
2025-04-04 16:14:13,729 - INFO - Epoch 4, Batch 16: Training loss = 0.8500
2025-04-04 16:14:13,829 - INFO - Epoch 4, Batch 17: Training loss = 0.8400
2025-04-04 16:14:13,930 - INFO - Epoch 4, Batch 18: Training loss = 0.8300
2025-04-04 16:14:14,030 - INFO - Epoch 4, Batch 19: Training loss = 0.8200
2025-04-04 16:14:14,131 - INFO - Epoch 4, Batch 20: Training loss = 0.8100
2025-04-04 16:14:14,232 - INFO - Epoch 4, Batch 21: Training loss = 0.8000
2025-04-04 16:14:14,333 - INFO - Epoch 4, Batch 22: Training loss = 0.7900
2025-04-04 16:14:14,434 - INFO - Epoch 4, Batch 23: Training loss = 0.7800
2025-04-04 16:14:14,535 - INFO - Epoch 4, Batch 24: Training loss = 0.7700
2025-04-04 16:14:14,635 - INFO - Epoch 4, Batch 25: Training loss = 0.7600
2025-04-04 16:14:14,736 - INFO - Epoch 4, Batch 26: Training loss = 0.7500
2025-04-04 16:14:14,836 - INFO - Epoch 4, Batch 27: Training loss = 0.7400
2025-04-04 16:14:14,937 - INFO - Epoch 4, Batch 28: Training loss = 0.7300
2025-04-04 16:14:15,038 - INFO - Epoch 4, Batch 29: Training loss = 0.7200
2025-04-04 16:14:15,138 - INFO - Epoch 4, Batch 30: Training loss = 0.7100
2025-04-04 16:14:15,239 - INFO - Epoch 4, Batch 31: Training loss = 0.7000
2025-04-04 16:14:15,339 - INFO - Epoch 4, Batch 32: Training loss = 0.6900
2025-04-04 16:14:15,440 - INFO - Epoch 4, Batch 33: Training loss = 0.6800
2025-04-04 16:14:15,541 - INFO - Epoch 4, Batch 34: Training loss = 0.6700
2025-04-04 16:14:15,641 - INFO - Epoch 4, Batch 35: Training loss = 0.6600
2025-04-04 16:14:15,742 - INFO - Epoch 4, Batch 36: Training loss = 0.6500
2025-04-04 16:14:15,843 - INFO - Epoch 4, Batch 37: Training loss = 0.6400
2025-04-04 16:14:15,943 - INFO - Epoch 4, Batch 38: Training loss = 0.6300
2025-04-04 16:14:16,044 - INFO - Epoch 4, Batch 39: Training loss = 0.6200
2025-04-04 16:14:16,145 - INFO - Epoch 4, Batch 40: Training loss = 0.6100
2025-04-04 16:14:16,246 - INFO - Epoch 4, Batch 41: Training loss = 0.6000
2025-04-04 16:14:16,347 - INFO - Epoch 4, Batch 42: Training loss = 0.5900
2025-04-04 16:14:16,448 - INFO - Epoch 4, Batch 43: Training loss = 0.5800
2025-04-04 16:14:16,549 - INFO - Epoch 4, Batch 44: Training loss = 0.5700
2025-04-04 16:14:16,649 - INFO - Epoch 4, Batch 45: Training loss = 0.5600
2025-04-04 16:14:16,750 - INFO - Epoch 4, Batch 46: Training loss = 0.5500
2025-04-04 16:14:16,851 - INFO - Epoch 4, Batch 47: Training loss = 0.5400
2025-04-04 16:14:16,952 - INFO - Epoch 4, Batch 48: Training loss = 0.5300
2025-04-04 16:14:17,053 - INFO - Epoch 4, Batch 49: Training loss = 0.5200
2025-04-04 16:14:17,153 - INFO - Epoch 4, Batch 50: Training loss = 0.5100
2025-04-04 16:14:17,254 - INFO - Epoch 4, Batch 51: Training loss = 0.5000
2025-04-04 16:14:17,355 - INFO - Epoch 4, Batch 52: Training loss = 0.4900
2025-04-04 16:14:17,455 - INFO - Epoch 4, Batch 53: Training loss = 0.4800
2025-04-04 16:14:17,556 - INFO - Epoch 4, Batch 54: Training loss = 0.4700
2025-04-04 16:14:17,657 - INFO - Epoch 4, Batch 55: Training loss = 0.4600
2025-04-04 16:14:17,758 - INFO - Epoch 4, Batch 56: Training loss = 0.4500
2025-04-04 16:14:17,859 - INFO - Epoch 4, Batch 57: Training loss = 0.4400
2025-04-04 16:14:17,959 - INFO - Epoch 4, Batch 58: Training loss = 0.4300
2025-04-04 16:14:18,061 - INFO - Epoch 4, Batch 59: Training loss = 0.4200
2025-04-04 16:14:18,162 - INFO - Epoch 4, Batch 60: Training loss = 0.4100
2025-04-04 16:14:18,263 - INFO - Epoch 4, Batch 61: Training loss = 0.4000
2025-04-04 16:14:18,364 - INFO - Epoch 4, Batch 62: Training loss = 0.3900
2025-04-04 16:14:18,464 - INFO - Epoch 4, Batch 63: Training loss = 0.3800
2025-04-04 16:14:18,566 - INFO - Epoch 4, Batch 64: Training loss = 0.3700
2025-04-04 16:14:18,666 - INFO - Epoch 4, Batch 65: Training loss = 0.3600
2025-04-04 16:14:18,767 - INFO - Epoch 4, Batch 66: Training loss = 0.3500
2025-04-04 16:14:18,867 - INFO - Epoch 4, Batch 67: Training loss = 0.3400
2025-04-04 16:14:18,968 - INFO - Epoch 4, Batch 68: Training loss = 0.3300
2025-04-04 16:14:19,069 - INFO - Epoch 4, Batch 69: Training loss = 0.3200
2025-04-04 16:14:19,170 - INFO - Epoch 4, Batch 70: Training loss = 0.3100
2025-04-04 16:14:19,271 - INFO - Epoch 4, Batch 71: Training loss = 0.3000
2025-04-04 16:14:19,372 - INFO - Epoch 4, Batch 72: Training loss = 0.2900
2025-04-04 16:14:19,472 - INFO - Epoch 4, Batch 73: Training loss = 0.2800
2025-04-04 16:14:19,573 - INFO - Epoch 4, Batch 74: Training loss = 0.2700
2025-04-04 16:14:19,674 - INFO - Epoch 4, Batch 75: Training loss = 0.2600
2025-04-04 16:14:19,775 - INFO - Epoch 4, Batch 76: Training loss = 0.2500
2025-04-04 16:14:19,875 - INFO - Epoch 4, Batch 77: Training loss = 0.2400
2025-04-04 16:14:19,976 - INFO - Epoch 4, Batch 78: Training loss = 0.2300
2025-04-04 16:14:20,077 - INFO - Epoch 4, Batch 79: Training loss = 0.2200
2025-04-04 16:14:20,178 - INFO - Epoch 4, Batch 80: Training loss = 0.2100
2025-04-04 16:14:20,280 - INFO - Epoch 4, Batch 81: Training loss = 0.2000
2025-04-04 16:14:20,381 - INFO - Epoch 4, Batch 82: Training loss = 0.1900
2025-04-04 16:14:20,482 - INFO - Epoch 4, Batch 83: Training loss = 0.1800
2025-04-04 16:14:20,583 - INFO - Epoch 4, Batch 84: Training loss = 0.1700
2025-04-04 16:14:20,684 - INFO - Epoch 4, Batch 85: Training loss = 0.1600
2025-04-04 16:14:20,785 - INFO - Epoch 4, Batch 86: Training loss = 0.1500
2025-04-04 16:14:20,885 - INFO - Epoch 4, Batch 87: Training loss = 0.1400
2025-04-04 16:14:20,986 - INFO - Epoch 4, Batch 88: Training loss = 0.1300
2025-04-04 16:14:21,087 - INFO - Epoch 4, Batch 89: Training loss = 0.1200
2025-04-04 16:14:21,188 - INFO - Epoch 4, Batch 90: Training loss = 0.1100
2025-04-04 16:14:21,288 - INFO - Epoch 4, Batch 91: Training loss = 0.1000
2025-04-04 16:14:21,389 - INFO - Epoch 4, Batch 92: Training loss = 0.0900
2025-04-04 16:14:21,490 - INFO - Epoch 4, Batch 93: Training loss = 0.0800
2025-04-04 16:14:21,591 - INFO - Epoch 4, Batch 94: Training loss = 0.0700
2025-04-04 16:14:21,691 - INFO - Epoch 4, Batch 95: Training loss = 0.0600
2025-04-04 16:14:21,792 - INFO - Epoch 4, Batch 96: Training loss = 0.0500
2025-04-04 16:14:21,893 - INFO - Epoch 4, Batch 97: Training loss = 0.0400
2025-04-04 16:14:21,994 - INFO - Epoch 4, Batch 98: Training loss = 0.0300
2025-04-04 16:14:22,095 - INFO - Epoch 4, Batch 99: Training loss = 0.0200
2025-04-04 16:14:22,196 - INFO - Epoch 4, Batch 100: Training loss = 0.0100
2025-04-04 16:14:22,297 - INFO - Epoch 4: Validation loss = 0.0700, Validation accuracy = 0.8600
2025-04-04 16:14:22,297 - INFO - Epoch 4: MSE = 0.0700, RMSE = 0.2646, MAE = 0.0560, RÂ² = 0.9300
2025-04-04 16:14:22,297 - INFO - Epoch 5 started.
2025-04-04 16:14:22,297 - INFO - Epoch 5, Batch 1: Training loss = 1.0000
2025-04-04 16:14:22,398 - INFO - Epoch 5, Batch 2: Training loss = 0.9900
2025-04-04 16:14:22,499 - INFO - Epoch 5, Batch 3: Training loss = 0.9800
2025-04-04 16:14:22,599 - INFO - Epoch 5, Batch 4: Training loss = 0.9700
2025-04-04 16:14:22,700 - INFO - Epoch 5, Batch 5: Training loss = 0.9600
2025-04-04 16:14:22,801 - INFO - Epoch 5, Batch 6: Training loss = 0.9500
2025-04-04 16:14:22,901 - INFO - Epoch 5, Batch 7: Training loss = 0.9400
2025-04-04 16:14:23,002 - INFO - Epoch 5, Batch 8: Training loss = 0.9300
2025-04-04 16:14:23,103 - INFO - Epoch 5, Batch 9: Training loss = 0.9200
2025-04-04 16:14:23,203 - INFO - Epoch 5, Batch 10: Training loss = 0.9100
2025-04-04 16:14:23,304 - INFO - Epoch 5, Batch 11: Training loss = 0.9000
2025-04-04 16:14:23,405 - INFO - Epoch 5, Batch 12: Training loss = 0.8900
2025-04-04 16:14:23,506 - INFO - Epoch 5, Batch 13: Training loss = 0.8800
2025-04-04 16:14:23,606 - INFO - Epoch 5, Batch 14: Training loss = 0.8700
2025-04-04 16:14:23,707 - INFO - Epoch 5, Batch 15: Training loss = 0.8600
2025-04-04 16:14:23,808 - INFO - Epoch 5, Batch 16: Training loss = 0.8500
2025-04-04 16:14:23,908 - INFO - Epoch 5, Batch 17: Training loss = 0.8400
2025-04-04 16:14:24,009 - INFO - Epoch 5, Batch 18: Training loss = 0.8300
2025-04-04 16:14:24,110 - INFO - Epoch 5, Batch 19: Training loss = 0.8200
2025-04-04 16:14:24,210 - INFO - Epoch 5, Batch 20: Training loss = 0.8100
2025-04-04 16:14:24,311 - INFO - Epoch 5, Batch 21: Training loss = 0.8000
2025-04-04 16:14:24,412 - INFO - Epoch 5, Batch 22: Training loss = 0.7900
2025-04-04 16:14:24,512 - INFO - Epoch 5, Batch 23: Training loss = 0.7800
2025-04-04 16:14:24,613 - INFO - Epoch 5, Batch 24: Training loss = 0.7700
2025-04-04 16:14:24,715 - INFO - Epoch 5, Batch 25: Training loss = 0.7600
2025-04-04 16:14:24,816 - INFO - Epoch 5, Batch 26: Training loss = 0.7500
2025-04-04 16:14:24,916 - INFO - Epoch 5, Batch 27: Training loss = 0.7400
2025-04-04 16:14:25,017 - INFO - Epoch 5, Batch 28: Training loss = 0.7300
2025-04-04 16:14:25,117 - INFO - Epoch 5, Batch 29: Training loss = 0.7200
2025-04-04 16:14:25,218 - INFO - Epoch 5, Batch 30: Training loss = 0.7100
2025-04-04 16:14:25,319 - INFO - Epoch 5, Batch 31: Training loss = 0.7000
2025-04-04 16:14:25,420 - INFO - Epoch 5, Batch 32: Training loss = 0.6900
2025-04-04 16:14:25,520 - INFO - Epoch 5, Batch 33: Training loss = 0.6800
2025-04-04 16:14:25,621 - INFO - Epoch 5, Batch 34: Training loss = 0.6700
2025-04-04 16:14:25,721 - INFO - Epoch 5, Batch 35: Training loss = 0.6600
2025-04-04 16:14:25,822 - INFO - Epoch 5, Batch 36: Training loss = 0.6500
2025-04-04 16:14:25,923 - INFO - Epoch 5, Batch 37: Training loss = 0.6400
2025-04-04 16:14:26,023 - INFO - Epoch 5, Batch 38: Training loss = 0.6300
2025-04-04 16:14:26,124 - INFO - Epoch 5, Batch 39: Training loss = 0.6200
2025-04-04 16:14:26,224 - INFO - Epoch 5, Batch 40: Training loss = 0.6100
2025-04-04 16:14:26,324 - INFO - Epoch 5, Batch 41: Training loss = 0.6000
2025-04-04 16:14:26,425 - INFO - Epoch 5, Batch 42: Training loss = 0.5900
2025-04-04 16:14:26,526 - INFO - Epoch 5, Batch 43: Training loss = 0.5800
2025-04-04 16:14:26,627 - INFO - Epoch 5, Batch 44: Training loss = 0.5700
2025-04-04 16:14:26,727 - INFO - Epoch 5, Batch 45: Training loss = 0.5600
2025-04-04 16:14:26,827 - INFO - Epoch 5, Batch 46: Training loss = 0.5500
2025-04-04 16:14:26,928 - INFO - Epoch 5, Batch 47: Training loss = 0.5400
2025-04-04 16:14:27,028 - INFO - Epoch 5, Batch 48: Training loss = 0.5300
2025-04-04 16:14:27,128 - INFO - Epoch 5, Batch 49: Training loss = 0.5200
2025-04-04 16:14:27,229 - INFO - Epoch 5, Batch 50: Training loss = 0.5100
2025-04-04 16:14:27,330 - INFO - Epoch 5, Batch 51: Training loss = 0.5000
2025-04-04 16:14:27,430 - INFO - Epoch 5, Batch 52: Training loss = 0.4900
2025-04-04 16:14:27,531 - INFO - Epoch 5, Batch 53: Training loss = 0.4800
2025-04-04 16:14:27,632 - INFO - Epoch 5, Batch 54: Training loss = 0.4700
2025-04-04 16:14:27,732 - INFO - Epoch 5, Batch 55: Training loss = 0.4600
2025-04-04 16:14:27,833 - INFO - Epoch 5, Batch 56: Training loss = 0.4500
2025-04-04 16:14:27,934 - INFO - Epoch 5, Batch 57: Training loss = 0.4400
2025-04-04 16:14:28,034 - INFO - Epoch 5, Batch 58: Training loss = 0.4300
2025-04-04 16:14:28,135 - INFO - Epoch 5, Batch 59: Training loss = 0.4200
2025-04-04 16:14:28,235 - INFO - Epoch 5, Batch 60: Training loss = 0.4100
2025-04-04 16:14:28,336 - INFO - Epoch 5, Batch 61: Training loss = 0.4000
2025-04-04 16:14:28,436 - INFO - Epoch 5, Batch 62: Training loss = 0.3900
2025-04-04 16:14:28,537 - INFO - Epoch 5, Batch 63: Training loss = 0.3800
2025-04-04 16:14:28,637 - INFO - Epoch 5, Batch 64: Training loss = 0.3700
2025-04-04 16:14:28,738 - INFO - Epoch 5, Batch 65: Training loss = 0.3600
2025-04-04 16:14:28,838 - INFO - Epoch 5, Batch 66: Training loss = 0.3500
2025-04-04 16:14:28,938 - INFO - Epoch 5, Batch 67: Training loss = 0.3400
2025-04-04 16:14:29,039 - INFO - Epoch 5, Batch 68: Training loss = 0.3300
2025-04-04 16:14:29,140 - INFO - Epoch 5, Batch 69: Training loss = 0.3200
2025-04-04 16:14:29,240 - INFO - Epoch 5, Batch 70: Training loss = 0.3100
2025-04-04 16:14:29,341 - INFO - Epoch 5, Batch 71: Training loss = 0.3000
2025-04-04 16:14:29,441 - INFO - Epoch 5, Batch 72: Training loss = 0.2900
2025-04-04 16:14:29,542 - INFO - Epoch 5, Batch 73: Training loss = 0.2800
2025-04-04 16:14:29,642 - INFO - Epoch 5, Batch 74: Training loss = 0.2700
2025-04-04 16:14:29,743 - INFO - Epoch 5, Batch 75: Training loss = 0.2600
2025-04-04 16:14:29,843 - INFO - Epoch 5, Batch 76: Training loss = 0.2500
2025-04-04 16:14:29,944 - INFO - Epoch 5, Batch 77: Training loss = 0.2400
2025-04-04 16:14:30,044 - INFO - Epoch 5, Batch 78: Training loss = 0.2300
2025-04-04 16:14:30,145 - INFO - Epoch 5, Batch 79: Training loss = 0.2200
2025-04-04 16:14:30,246 - INFO - Epoch 5, Batch 80: Training loss = 0.2100
2025-04-04 16:14:30,346 - INFO - Epoch 5, Batch 81: Training loss = 0.2000
2025-04-04 16:14:30,447 - INFO - Epoch 5, Batch 82: Training loss = 0.1900
2025-04-04 16:14:30,548 - INFO - Epoch 5, Batch 83: Training loss = 0.1800
2025-04-04 16:14:30,648 - INFO - Epoch 5, Batch 84: Training loss = 0.1700
2025-04-04 16:14:30,749 - INFO - Epoch 5, Batch 85: Training loss = 0.1600
2025-04-04 16:14:30,849 - INFO - Epoch 5, Batch 86: Training loss = 0.1500
2025-04-04 16:14:30,950 - INFO - Epoch 5, Batch 87: Training loss = 0.1400
2025-04-04 16:14:31,051 - INFO - Epoch 5, Batch 88: Training loss = 0.1300
2025-04-04 16:14:31,151 - INFO - Epoch 5, Batch 89: Training loss = 0.1200
2025-04-04 16:14:31,252 - INFO - Epoch 5, Batch 90: Training loss = 0.1100
2025-04-04 16:14:31,353 - INFO - Epoch 5, Batch 91: Training loss = 0.1000
2025-04-04 16:14:31,453 - INFO - Epoch 5, Batch 92: Training loss = 0.0900
2025-04-04 16:14:31,554 - INFO - Epoch 5, Batch 93: Training loss = 0.0800
2025-04-04 16:14:31,655 - INFO - Epoch 5, Batch 94: Training loss = 0.0700
2025-04-04 16:14:31,756 - INFO - Epoch 5, Batch 95: Training loss = 0.0600
2025-04-04 16:14:31,856 - INFO - Epoch 5, Batch 96: Training loss = 0.0500
2025-04-04 16:14:31,957 - INFO - Epoch 5, Batch 97: Training loss = 0.0400
2025-04-04 16:14:32,057 - INFO - Epoch 5, Batch 98: Training loss = 0.0300
2025-04-04 16:14:32,158 - INFO - Epoch 5, Batch 99: Training loss = 0.0200
2025-04-04 16:14:32,259 - INFO - Epoch 5, Batch 100: Training loss = 0.0100
2025-04-04 16:14:32,360 - INFO - Epoch 5: Validation loss = 0.0600, Validation accuracy = 0.8800
2025-04-04 16:14:32,360 - INFO - Epoch 5: MSE = 0.0600, RMSE = 0.2449, MAE = 0.0480, RÂ² = 0.9400
2025-04-04 16:14:32,360 - INFO - Epoch 6 started.
2025-04-04 16:14:32,360 - INFO - Epoch 6, Batch 1: Training loss = 1.0000
2025-04-04 16:14:32,461 - INFO - Epoch 6, Batch 2: Training loss = 0.9900
2025-04-04 16:14:32,561 - INFO - Epoch 6, Batch 3: Training loss = 0.9800
2025-04-04 16:14:32,662 - INFO - Epoch 6, Batch 4: Training loss = 0.9700
2025-04-04 16:14:32,763 - INFO - Epoch 6, Batch 5: Training loss = 0.9600
2025-04-04 16:14:32,863 - INFO - Epoch 6, Batch 6: Training loss = 0.9500
2025-04-04 16:14:32,964 - INFO - Epoch 6, Batch 7: Training loss = 0.9400
2025-04-04 16:14:33,065 - INFO - Epoch 6, Batch 8: Training loss = 0.9300
2025-04-04 16:14:33,165 - INFO - Epoch 6, Batch 9: Training loss = 0.9200
2025-04-04 16:14:33,266 - INFO - Epoch 6, Batch 10: Training loss = 0.9100
2025-04-04 16:14:33,367 - INFO - Epoch 6, Batch 11: Training loss = 0.9000
2025-04-04 16:14:33,467 - INFO - Epoch 6, Batch 12: Training loss = 0.8900
2025-04-04 16:14:33,568 - INFO - Epoch 6, Batch 13: Training loss = 0.8800
2025-04-04 16:14:33,668 - INFO - Epoch 6, Batch 14: Training loss = 0.8700
2025-04-04 16:14:33,769 - INFO - Epoch 6, Batch 15: Training loss = 0.8600
2025-04-04 16:14:33,870 - INFO - Epoch 6, Batch 16: Training loss = 0.8500
2025-04-04 16:14:33,970 - INFO - Epoch 6, Batch 17: Training loss = 0.8400
2025-04-04 16:14:34,071 - INFO - Epoch 6, Batch 18: Training loss = 0.8300
2025-04-04 16:14:34,171 - INFO - Epoch 6, Batch 19: Training loss = 0.8200
2025-04-04 16:14:34,272 - INFO - Epoch 6, Batch 20: Training loss = 0.8100
2025-04-04 16:14:34,373 - INFO - Epoch 6, Batch 21: Training loss = 0.8000
2025-04-04 16:14:34,473 - INFO - Epoch 6, Batch 22: Training loss = 0.7900
2025-04-04 16:14:34,574 - INFO - Epoch 6, Batch 23: Training loss = 0.7800
2025-04-04 16:14:34,675 - INFO - Epoch 6, Batch 24: Training loss = 0.7700
2025-04-04 16:14:34,775 - INFO - Epoch 6, Batch 25: Training loss = 0.7600
2025-04-04 16:14:34,876 - INFO - Epoch 6, Batch 26: Training loss = 0.7500
2025-04-04 16:14:34,977 - INFO - Epoch 6, Batch 27: Training loss = 0.7400
2025-04-04 16:14:35,078 - INFO - Epoch 6, Batch 28: Training loss = 0.7300
2025-04-04 16:14:35,179 - INFO - Epoch 6, Batch 29: Training loss = 0.7200
2025-04-04 16:14:35,280 - INFO - Epoch 6, Batch 30: Training loss = 0.7100
2025-04-04 16:14:35,381 - INFO - Epoch 6, Batch 31: Training loss = 0.7000
2025-04-04 16:14:35,482 - INFO - Epoch 6, Batch 32: Training loss = 0.6900
2025-04-04 16:14:35,583 - INFO - Epoch 6, Batch 33: Training loss = 0.6800
2025-04-04 16:14:35,683 - INFO - Epoch 6, Batch 34: Training loss = 0.6700
2025-04-04 16:14:35,784 - INFO - Epoch 6, Batch 35: Training loss = 0.6600
2025-04-04 16:14:35,884 - INFO - Epoch 6, Batch 36: Training loss = 0.6500
2025-04-04 16:14:35,985 - INFO - Epoch 6, Batch 37: Training loss = 0.6400
2025-04-04 16:14:36,085 - INFO - Epoch 6, Batch 38: Training loss = 0.6300
2025-04-04 16:14:36,185 - INFO - Epoch 6, Batch 39: Training loss = 0.6200
2025-04-04 16:14:36,286 - INFO - Epoch 6, Batch 40: Training loss = 0.6100
2025-04-04 16:14:36,387 - INFO - Epoch 6, Batch 41: Training loss = 0.6000
2025-04-04 16:14:36,487 - INFO - Epoch 6, Batch 42: Training loss = 0.5900
2025-04-04 16:14:36,588 - INFO - Epoch 6, Batch 43: Training loss = 0.5800
2025-04-04 16:14:36,689 - INFO - Epoch 6, Batch 44: Training loss = 0.5700
2025-04-04 16:14:36,790 - INFO - Epoch 6, Batch 45: Training loss = 0.5600
2025-04-04 16:14:36,890 - INFO - Epoch 6, Batch 46: Training loss = 0.5500
2025-04-04 16:14:36,991 - INFO - Epoch 6, Batch 47: Training loss = 0.5400
2025-04-04 16:14:37,092 - INFO - Epoch 6, Batch 48: Training loss = 0.5300
2025-04-04 16:14:37,193 - INFO - Epoch 6, Batch 49: Training loss = 0.5200
2025-04-04 16:14:37,293 - INFO - Epoch 6, Batch 50: Training loss = 0.5100
2025-04-04 16:14:37,394 - INFO - Epoch 6, Batch 51: Training loss = 0.5000
2025-04-04 16:14:37,495 - INFO - Epoch 6, Batch 52: Training loss = 0.4900
2025-04-04 16:14:37,596 - INFO - Epoch 6, Batch 53: Training loss = 0.4800
2025-04-04 16:14:37,697 - INFO - Epoch 6, Batch 54: Training loss = 0.4700
2025-04-04 16:14:37,797 - INFO - Epoch 6, Batch 55: Training loss = 0.4600
2025-04-04 16:14:37,898 - INFO - Epoch 6, Batch 56: Training loss = 0.4500
2025-04-04 16:14:37,998 - INFO - Epoch 6, Batch 57: Training loss = 0.4400
2025-04-04 16:14:38,099 - INFO - Epoch 6, Batch 58: Training loss = 0.4300
2025-04-04 16:14:38,199 - INFO - Epoch 6, Batch 59: Training loss = 0.4200
2025-04-04 16:14:38,299 - INFO - Epoch 6, Batch 60: Training loss = 0.4100
2025-04-04 16:14:38,400 - INFO - Epoch 6, Batch 61: Training loss = 0.4000
2025-04-04 16:14:38,500 - INFO - Epoch 6, Batch 62: Training loss = 0.3900
2025-04-04 16:14:38,601 - INFO - Epoch 6, Batch 63: Training loss = 0.3800
2025-04-04 16:14:38,701 - INFO - Epoch 6, Batch 64: Training loss = 0.3700
2025-04-04 16:14:38,802 - INFO - Epoch 6, Batch 65: Training loss = 0.3600
2025-04-04 16:14:38,902 - INFO - Epoch 6, Batch 66: Training loss = 0.3500
2025-04-04 16:14:39,003 - INFO - Epoch 6, Batch 67: Training loss = 0.3400
2025-04-04 16:14:39,103 - INFO - Epoch 6, Batch 68: Training loss = 0.3300
2025-04-04 16:14:39,203 - INFO - Epoch 6, Batch 69: Training loss = 0.3200
2025-04-04 16:14:39,304 - INFO - Epoch 6, Batch 70: Training loss = 0.3100
2025-04-04 16:14:39,404 - INFO - Epoch 6, Batch 71: Training loss = 0.3000
2025-04-04 16:14:39,505 - INFO - Epoch 6, Batch 72: Training loss = 0.2900
2025-04-04 16:14:39,605 - INFO - Epoch 6, Batch 73: Training loss = 0.2800
2025-04-04 16:14:39,706 - INFO - Epoch 6, Batch 74: Training loss = 0.2700
2025-04-04 16:14:39,806 - INFO - Epoch 6, Batch 75: Training loss = 0.2600
2025-04-04 16:14:39,907 - INFO - Epoch 6, Batch 76: Training loss = 0.2500
2025-04-04 16:14:40,008 - INFO - Epoch 6, Batch 77: Training loss = 0.2400
2025-04-04 16:14:40,108 - INFO - Epoch 6, Batch 78: Training loss = 0.2300
2025-04-04 16:14:40,209 - INFO - Epoch 6, Batch 79: Training loss = 0.2200
2025-04-04 16:14:40,310 - INFO - Epoch 6, Batch 80: Training loss = 0.2100
2025-04-04 16:14:40,411 - INFO - Epoch 6, Batch 81: Training loss = 0.2000
2025-04-04 16:14:40,511 - INFO - Epoch 6, Batch 82: Training loss = 0.1900
2025-04-04 16:14:40,612 - INFO - Epoch 6, Batch 83: Training loss = 0.1800
2025-04-04 16:14:40,713 - INFO - Epoch 6, Batch 84: Training loss = 0.1700
2025-04-04 16:14:40,813 - INFO - Epoch 6, Batch 85: Training loss = 0.1600
2025-04-04 16:14:40,914 - INFO - Epoch 6, Batch 86: Training loss = 0.1500
2025-04-04 16:14:41,015 - INFO - Epoch 6, Batch 87: Training loss = 0.1400
2025-04-04 16:14:41,116 - INFO - Epoch 6, Batch 88: Training loss = 0.1300
2025-04-04 16:14:41,217 - INFO - Epoch 6, Batch 89: Training loss = 0.1200
2025-04-04 16:14:41,318 - INFO - Epoch 6, Batch 90: Training loss = 0.1100
2025-04-04 16:14:41,418 - INFO - Epoch 6, Batch 91: Training loss = 0.1000
2025-04-04 16:14:41,520 - INFO - Epoch 6, Batch 92: Training loss = 0.0900
2025-04-04 16:14:41,620 - INFO - Epoch 6, Batch 93: Training loss = 0.0800
2025-04-04 16:14:41,721 - INFO - Epoch 6, Batch 94: Training loss = 0.0700
2025-04-04 16:14:41,822 - INFO - Epoch 6, Batch 95: Training loss = 0.0600
2025-04-04 16:14:41,923 - INFO - Epoch 6, Batch 96: Training loss = 0.0500
2025-04-04 16:14:42,023 - INFO - Epoch 6, Batch 97: Training loss = 0.0400
2025-04-04 16:14:42,124 - INFO - Epoch 6, Batch 98: Training loss = 0.0300
2025-04-04 16:14:42,225 - INFO - Epoch 6, Batch 99: Training loss = 0.0200
2025-04-04 16:14:42,326 - INFO - Epoch 6, Batch 100: Training loss = 0.0100
2025-04-04 16:14:42,427 - INFO - Epoch 6: Validation loss = 0.0500, Validation accuracy = 0.9000
2025-04-04 16:14:42,427 - INFO - Epoch 6: MSE = 0.0500, RMSE = 0.2236, MAE = 0.0400, RÂ² = 0.9500
2025-04-04 16:14:42,427 - INFO - Epoch 7 started.
2025-04-04 16:14:42,428 - INFO - Epoch 7, Batch 1: Training loss = 1.0000
2025-04-04 16:14:42,528 - INFO - Epoch 7, Batch 2: Training loss = 0.9900
2025-04-04 16:14:42,629 - INFO - Epoch 7, Batch 3: Training loss = 0.9800
2025-04-04 16:14:42,729 - INFO - Epoch 7, Batch 4: Training loss = 0.9700
2025-04-04 16:14:42,830 - INFO - Epoch 7, Batch 5: Training loss = 0.9600
2025-04-04 16:14:42,931 - INFO - Epoch 7, Batch 6: Training loss = 0.9500
2025-04-04 16:14:43,031 - INFO - Epoch 7, Batch 7: Training loss = 0.9400
2025-04-04 16:14:43,132 - INFO - Epoch 7, Batch 8: Training loss = 0.9300
2025-04-04 16:14:43,232 - INFO - Epoch 7, Batch 9: Training loss = 0.9200
2025-04-04 16:14:43,333 - INFO - Epoch 7, Batch 10: Training loss = 0.9100
2025-04-04 16:14:43,433 - INFO - Epoch 7, Batch 11: Training loss = 0.9000
2025-04-04 16:14:43,534 - INFO - Epoch 7, Batch 12: Training loss = 0.8900
2025-04-04 16:14:43,635 - INFO - Epoch 7, Batch 13: Training loss = 0.8800
2025-04-04 16:14:43,736 - INFO - Epoch 7, Batch 14: Training loss = 0.8700
2025-04-04 16:14:43,836 - INFO - Epoch 7, Batch 15: Training loss = 0.8600
2025-04-04 16:14:43,937 - INFO - Epoch 7, Batch 16: Training loss = 0.8500
2025-04-04 16:14:44,037 - INFO - Epoch 7, Batch 17: Training loss = 0.8400
2025-04-04 16:14:44,138 - INFO - Epoch 7, Batch 18: Training loss = 0.8300
2025-04-04 16:14:44,239 - INFO - Epoch 7, Batch 19: Training loss = 0.8200
2025-04-04 16:14:44,340 - INFO - Epoch 7, Batch 20: Training loss = 0.8100
2025-04-04 16:14:44,440 - INFO - Epoch 7, Batch 21: Training loss = 0.8000
2025-04-04 16:14:44,541 - INFO - Epoch 7, Batch 22: Training loss = 0.7900
2025-04-04 16:14:44,641 - INFO - Epoch 7, Batch 23: Training loss = 0.7800
2025-04-04 16:14:44,742 - INFO - Epoch 7, Batch 24: Training loss = 0.7700
2025-04-04 16:14:44,843 - INFO - Epoch 7, Batch 25: Training loss = 0.7600
2025-04-04 16:14:44,944 - INFO - Epoch 7, Batch 26: Training loss = 0.7500
2025-04-04 16:14:45,045 - INFO - Epoch 7, Batch 27: Training loss = 0.7400
2025-04-04 16:14:45,145 - INFO - Epoch 7, Batch 28: Training loss = 0.7300
2025-04-04 16:14:45,246 - INFO - Epoch 7, Batch 29: Training loss = 0.7200
2025-04-04 16:14:45,346 - INFO - Epoch 7, Batch 30: Training loss = 0.7100
2025-04-04 16:14:45,446 - INFO - Epoch 7, Batch 31: Training loss = 0.7000
2025-04-04 16:14:45,547 - INFO - Epoch 7, Batch 32: Training loss = 0.6900
2025-04-04 16:14:45,648 - INFO - Epoch 7, Batch 33: Training loss = 0.6800
2025-04-04 16:14:45,748 - INFO - Epoch 7, Batch 34: Training loss = 0.6700
2025-04-04 16:14:45,849 - INFO - Epoch 7, Batch 35: Training loss = 0.6600
2025-04-04 16:14:45,949 - INFO - Epoch 7, Batch 36: Training loss = 0.6500
2025-04-04 16:14:46,050 - INFO - Epoch 7, Batch 37: Training loss = 0.6400
2025-04-04 16:14:46,150 - INFO - Epoch 7, Batch 38: Training loss = 0.6300
2025-04-04 16:14:46,250 - INFO - Epoch 7, Batch 39: Training loss = 0.6200
2025-04-04 16:14:46,351 - INFO - Epoch 7, Batch 40: Training loss = 0.6100
2025-04-04 16:14:46,451 - INFO - Epoch 7, Batch 41: Training loss = 0.6000
2025-04-04 16:14:46,552 - INFO - Epoch 7, Batch 42: Training loss = 0.5900
2025-04-04 16:14:46,653 - INFO - Epoch 7, Batch 43: Training loss = 0.5800
2025-04-04 16:14:46,753 - INFO - Epoch 7, Batch 44: Training loss = 0.5700
2025-04-04 16:14:46,854 - INFO - Epoch 7, Batch 45: Training loss = 0.5600
2025-04-04 16:14:46,955 - INFO - Epoch 7, Batch 46: Training loss = 0.5500
2025-04-04 16:14:47,055 - INFO - Epoch 7, Batch 47: Training loss = 0.5400
2025-04-04 16:14:47,156 - INFO - Epoch 7, Batch 48: Training loss = 0.5300
2025-04-04 16:14:47,257 - INFO - Epoch 7, Batch 49: Training loss = 0.5200
2025-04-04 16:14:47,358 - INFO - Epoch 7, Batch 50: Training loss = 0.5100
2025-04-04 16:14:47,458 - INFO - Epoch 7, Batch 51: Training loss = 0.5000
2025-04-04 16:14:47,559 - INFO - Epoch 7, Batch 52: Training loss = 0.4900
2025-04-04 16:14:47,660 - INFO - Epoch 7, Batch 53: Training loss = 0.4800
2025-04-04 16:14:47,760 - INFO - Epoch 7, Batch 54: Training loss = 0.4700
2025-04-04 16:14:47,861 - INFO - Epoch 7, Batch 55: Training loss = 0.4600
2025-04-04 16:14:47,961 - INFO - Epoch 7, Batch 56: Training loss = 0.4500
2025-04-04 16:14:48,062 - INFO - Epoch 7, Batch 57: Training loss = 0.4400
2025-04-04 16:14:48,162 - INFO - Epoch 7, Batch 58: Training loss = 0.4300
2025-04-04 16:14:48,263 - INFO - Epoch 7, Batch 59: Training loss = 0.4200
2025-04-04 16:14:48,363 - INFO - Epoch 7, Batch 60: Training loss = 0.4100
2025-04-04 16:14:48,464 - INFO - Epoch 7, Batch 61: Training loss = 0.4000
2025-04-04 16:14:48,564 - INFO - Epoch 7, Batch 62: Training loss = 0.3900
2025-04-04 16:14:48,665 - INFO - Epoch 7, Batch 63: Training loss = 0.3800
2025-04-04 16:14:48,766 - INFO - Epoch 7, Batch 64: Training loss = 0.3700
2025-04-04 16:14:48,866 - INFO - Epoch 7, Batch 65: Training loss = 0.3600
2025-04-04 16:14:48,967 - INFO - Epoch 7, Batch 66: Training loss = 0.3500
2025-04-04 16:14:49,068 - INFO - Epoch 7, Batch 67: Training loss = 0.3400
2025-04-04 16:14:49,168 - INFO - Epoch 7, Batch 68: Training loss = 0.3300
2025-04-04 16:14:49,269 - INFO - Epoch 7, Batch 69: Training loss = 0.3200
2025-04-04 16:14:49,370 - INFO - Epoch 7, Batch 70: Training loss = 0.3100
2025-04-04 16:14:49,470 - INFO - Epoch 7, Batch 71: Training loss = 0.3000
2025-04-04 16:14:49,571 - INFO - Epoch 7, Batch 72: Training loss = 0.2900
2025-04-04 16:14:49,671 - INFO - Epoch 7, Batch 73: Training loss = 0.2800
2025-04-04 16:14:49,772 - INFO - Epoch 7, Batch 74: Training loss = 0.2700
2025-04-04 16:14:49,872 - INFO - Epoch 7, Batch 75: Training loss = 0.2600
2025-04-04 16:14:49,973 - INFO - Epoch 7, Batch 76: Training loss = 0.2500
2025-04-04 16:14:50,073 - INFO - Epoch 7, Batch 77: Training loss = 0.2400
2025-04-04 16:14:50,173 - INFO - Epoch 7, Batch 78: Training loss = 0.2300
2025-04-04 16:14:50,274 - INFO - Epoch 7, Batch 79: Training loss = 0.2200
2025-04-04 16:14:50,374 - INFO - Epoch 7, Batch 80: Training loss = 0.2100
2025-04-04 16:14:50,475 - INFO - Epoch 7, Batch 81: Training loss = 0.2000
2025-04-04 16:14:50,576 - INFO - Epoch 7, Batch 82: Training loss = 0.1900
2025-04-04 16:14:50,677 - INFO - Epoch 7, Batch 83: Training loss = 0.1800
2025-04-04 16:14:50,778 - INFO - Epoch 7, Batch 84: Training loss = 0.1700
2025-04-04 16:14:50,878 - INFO - Epoch 7, Batch 85: Training loss = 0.1600
2025-04-04 16:14:50,979 - INFO - Epoch 7, Batch 86: Training loss = 0.1500
2025-04-04 16:14:51,079 - INFO - Epoch 7, Batch 87: Training loss = 0.1400
2025-04-04 16:14:51,179 - INFO - Epoch 7, Batch 88: Training loss = 0.1300
2025-04-04 16:14:51,280 - INFO - Epoch 7, Batch 89: Training loss = 0.1200
2025-04-04 16:14:51,381 - INFO - Epoch 7, Batch 90: Training loss = 0.1100
2025-04-04 16:14:51,481 - INFO - Epoch 7, Batch 91: Training loss = 0.1000
2025-04-04 16:14:51,582 - INFO - Epoch 7, Batch 92: Training loss = 0.0900
2025-04-04 16:14:51,682 - INFO - Epoch 7, Batch 93: Training loss = 0.0800
2025-04-04 16:14:51,783 - INFO - Epoch 7, Batch 94: Training loss = 0.0700
2025-04-04 16:14:51,883 - INFO - Epoch 7, Batch 95: Training loss = 0.0600
2025-04-04 16:14:51,984 - INFO - Epoch 7, Batch 96: Training loss = 0.0500
2025-04-04 16:14:52,084 - INFO - Epoch 7, Batch 97: Training loss = 0.0400
2025-04-04 16:14:52,185 - INFO - Epoch 7, Batch 98: Training loss = 0.0300
2025-04-04 16:14:52,285 - INFO - Epoch 7, Batch 99: Training loss = 0.0200
2025-04-04 16:14:52,386 - INFO - Epoch 7, Batch 100: Training loss = 0.0100
2025-04-04 16:14:52,486 - INFO - Epoch 7: Validation loss = 0.0400, Validation accuracy = 0.9200
2025-04-04 16:14:52,486 - INFO - Epoch 7: MSE = 0.0400, RMSE = 0.2000, MAE = 0.0320, RÂ² = 0.9600
2025-04-04 16:14:52,486 - INFO - Epoch 8 started.
2025-04-04 16:14:52,487 - INFO - Epoch 8, Batch 1: Training loss = 1.0000
2025-04-04 16:14:52,587 - INFO - Epoch 8, Batch 2: Training loss = 0.9900
2025-04-04 16:14:52,688 - INFO - Epoch 8, Batch 3: Training loss = 0.9800
2025-04-04 16:14:52,788 - INFO - Epoch 8, Batch 4: Training loss = 0.9700
2025-04-04 16:14:52,889 - INFO - Epoch 8, Batch 5: Training loss = 0.9600
2025-04-04 16:14:52,990 - INFO - Epoch 8, Batch 6: Training loss = 0.9500
2025-04-04 16:14:53,090 - INFO - Epoch 8, Batch 7: Training loss = 0.9400
2025-04-04 16:14:53,191 - INFO - Epoch 8, Batch 8: Training loss = 0.9300
2025-04-04 16:14:53,291 - INFO - Epoch 8, Batch 9: Training loss = 0.9200
2025-04-04 16:14:53,392 - INFO - Epoch 8, Batch 10: Training loss = 0.9100
2025-04-04 16:14:53,492 - INFO - Epoch 8, Batch 11: Training loss = 0.9000
2025-04-04 16:14:53,592 - INFO - Epoch 8, Batch 12: Training loss = 0.8900
2025-04-04 16:14:53,693 - INFO - Epoch 8, Batch 13: Training loss = 0.8800
2025-04-04 16:14:53,793 - INFO - Epoch 8, Batch 14: Training loss = 0.8700
2025-04-04 16:14:53,894 - INFO - Epoch 8, Batch 15: Training loss = 0.8600
2025-04-04 16:14:53,994 - INFO - Epoch 8, Batch 16: Training loss = 0.8500
2025-04-04 16:14:54,095 - INFO - Epoch 8, Batch 17: Training loss = 0.8400
2025-04-04 16:14:54,195 - INFO - Epoch 8, Batch 18: Training loss = 0.8300
2025-04-04 16:14:54,295 - INFO - Epoch 8, Batch 19: Training loss = 0.8200
2025-04-04 16:14:54,396 - INFO - Epoch 8, Batch 20: Training loss = 0.8100
2025-04-04 16:14:54,497 - INFO - Epoch 8, Batch 21: Training loss = 0.8000
2025-04-04 16:14:54,597 - INFO - Epoch 8, Batch 22: Training loss = 0.7900
2025-04-04 16:14:54,698 - INFO - Epoch 8, Batch 23: Training loss = 0.7800
2025-04-04 16:14:54,798 - INFO - Epoch 8, Batch 24: Training loss = 0.7700
2025-04-04 16:14:54,899 - INFO - Epoch 8, Batch 25: Training loss = 0.7600
2025-04-04 16:14:54,999 - INFO - Epoch 8, Batch 26: Training loss = 0.7500
2025-04-04 16:14:55,100 - INFO - Epoch 8, Batch 27: Training loss = 0.7400
2025-04-04 16:14:55,201 - INFO - Epoch 8, Batch 28: Training loss = 0.7300
2025-04-04 16:14:55,301 - INFO - Epoch 8, Batch 29: Training loss = 0.7200
2025-04-04 16:14:55,402 - INFO - Epoch 8, Batch 30: Training loss = 0.7100
2025-04-04 16:14:55,502 - INFO - Epoch 8, Batch 31: Training loss = 0.7000
2025-04-04 16:14:55,603 - INFO - Epoch 8, Batch 32: Training loss = 0.6900
2025-04-04 16:14:55,703 - INFO - Epoch 8, Batch 33: Training loss = 0.6800
2025-04-04 16:14:55,804 - INFO - Epoch 8, Batch 34: Training loss = 0.6700
2025-04-04 16:14:55,904 - INFO - Epoch 8, Batch 35: Training loss = 0.6600
2025-04-04 16:14:56,005 - INFO - Epoch 8, Batch 36: Training loss = 0.6500
2025-04-04 16:14:56,105 - INFO - Epoch 8, Batch 37: Training loss = 0.6400
2025-04-04 16:14:56,205 - INFO - Epoch 8, Batch 38: Training loss = 0.6300
2025-04-04 16:14:56,306 - INFO - Epoch 8, Batch 39: Training loss = 0.6200
2025-04-04 16:14:56,407 - INFO - Epoch 8, Batch 40: Training loss = 0.6100
2025-04-04 16:14:56,507 - INFO - Epoch 8, Batch 41: Training loss = 0.6000
2025-04-04 16:14:56,608 - INFO - Epoch 8, Batch 42: Training loss = 0.5900
2025-04-04 16:14:56,708 - INFO - Epoch 8, Batch 43: Training loss = 0.5800
2025-04-04 16:14:56,809 - INFO - Epoch 8, Batch 44: Training loss = 0.5700
2025-04-04 16:14:56,909 - INFO - Epoch 8, Batch 45: Training loss = 0.5600
2025-04-04 16:14:57,010 - INFO - Epoch 8, Batch 46: Training loss = 0.5500
2025-04-04 16:14:57,110 - INFO - Epoch 8, Batch 47: Training loss = 0.5400
2025-04-04 16:14:57,211 - INFO - Epoch 8, Batch 48: Training loss = 0.5300
2025-04-04 16:14:57,311 - INFO - Epoch 8, Batch 49: Training loss = 0.5200
2025-04-04 16:14:57,412 - INFO - Epoch 8, Batch 50: Training loss = 0.5100
2025-04-04 16:14:57,512 - INFO - Epoch 8, Batch 51: Training loss = 0.5000
2025-04-04 16:14:57,612 - INFO - Epoch 8, Batch 52: Training loss = 0.4900
2025-04-04 16:14:57,713 - INFO - Epoch 8, Batch 53: Training loss = 0.4800
2025-04-04 16:14:57,813 - INFO - Epoch 8, Batch 54: Training loss = 0.4700
2025-04-04 16:14:57,914 - INFO - Epoch 8, Batch 55: Training loss = 0.4600
2025-04-04 16:14:58,014 - INFO - Epoch 8, Batch 56: Training loss = 0.4500
2025-04-04 16:14:58,115 - INFO - Epoch 8, Batch 57: Training loss = 0.4400
2025-04-04 16:14:58,215 - INFO - Epoch 8, Batch 58: Training loss = 0.4300
2025-04-04 16:14:58,316 - INFO - Epoch 8, Batch 59: Training loss = 0.4200
2025-04-04 16:14:58,417 - INFO - Epoch 8, Batch 60: Training loss = 0.4100
2025-04-04 16:14:58,518 - INFO - Epoch 8, Batch 61: Training loss = 0.4000
2025-04-04 16:14:58,619 - INFO - Epoch 8, Batch 62: Training loss = 0.3900
2025-04-04 16:14:58,719 - INFO - Epoch 8, Batch 63: Training loss = 0.3800
2025-04-04 16:14:58,820 - INFO - Epoch 8, Batch 64: Training loss = 0.3700
2025-04-04 16:14:58,921 - INFO - Epoch 8, Batch 65: Training loss = 0.3600
2025-04-04 16:14:59,021 - INFO - Epoch 8, Batch 66: Training loss = 0.3500
2025-04-04 16:14:59,122 - INFO - Epoch 8, Batch 67: Training loss = 0.3400
2025-04-04 16:14:59,222 - INFO - Epoch 8, Batch 68: Training loss = 0.3300
2025-04-04 16:14:59,323 - INFO - Epoch 8, Batch 69: Training loss = 0.3200
2025-04-04 16:14:59,424 - INFO - Epoch 8, Batch 70: Training loss = 0.3100
2025-04-04 16:14:59,524 - INFO - Epoch 8, Batch 71: Training loss = 0.3000
2025-04-04 16:14:59,625 - INFO - Epoch 8, Batch 72: Training loss = 0.2900
2025-04-04 16:14:59,726 - INFO - Epoch 8, Batch 73: Training loss = 0.2800
2025-04-04 16:14:59,827 - INFO - Epoch 8, Batch 74: Training loss = 0.2700
2025-04-04 16:14:59,927 - INFO - Epoch 8, Batch 75: Training loss = 0.2600
2025-04-04 16:15:00,027 - INFO - Epoch 8, Batch 76: Training loss = 0.2500
2025-04-04 16:15:00,128 - INFO - Epoch 8, Batch 77: Training loss = 0.2400
2025-04-04 16:15:00,228 - INFO - Epoch 8, Batch 78: Training loss = 0.2300
2025-04-04 16:15:00,329 - INFO - Epoch 8, Batch 79: Training loss = 0.2200
2025-04-04 16:15:00,429 - INFO - Epoch 8, Batch 80: Training loss = 0.2100
2025-04-04 16:15:00,530 - INFO - Epoch 8, Batch 81: Training loss = 0.2000
2025-04-04 16:15:00,630 - INFO - Epoch 8, Batch 82: Training loss = 0.1900
2025-04-04 16:15:00,731 - INFO - Epoch 8, Batch 83: Training loss = 0.1800
2025-04-04 16:15:00,831 - INFO - Epoch 8, Batch 84: Training loss = 0.1700
2025-04-04 16:15:00,932 - INFO - Epoch 8, Batch 85: Training loss = 0.1600
2025-04-04 16:15:01,033 - INFO - Epoch 8, Batch 86: Training loss = 0.1500
2025-04-04 16:15:01,134 - INFO - Epoch 8, Batch 87: Training loss = 0.1400
2025-04-04 16:15:01,235 - INFO - Epoch 8, Batch 88: Training loss = 0.1300
2025-04-04 16:15:01,336 - INFO - Epoch 8, Batch 89: Training loss = 0.1200
2025-04-04 16:15:01,437 - INFO - Epoch 8, Batch 90: Training loss = 0.1100
2025-04-04 16:15:01,537 - INFO - Epoch 8, Batch 91: Training loss = 0.1000
2025-04-04 16:15:01,638 - INFO - Epoch 8, Batch 92: Training loss = 0.0900
2025-04-04 16:15:01,738 - INFO - Epoch 8, Batch 93: Training loss = 0.0800
2025-04-04 16:15:01,839 - INFO - Epoch 8, Batch 94: Training loss = 0.0700
2025-04-04 16:15:01,940 - INFO - Epoch 8, Batch 95: Training loss = 0.0600
2025-04-04 16:15:02,041 - INFO - Epoch 8, Batch 96: Training loss = 0.0500
2025-04-04 16:15:02,141 - INFO - Epoch 8, Batch 97: Training loss = 0.0400
2025-04-04 16:15:02,242 - INFO - Epoch 8, Batch 98: Training loss = 0.0300
2025-04-04 16:15:02,342 - INFO - Epoch 8, Batch 99: Training loss = 0.0200
2025-04-04 16:15:02,443 - INFO - Epoch 8, Batch 100: Training loss = 0.0100
2025-04-04 16:15:02,543 - INFO - Epoch 8: Validation loss = 0.0300, Validation accuracy = 0.9400
2025-04-04 16:15:02,544 - INFO - Epoch 8: MSE = 0.0300, RMSE = 0.1732, MAE = 0.0240, RÂ² = 0.9700
2025-04-04 16:15:02,544 - INFO - Epoch 9 started.
2025-04-04 16:15:02,544 - INFO - Epoch 9, Batch 1: Training loss = 1.0000
2025-04-04 16:15:02,644 - INFO - Epoch 9, Batch 2: Training loss = 0.9900
2025-04-04 16:15:02,744 - INFO - Epoch 9, Batch 3: Training loss = 0.9800
2025-04-04 16:15:02,845 - INFO - Epoch 9, Batch 4: Training loss = 0.9700
2025-04-04 16:15:02,946 - INFO - Epoch 9, Batch 5: Training loss = 0.9600
2025-04-04 16:15:03,046 - INFO - Epoch 9, Batch 6: Training loss = 0.9500
2025-04-04 16:15:03,147 - INFO - Epoch 9, Batch 7: Training loss = 0.9400
2025-04-04 16:15:03,248 - INFO - Epoch 9, Batch 8: Training loss = 0.9300
2025-04-04 16:15:03,349 - INFO - Epoch 9, Batch 9: Training loss = 0.9200
2025-04-04 16:15:03,450 - INFO - Epoch 9, Batch 10: Training loss = 0.9100
2025-04-04 16:15:03,550 - INFO - Epoch 9, Batch 11: Training loss = 0.9000
2025-04-04 16:15:03,651 - INFO - Epoch 9, Batch 12: Training loss = 0.8900
2025-04-04 16:15:03,751 - INFO - Epoch 9, Batch 13: Training loss = 0.8800
2025-04-04 16:15:03,852 - INFO - Epoch 9, Batch 14: Training loss = 0.8700
2025-04-04 16:15:03,953 - INFO - Epoch 9, Batch 15: Training loss = 0.8600
2025-04-04 16:15:04,054 - INFO - Epoch 9, Batch 16: Training loss = 0.8500
2025-04-04 16:15:04,154 - INFO - Epoch 9, Batch 17: Training loss = 0.8400
2025-04-04 16:15:04,256 - INFO - Epoch 9, Batch 18: Training loss = 0.8300
2025-04-04 16:15:04,356 - INFO - Epoch 9, Batch 19: Training loss = 0.8200
2025-04-04 16:15:04,457 - INFO - Epoch 9, Batch 20: Training loss = 0.8100
2025-04-04 16:15:04,558 - INFO - Epoch 9, Batch 21: Training loss = 0.8000
2025-04-04 16:15:04,659 - INFO - Epoch 9, Batch 22: Training loss = 0.7900
2025-04-04 16:15:04,759 - INFO - Epoch 9, Batch 23: Training loss = 0.7800
2025-04-04 16:15:04,860 - INFO - Epoch 9, Batch 24: Training loss = 0.7700
2025-04-04 16:15:04,960 - INFO - Epoch 9, Batch 25: Training loss = 0.7600
2025-04-04 16:15:05,061 - INFO - Epoch 9, Batch 26: Training loss = 0.7500
2025-04-04 16:15:05,161 - INFO - Epoch 9, Batch 27: Training loss = 0.7400
2025-04-04 16:15:05,262 - INFO - Epoch 9, Batch 28: Training loss = 0.7300
2025-04-04 16:15:05,363 - INFO - Epoch 9, Batch 29: Training loss = 0.7200
2025-04-04 16:15:05,463 - INFO - Epoch 9, Batch 30: Training loss = 0.7100
2025-04-04 16:15:05,564 - INFO - Epoch 9, Batch 31: Training loss = 0.7000
2025-04-04 16:15:05,665 - INFO - Epoch 9, Batch 32: Training loss = 0.6900
2025-04-04 16:15:05,765 - INFO - Epoch 9, Batch 33: Training loss = 0.6800
2025-04-04 16:15:05,866 - INFO - Epoch 9, Batch 34: Training loss = 0.6700
2025-04-04 16:15:05,966 - INFO - Epoch 9, Batch 35: Training loss = 0.6600
2025-04-04 16:15:06,067 - INFO - Epoch 9, Batch 36: Training loss = 0.6500
2025-04-04 16:15:06,167 - INFO - Epoch 9, Batch 37: Training loss = 0.6400
2025-04-04 16:15:06,268 - INFO - Epoch 9, Batch 38: Training loss = 0.6300
2025-04-04 16:15:06,369 - INFO - Epoch 9, Batch 39: Training loss = 0.6200
2025-04-04 16:15:06,470 - INFO - Epoch 9, Batch 40: Training loss = 0.6100
2025-04-04 16:15:06,571 - INFO - Epoch 9, Batch 41: Training loss = 0.6000
2025-04-04 16:15:06,673 - INFO - Epoch 9, Batch 42: Training loss = 0.5900
2025-04-04 16:15:06,773 - INFO - Epoch 9, Batch 43: Training loss = 0.5800
2025-04-04 16:15:06,874 - INFO - Epoch 9, Batch 44: Training loss = 0.5700
2025-04-04 16:15:06,974 - INFO - Epoch 9, Batch 45: Training loss = 0.5600
2025-04-04 16:15:07,074 - INFO - Epoch 9, Batch 46: Training loss = 0.5500
2025-04-04 16:15:07,175 - INFO - Epoch 9, Batch 47: Training loss = 0.5400
2025-04-04 16:15:07,275 - INFO - Epoch 9, Batch 48: Training loss = 0.5300
2025-04-04 16:15:07,376 - INFO - Epoch 9, Batch 49: Training loss = 0.5200
2025-04-04 16:15:07,477 - INFO - Epoch 9, Batch 50: Training loss = 0.5100
2025-04-04 16:15:07,578 - INFO - Epoch 9, Batch 51: Training loss = 0.5000
2025-04-04 16:15:07,678 - INFO - Epoch 9, Batch 52: Training loss = 0.4900
2025-04-04 16:15:07,779 - INFO - Epoch 9, Batch 53: Training loss = 0.4800
2025-04-04 16:15:07,880 - INFO - Epoch 9, Batch 54: Training loss = 0.4700
2025-04-04 16:15:07,981 - INFO - Epoch 9, Batch 55: Training loss = 0.4600
2025-04-04 16:15:08,082 - INFO - Epoch 9, Batch 56: Training loss = 0.4500
2025-04-04 16:15:08,183 - INFO - Epoch 9, Batch 57: Training loss = 0.4400
2025-04-04 16:15:08,284 - INFO - Epoch 9, Batch 58: Training loss = 0.4300
2025-04-04 16:15:08,384 - INFO - Epoch 9, Batch 59: Training loss = 0.4200
2025-04-04 16:15:08,485 - INFO - Epoch 9, Batch 60: Training loss = 0.4100
2025-04-04 16:15:08,586 - INFO - Epoch 9, Batch 61: Training loss = 0.4000
2025-04-04 16:15:08,687 - INFO - Epoch 9, Batch 62: Training loss = 0.3900
2025-04-04 16:15:08,787 - INFO - Epoch 9, Batch 63: Training loss = 0.3800
2025-04-04 16:15:08,888 - INFO - Epoch 9, Batch 64: Training loss = 0.3700
2025-04-04 16:15:08,989 - INFO - Epoch 9, Batch 65: Training loss = 0.3600
2025-04-04 16:15:09,090 - INFO - Epoch 9, Batch 66: Training loss = 0.3500
2025-04-04 16:15:09,190 - INFO - Epoch 9, Batch 67: Training loss = 0.3400
2025-04-04 16:15:09,291 - INFO - Epoch 9, Batch 68: Training loss = 0.3300
2025-04-04 16:15:09,392 - INFO - Epoch 9, Batch 69: Training loss = 0.3200
2025-04-04 16:15:09,493 - INFO - Epoch 9, Batch 70: Training loss = 0.3100
2025-04-04 16:15:09,593 - INFO - Epoch 9, Batch 71: Training loss = 0.3000
2025-04-04 16:15:09,694 - INFO - Epoch 9, Batch 72: Training loss = 0.2900
2025-04-04 16:15:09,795 - INFO - Epoch 9, Batch 73: Training loss = 0.2800
2025-04-04 16:15:09,895 - INFO - Epoch 9, Batch 74: Training loss = 0.2700
2025-04-04 16:15:09,996 - INFO - Epoch 9, Batch 75: Training loss = 0.2600
2025-04-04 16:15:10,097 - INFO - Epoch 9, Batch 76: Training loss = 0.2500
2025-04-04 16:15:10,198 - INFO - Epoch 9, Batch 77: Training loss = 0.2400
2025-04-04 16:15:10,299 - INFO - Epoch 9, Batch 78: Training loss = 0.2300
2025-04-04 16:15:10,400 - INFO - Epoch 9, Batch 79: Training loss = 0.2200
2025-04-04 16:15:10,500 - INFO - Epoch 9, Batch 80: Training loss = 0.2100
2025-04-04 16:15:10,602 - INFO - Epoch 9, Batch 81: Training loss = 0.2000
2025-04-04 16:15:10,702 - INFO - Epoch 9, Batch 82: Training loss = 0.1900
2025-04-04 16:15:10,803 - INFO - Epoch 9, Batch 83: Training loss = 0.1800
2025-04-04 16:15:10,903 - INFO - Epoch 9, Batch 84: Training loss = 0.1700
2025-04-04 16:15:11,004 - INFO - Epoch 9, Batch 85: Training loss = 0.1600
2025-04-04 16:15:11,104 - INFO - Epoch 9, Batch 86: Training loss = 0.1500
2025-04-04 16:15:11,205 - INFO - Epoch 9, Batch 87: Training loss = 0.1400
2025-04-04 16:15:11,305 - INFO - Epoch 9, Batch 88: Training loss = 0.1300
2025-04-04 16:15:11,406 - INFO - Epoch 9, Batch 89: Training loss = 0.1200
2025-04-04 16:15:11,507 - INFO - Epoch 9, Batch 90: Training loss = 0.1100
2025-04-04 16:15:11,608 - INFO - Epoch 9, Batch 91: Training loss = 0.1000
2025-04-04 16:15:11,709 - INFO - Epoch 9, Batch 92: Training loss = 0.0900
2025-04-04 16:15:11,809 - INFO - Epoch 9, Batch 93: Training loss = 0.0800
2025-04-04 16:15:11,910 - INFO - Epoch 9, Batch 94: Training loss = 0.0700
2025-04-04 16:15:12,011 - INFO - Epoch 9, Batch 95: Training loss = 0.0600
2025-04-04 16:15:12,111 - INFO - Epoch 9, Batch 96: Training loss = 0.0500
2025-04-04 16:15:12,212 - INFO - Epoch 9, Batch 97: Training loss = 0.0400
2025-04-04 16:15:12,314 - INFO - Epoch 9, Batch 98: Training loss = 0.0300
2025-04-04 16:15:12,414 - INFO - Epoch 9, Batch 99: Training loss = 0.0200
2025-04-04 16:15:12,515 - INFO - Epoch 9, Batch 100: Training loss = 0.0100
2025-04-04 16:15:12,616 - INFO - Epoch 9: Validation loss = 0.0200, Validation accuracy = 0.9600
2025-04-04 16:15:12,616 - INFO - Epoch 9: MSE = 0.0200, RMSE = 0.1414, MAE = 0.0160, RÂ² = 0.9800
2025-04-04 16:15:12,616 - INFO - Epoch 10 started.
2025-04-04 16:15:12,616 - INFO - Epoch 10, Batch 1: Training loss = 1.0000
2025-04-04 16:15:12,717 - INFO - Epoch 10, Batch 2: Training loss = 0.9900
2025-04-04 16:15:12,817 - INFO - Epoch 10, Batch 3: Training loss = 0.9800
2025-04-04 16:15:12,918 - INFO - Epoch 10, Batch 4: Training loss = 0.9700
2025-04-04 16:15:13,019 - INFO - Epoch 10, Batch 5: Training loss = 0.9600
2025-04-04 16:15:13,120 - INFO - Epoch 10, Batch 6: Training loss = 0.9500
2025-04-04 16:15:13,221 - INFO - Epoch 10, Batch 7: Training loss = 0.9400
2025-04-04 16:15:13,321 - INFO - Epoch 10, Batch 8: Training loss = 0.9300
2025-04-04 16:15:13,422 - INFO - Epoch 10, Batch 9: Training loss = 0.9200
2025-04-04 16:15:13,522 - INFO - Epoch 10, Batch 10: Training loss = 0.9100
2025-04-04 16:15:13,623 - INFO - Epoch 10, Batch 11: Training loss = 0.9000
2025-04-04 16:15:13,724 - INFO - Epoch 10, Batch 12: Training loss = 0.8900
2025-04-04 16:15:13,824 - INFO - Epoch 10, Batch 13: Training loss = 0.8800
2025-04-04 16:15:13,925 - INFO - Epoch 10, Batch 14: Training loss = 0.8700
2025-04-04 16:15:14,025 - INFO - Epoch 10, Batch 15: Training loss = 0.8600
2025-04-04 16:15:14,126 - INFO - Epoch 10, Batch 16: Training loss = 0.8500
2025-04-04 16:15:14,227 - INFO - Epoch 10, Batch 17: Training loss = 0.8400
2025-04-04 16:15:14,328 - INFO - Epoch 10, Batch 18: Training loss = 0.8300
2025-04-04 16:15:14,429 - INFO - Epoch 10, Batch 19: Training loss = 0.8200
2025-04-04 16:15:14,529 - INFO - Epoch 10, Batch 20: Training loss = 0.8100
2025-04-04 16:15:14,630 - INFO - Epoch 10, Batch 21: Training loss = 0.8000
2025-04-04 16:15:14,730 - INFO - Epoch 10, Batch 22: Training loss = 0.7900
2025-04-04 16:15:14,831 - INFO - Epoch 10, Batch 23: Training loss = 0.7800
2025-04-04 16:15:14,931 - INFO - Epoch 10, Batch 24: Training loss = 0.7700
2025-04-04 16:15:15,032 - INFO - Epoch 10, Batch 25: Training loss = 0.7600
2025-04-04 16:15:15,133 - INFO - Epoch 10, Batch 26: Training loss = 0.7500
2025-04-04 16:15:15,234 - INFO - Epoch 10, Batch 27: Training loss = 0.7400
2025-04-04 16:15:15,334 - INFO - Epoch 10, Batch 28: Training loss = 0.7300
2025-04-04 16:15:15,435 - INFO - Epoch 10, Batch 29: Training loss = 0.7200
2025-04-04 16:15:15,535 - INFO - Epoch 10, Batch 30: Training loss = 0.7100
2025-04-04 16:15:15,636 - INFO - Epoch 10, Batch 31: Training loss = 0.7000
2025-04-04 16:15:15,736 - INFO - Epoch 10, Batch 32: Training loss = 0.6900
2025-04-04 16:15:15,837 - INFO - Epoch 10, Batch 33: Training loss = 0.6800
2025-04-04 16:15:15,937 - INFO - Epoch 10, Batch 34: Training loss = 0.6700
2025-04-04 16:15:16,037 - INFO - Epoch 10, Batch 35: Training loss = 0.6600
2025-04-04 16:15:16,138 - INFO - Epoch 10, Batch 36: Training loss = 0.6500
2025-04-04 16:15:16,239 - INFO - Epoch 10, Batch 37: Training loss = 0.6400
2025-04-04 16:15:16,340 - INFO - Epoch 10, Batch 38: Training loss = 0.6300
2025-04-04 16:15:16,440 - INFO - Epoch 10, Batch 39: Training loss = 0.6200
2025-04-04 16:15:16,541 - INFO - Epoch 10, Batch 40: Training loss = 0.6100
2025-04-04 16:15:16,642 - INFO - Epoch 10, Batch 41: Training loss = 0.6000
2025-04-04 16:15:16,742 - INFO - Epoch 10, Batch 42: Training loss = 0.5900
2025-04-04 16:15:16,843 - INFO - Epoch 10, Batch 43: Training loss = 0.5800
2025-04-04 16:15:16,944 - INFO - Epoch 10, Batch 44: Training loss = 0.5700
2025-04-04 16:15:17,044 - INFO - Epoch 10, Batch 45: Training loss = 0.5600
2025-04-04 16:15:17,145 - INFO - Epoch 10, Batch 46: Training loss = 0.5500
2025-04-04 16:15:17,246 - INFO - Epoch 10, Batch 47: Training loss = 0.5400
2025-04-04 16:15:17,347 - INFO - Epoch 10, Batch 48: Training loss = 0.5300
2025-04-04 16:15:17,448 - INFO - Epoch 10, Batch 49: Training loss = 0.5200
2025-04-04 16:15:17,548 - INFO - Epoch 10, Batch 50: Training loss = 0.5100
2025-04-04 16:15:17,649 - INFO - Epoch 10, Batch 51: Training loss = 0.5000
2025-04-04 16:15:17,750 - INFO - Epoch 10, Batch 52: Training loss = 0.4900
2025-04-04 16:15:17,850 - INFO - Epoch 10, Batch 53: Training loss = 0.4800
2025-04-04 16:15:17,951 - INFO - Epoch 10, Batch 54: Training loss = 0.4700
2025-04-04 16:15:18,051 - INFO - Epoch 10, Batch 55: Training loss = 0.4600
2025-04-04 16:15:18,152 - INFO - Epoch 10, Batch 56: Training loss = 0.4500
2025-04-04 16:15:18,252 - INFO - Epoch 10, Batch 57: Training loss = 0.4400
2025-04-04 16:15:18,353 - INFO - Epoch 10, Batch 58: Training loss = 0.4300
2025-04-04 16:15:18,453 - INFO - Epoch 10, Batch 59: Training loss = 0.4200
2025-04-04 16:15:18,554 - INFO - Epoch 10, Batch 60: Training loss = 0.4100
2025-04-04 16:15:18,655 - INFO - Epoch 10, Batch 61: Training loss = 0.4000
2025-04-04 16:15:18,756 - INFO - Epoch 10, Batch 62: Training loss = 0.3900
2025-04-04 16:15:18,857 - INFO - Epoch 10, Batch 63: Training loss = 0.3800
2025-04-04 16:15:18,957 - INFO - Epoch 10, Batch 64: Training loss = 0.3700
2025-04-04 16:15:19,058 - INFO - Epoch 10, Batch 65: Training loss = 0.3600
2025-04-04 16:15:19,158 - INFO - Epoch 10, Batch 66: Training loss = 0.3500
2025-04-04 16:15:19,259 - INFO - Epoch 10, Batch 67: Training loss = 0.3400
2025-04-04 16:15:19,359 - INFO - Epoch 10, Batch 68: Training loss = 0.3300
2025-04-04 16:15:19,460 - INFO - Epoch 10, Batch 69: Training loss = 0.3200
2025-04-04 16:15:19,560 - INFO - Epoch 10, Batch 70: Training loss = 0.3100
2025-04-04 16:15:19,660 - INFO - Epoch 10, Batch 71: Training loss = 0.3000
2025-04-04 16:15:19,761 - INFO - Epoch 10, Batch 72: Training loss = 0.2900
2025-04-04 16:15:19,862 - INFO - Epoch 10, Batch 73: Training loss = 0.2800
2025-04-04 16:15:19,963 - INFO - Epoch 10, Batch 74: Training loss = 0.2700
2025-04-04 16:15:20,063 - INFO - Epoch 10, Batch 75: Training loss = 0.2600
2025-04-04 16:15:20,164 - INFO - Epoch 10, Batch 76: Training loss = 0.2500
2025-04-04 16:15:20,264 - INFO - Epoch 10, Batch 77: Training loss = 0.2400
2025-04-04 16:15:20,365 - INFO - Epoch 10, Batch 78: Training loss = 0.2300
2025-04-04 16:15:20,466 - INFO - Epoch 10, Batch 79: Training loss = 0.2200
2025-04-04 16:15:20,566 - INFO - Epoch 10, Batch 80: Training loss = 0.2100
2025-04-04 16:15:20,667 - INFO - Epoch 10, Batch 81: Training loss = 0.2000
2025-04-04 16:15:20,767 - INFO - Epoch 10, Batch 82: Training loss = 0.1900
2025-04-04 16:15:20,868 - INFO - Epoch 10, Batch 83: Training loss = 0.1800
2025-04-04 16:15:20,968 - INFO - Epoch 10, Batch 84: Training loss = 0.1700
2025-04-04 16:15:21,069 - INFO - Epoch 10, Batch 85: Training loss = 0.1600
2025-04-04 16:15:21,170 - INFO - Epoch 10, Batch 86: Training loss = 0.1500
2025-04-04 16:15:21,271 - INFO - Epoch 10, Batch 87: Training loss = 0.1400
2025-04-04 16:15:21,372 - INFO - Epoch 10, Batch 88: Training loss = 0.1300
2025-04-04 16:15:21,472 - INFO - Epoch 10, Batch 89: Training loss = 0.1200
2025-04-04 16:15:21,573 - INFO - Epoch 10, Batch 90: Training loss = 0.1100
2025-04-04 16:15:21,673 - INFO - Epoch 10, Batch 91: Training loss = 0.1000
2025-04-04 16:15:21,774 - INFO - Epoch 10, Batch 92: Training loss = 0.0900
2025-04-04 16:15:21,874 - INFO - Epoch 10, Batch 93: Training loss = 0.0800
2025-04-04 16:15:21,975 - INFO - Epoch 10, Batch 94: Training loss = 0.0700
2025-04-04 16:15:22,075 - INFO - Epoch 10, Batch 95: Training loss = 0.0600
2025-04-04 16:15:22,175 - INFO - Epoch 10, Batch 96: Training loss = 0.0500
2025-04-04 16:15:22,276 - INFO - Epoch 10, Batch 97: Training loss = 0.0400
2025-04-04 16:15:22,377 - INFO - Epoch 10, Batch 98: Training loss = 0.0300
2025-04-04 16:15:22,477 - INFO - Epoch 10, Batch 99: Training loss = 0.0200
2025-04-04 16:15:22,578 - INFO - Epoch 10, Batch 100: Training loss = 0.0100
2025-04-04 16:15:22,678 - INFO - Epoch 10: Validation loss = 0.0100, Validation accuracy = 0.9800
2025-04-04 16:15:22,678 - INFO - Epoch 10: MSE = 0.0100, RMSE = 0.1000, MAE = 0.0080, RÂ² = 0.9900
2025-04-04 16:15:22,679 - INFO - Regression training process completed.
